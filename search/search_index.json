{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"amba-analysis-worker-pubfinder Worker to try and find publication for unknown events","title":"Home"},{"location":"#amba-analysis-worker-pubfinder","text":"Worker to try and find publication for unknown events","title":"amba-analysis-worker-pubfinder"},{"location":"amba_source_ref/","text":"","title":"amba source"},{"location":"base_source_ref/","text":"","title":"base source"},{"location":"crossref_source_ref/","text":"fetch ( doi ) fetch response to add data to publication cache up to 100 since we should not have doi be occurring multiple times Parameters: Name Type Description Default doi the doi to be fetched required Source code in src/crossref_source.py @lru_cache ( maxsize = 10 ) def fetch ( doi ): \"\"\"fetch response to add data to publication cache up to 100 since we should not have doi be occurring multiple times Arguments: doi: the doi to be fetched \"\"\" r = requests . get ( CrossrefSource . base_url + requests . utils . quote ( doi )) # check encoding if r . status_code == 200 : json_response = r . json () if 'status' in json_response : if json_response [ 'status' ] == 'ok' : if 'message' in json_response : return json_response [ 'message' ] return None","title":"crossref source"},{"location":"crossref_source_ref/#crossref_source.fetch","text":"fetch response to add data to publication cache up to 100 since we should not have doi be occurring multiple times Parameters: Name Type Description Default doi the doi to be fetched required Source code in src/crossref_source.py @lru_cache ( maxsize = 10 ) def fetch ( doi ): \"\"\"fetch response to add data to publication cache up to 100 since we should not have doi be occurring multiple times Arguments: doi: the doi to be fetched \"\"\" r = requests . get ( CrossrefSource . base_url + requests . utils . quote ( doi )) # check encoding if r . status_code == 200 : json_response = r . json () if 'status' in json_response : if json_response [ 'status' ] == 'ok' : if 'message' in json_response : return json_response [ 'message' ] return None","title":"fetch()"},{"location":"meta_source_ref/","text":"MetaSource \" this source will try to append data using meta tags in the url of the resolved doi url add_data_to_publication ( self , publication ) add data to a given publication, only append, no overwriting if a value is already set Parameters: Name Type Description Default publication the publication to add data too required Source code in src/meta_source.py def add_data_to_publication ( self , publication ): \"\"\"add data to a given publication, only append, no overwriting if a value is already set Arguments: publication: the publication to add data too \"\"\" response = self . fetch ( publication [ 'doi' ]) data = self . get_lxml ( response ) return self . map ( data , publication ) fetch ( self , doi ) fetch data from the source using its doi Parameters: Name Type Description Default doi the doi of the publication required Source code in src/meta_source.py def fetch ( self , doi ): \"\"\"fetch data from the source using its doi Arguments: doi: the doi of the publication \"\"\" session = Session () headers = { 'User-Agent' : 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_11_6) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/66.0.3359.181 Safari/537.36' , 'Pragma' : 'no-cache' } session . headers . update ( headers ) return get_response ( self . base_url + doi , session ) format_date ( date_text ) staticmethod format a date to end up in our preferred format %Y-%m-%d possible input formats 15 Oct 2014 1969-12-01 2003-07 2014-9-11 July 2021 Example Output 2021-02-01 Source code in src/meta_source.py @staticmethod def format_date ( date_text ): \"\"\"format a date to end up in our preferred format %Y-%m-%d possible input formats 15 Oct 2014 1969-12-01 2003-07 2014-9-11 July 2021 Example Output 2021-02-01 \"\"\" try : date = parse ( date_text ) except ValueError : logging . warning ( \"unable to parse date string %s \" % date_text ) else : return date . strftime ( '%Y-%m- %d ' ) get_lxml ( self , page ) use lxml to parse the page and create a data dict from this page Parameters: Name Type Description Default page the page required Source code in src/meta_source.py def get_lxml ( self , page ): \"\"\"use lxml to parse the page and create a data dict from this page Arguments: page: the page \"\"\" result = {} data = {} if not page : return None content = html . fromstring ( page . content ) # go through all meta tags in the head for meta in content . xpath ( '//meta' ): # iterate through for name , value in sorted ( meta . items ()): # abstracts if value . strip () . lower () in self . abstract_tags : result [ value . strip () . lower ()] = meta . get ( 'content' ) if value . strip () . lower () in self . title_tags : result [ value . strip () . lower ()] = meta . get ( 'content' ) if value . strip () . lower () in self . date_tags : result [ value . strip () . lower ()] = meta . get ( 'content' ) if value . strip () . lower () in self . year_tag : result [ value . strip () . lower ()] = meta . get ( 'content' ) if value . strip () . lower () in self . publisher_tags : result [ value . strip () . lower ()] = meta . get ( 'content' ) if value . strip () . lower () in self . type_tag : result [ value . strip () . lower ()] = meta . get ( 'content' ) if value . strip () . lower () in self . author_tags : result [ value . strip () . lower ()] = meta . get ( 'content' ) if value . strip () . lower () in self . author_tags : result [ value . strip () . lower ()] = meta . get ( 'content' ) if value . strip () . lower () in self . keyword_tag : result [ value . strip () . lower ()] = meta . get ( 'content' ) if value . strip () . lower () in self . citation_tag : result [ value . strip () . lower ()] = meta . get ( 'content' ) # logging.debug(self.log + \" could not resolve: \" + json.dumps(result)) for key in self . abstract_tags : if key in result : abstract = pubfinder_worker . PubFinderWorker . clean_abstract ( result [ key ]) if len ( abstract ) > len ( data [ 'abstract' ]): data [ 'abstract' ] = result [ key ] for key in self . title_tags : if 'title' not in data : if key in result : data [ 'title' ] = result [ key ] for key in self . date_tags : if 'pub_date' not in data : if key in result : dateTemp = result [ key ] . replace ( \"/\" , \"-\" ) data [ 'pub_date' ] = dateTemp for key in self . year_tag : if 'year' not in data : if key in result : data [ 'date' ] = result [ key ] for key in self . publisher_tags : if 'publisher' not in data : if key in result : data [ 'publisher' ] = result [ key ] for key in self . type_tag : if 'type' not in data : if key in result : data [ 'type' ] = result [ key ] authors = [] for key in self . author_tags : if key in result and len ( result [ key ] . strip ()) > 1 : authors . append ( result [ key ] . strip ()) data [ 'authors' ] = authors keywords = [] for key in self . keyword_tag : if key in result : keywords . append ( result [ key ]) data [ 'fields_of_study' ] = keywords citations = [] for key in self . citation_tag : if key in result : citations . append ( result [ key ]) data [ 'citations' ] = citations return data map ( self , response_data , publication ) map response data and the publication Parameters: Name Type Description Default response_data the response data required publication the publication required Source code in src/meta_source.py def map ( self , response_data , publication ): \"\"\"map response data and the publication Arguments: response_data: the response data publication: the publication \"\"\" if 'abstract' in response_data and \\ ( 'abstract' not in publication or not pubfinder_worker . PubFinderWorker . valid_abstract ( publication [ 'abstract' ])): abstract = pubfinder_worker . PubFinderWorker . clean_abstract ( response_data [ 'abstract' ]) if pubfinder_worker . PubFinderWorker . valid_abstract ( abstract ): publication [ 'abstract' ] = abstract if response_data and 'title' in response_data and ( 'title' not in publication or len ( publication [ 'title' ]) < 5 ): publication [ 'title' ] = pubfinder_worker . PubFinderWorker . clean_title ( response_data [ 'title' ]) publication [ 'normalized_title' ] = pubfinder_worker . PubFinderWorker . normalize ( publication [ 'title' ]) if pubfinder_worker . PubFinderWorker . should_update ( 'pub_date' , response_data , publication ): publication [ 'pub_date' ] = MetaSource . format_date ( response_data [ 'pub_date' ]) if pubfinder_worker . PubFinderWorker . should_update ( 'year' , response_data , publication ): publication [ 'year' ] = response_data [ 'year' ] if pubfinder_worker . PubFinderWorker . should_update ( 'publisher' , response_data , publication ): publication [ 'publisher' ] = response_data [ 'publisher' ] # todo mappings # if pubfinder_worker.PubFinderWorker.should_update('type', response_data, publication): # publication['type'] = response_data['type'] if pubfinder_worker . PubFinderWorker . should_update ( 'authors' , response_data , publication ): publication [ 'authors' ] = response_data [ 'authors' ] if pubfinder_worker . PubFinderWorker . should_update ( 'fields_of_study' , response_data , publication ): publication [ 'fields_of_study' ] = self . map_fields_of_study ( response_data [ 'fields_of_study' ]) if response_data and 'citations' in response_data and ( 'citation_count' not in publication or publication [ 'citation_count' ] == 0 ): publication [ 'citation_count' ] = len ( response_data [ 'citations' ]) if pubfinder_worker . PubFinderWorker . should_update ( 'citations' , response_data , publication ): publication [ 'citations' ] = response_data [ 'citations' ] return None worker ( self ) the worker thread function will ensure that the publications in the queue are all processed, it will sleep for 0.1s if no item is in the queue to reduce cpu usage Source code in src/meta_source.py def worker ( self ): \"\"\"the worker thread function will ensure that the publications in the queue are all processed, it will sleep for 0.1s if no item is in the queue to reduce cpu usage \"\"\" while self . running : try : item = self . work_queue . pop () except IndexError : time . sleep ( 0.1 ) pass else : if item : # logging.warning(self.log + \" item \" + str(item.get_json())) publication = pubfinder_worker . PubFinderWorker . get_publication ( item ) logging . warning ( self . log + \" work on item \" + publication [ 'doi' ]) # logging.warning(self.log + \" q \" + str(queue))x # source stuff publication_temp = self . add_data_to_publication ( publication ) # only if we have any data we set it if publication_temp : publication = publication_temp publication [ 'source' ] = self . tag # no meta since link already present source_ids = publication [ 'source_id' ] # todo check if actually anything was added source_ids . append ({ 'title' : 'Meta' , 'url' : 'https://doi.org/' + publication [ 'doi' ], 'license' : 'TODO' }) publication [ 'source_id' ] = source_ids if type ( item ) is Event : item . data [ 'obj' ][ 'data' ] = publication result = { 'item' : item , 'tag' : self . tag } self . result_deque . append ( result ) get_response ( url , s ) get a response from a given url using a given session s, a session can be used for headers, this function is cached up to 100 elements !!! arguments url: the url to get s: the session to use Source code in src/meta_source.py @lru_cache ( maxsize = 10 ) def get_response ( url , s ): \"\"\"get a response from a given url using a given session s, a session can be used for headers, this function is cached up to 100 elements Arguments: url: the url to get s: the session to use \"\"\" try : result = s . get ( url , timeout = 5 ) except ( ConnectionRefusedError , SSLError , ReadTimeoutError , requests . exceptions . TooManyRedirects , requests . exceptions . ReadTimeout , NewConnectionError , requests . exceptions . SSLError , ConnectionError ): logging . warning ( 'Meta Source - Pubfinder' ) s = Session () # get the response for the provided url headers = { 'User-Agent' : 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_11_6) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/66.0.3359.181 Safari/537.36' , 'Pragma' : 'no-cache' } s . headers . update ( headers ) else : return result return None","title":"meta source"},{"location":"meta_source_ref/#meta_source.MetaSource","text":"\" this source will try to append data using meta tags in the url of the resolved doi url","title":"MetaSource"},{"location":"meta_source_ref/#meta_source.MetaSource.add_data_to_publication","text":"add data to a given publication, only append, no overwriting if a value is already set Parameters: Name Type Description Default publication the publication to add data too required Source code in src/meta_source.py def add_data_to_publication ( self , publication ): \"\"\"add data to a given publication, only append, no overwriting if a value is already set Arguments: publication: the publication to add data too \"\"\" response = self . fetch ( publication [ 'doi' ]) data = self . get_lxml ( response ) return self . map ( data , publication )","title":"add_data_to_publication()"},{"location":"meta_source_ref/#meta_source.MetaSource.fetch","text":"fetch data from the source using its doi Parameters: Name Type Description Default doi the doi of the publication required Source code in src/meta_source.py def fetch ( self , doi ): \"\"\"fetch data from the source using its doi Arguments: doi: the doi of the publication \"\"\" session = Session () headers = { 'User-Agent' : 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_11_6) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/66.0.3359.181 Safari/537.36' , 'Pragma' : 'no-cache' } session . headers . update ( headers ) return get_response ( self . base_url + doi , session )","title":"fetch()"},{"location":"meta_source_ref/#meta_source.MetaSource.format_date","text":"format a date to end up in our preferred format %Y-%m-%d possible input formats 15 Oct 2014 1969-12-01 2003-07 2014-9-11 July 2021 Example Output 2021-02-01 Source code in src/meta_source.py @staticmethod def format_date ( date_text ): \"\"\"format a date to end up in our preferred format %Y-%m-%d possible input formats 15 Oct 2014 1969-12-01 2003-07 2014-9-11 July 2021 Example Output 2021-02-01 \"\"\" try : date = parse ( date_text ) except ValueError : logging . warning ( \"unable to parse date string %s \" % date_text ) else : return date . strftime ( '%Y-%m- %d ' )","title":"format_date()"},{"location":"meta_source_ref/#meta_source.MetaSource.get_lxml","text":"use lxml to parse the page and create a data dict from this page Parameters: Name Type Description Default page the page required Source code in src/meta_source.py def get_lxml ( self , page ): \"\"\"use lxml to parse the page and create a data dict from this page Arguments: page: the page \"\"\" result = {} data = {} if not page : return None content = html . fromstring ( page . content ) # go through all meta tags in the head for meta in content . xpath ( '//meta' ): # iterate through for name , value in sorted ( meta . items ()): # abstracts if value . strip () . lower () in self . abstract_tags : result [ value . strip () . lower ()] = meta . get ( 'content' ) if value . strip () . lower () in self . title_tags : result [ value . strip () . lower ()] = meta . get ( 'content' ) if value . strip () . lower () in self . date_tags : result [ value . strip () . lower ()] = meta . get ( 'content' ) if value . strip () . lower () in self . year_tag : result [ value . strip () . lower ()] = meta . get ( 'content' ) if value . strip () . lower () in self . publisher_tags : result [ value . strip () . lower ()] = meta . get ( 'content' ) if value . strip () . lower () in self . type_tag : result [ value . strip () . lower ()] = meta . get ( 'content' ) if value . strip () . lower () in self . author_tags : result [ value . strip () . lower ()] = meta . get ( 'content' ) if value . strip () . lower () in self . author_tags : result [ value . strip () . lower ()] = meta . get ( 'content' ) if value . strip () . lower () in self . keyword_tag : result [ value . strip () . lower ()] = meta . get ( 'content' ) if value . strip () . lower () in self . citation_tag : result [ value . strip () . lower ()] = meta . get ( 'content' ) # logging.debug(self.log + \" could not resolve: \" + json.dumps(result)) for key in self . abstract_tags : if key in result : abstract = pubfinder_worker . PubFinderWorker . clean_abstract ( result [ key ]) if len ( abstract ) > len ( data [ 'abstract' ]): data [ 'abstract' ] = result [ key ] for key in self . title_tags : if 'title' not in data : if key in result : data [ 'title' ] = result [ key ] for key in self . date_tags : if 'pub_date' not in data : if key in result : dateTemp = result [ key ] . replace ( \"/\" , \"-\" ) data [ 'pub_date' ] = dateTemp for key in self . year_tag : if 'year' not in data : if key in result : data [ 'date' ] = result [ key ] for key in self . publisher_tags : if 'publisher' not in data : if key in result : data [ 'publisher' ] = result [ key ] for key in self . type_tag : if 'type' not in data : if key in result : data [ 'type' ] = result [ key ] authors = [] for key in self . author_tags : if key in result and len ( result [ key ] . strip ()) > 1 : authors . append ( result [ key ] . strip ()) data [ 'authors' ] = authors keywords = [] for key in self . keyword_tag : if key in result : keywords . append ( result [ key ]) data [ 'fields_of_study' ] = keywords citations = [] for key in self . citation_tag : if key in result : citations . append ( result [ key ]) data [ 'citations' ] = citations return data","title":"get_lxml()"},{"location":"meta_source_ref/#meta_source.MetaSource.map","text":"map response data and the publication Parameters: Name Type Description Default response_data the response data required publication the publication required Source code in src/meta_source.py def map ( self , response_data , publication ): \"\"\"map response data and the publication Arguments: response_data: the response data publication: the publication \"\"\" if 'abstract' in response_data and \\ ( 'abstract' not in publication or not pubfinder_worker . PubFinderWorker . valid_abstract ( publication [ 'abstract' ])): abstract = pubfinder_worker . PubFinderWorker . clean_abstract ( response_data [ 'abstract' ]) if pubfinder_worker . PubFinderWorker . valid_abstract ( abstract ): publication [ 'abstract' ] = abstract if response_data and 'title' in response_data and ( 'title' not in publication or len ( publication [ 'title' ]) < 5 ): publication [ 'title' ] = pubfinder_worker . PubFinderWorker . clean_title ( response_data [ 'title' ]) publication [ 'normalized_title' ] = pubfinder_worker . PubFinderWorker . normalize ( publication [ 'title' ]) if pubfinder_worker . PubFinderWorker . should_update ( 'pub_date' , response_data , publication ): publication [ 'pub_date' ] = MetaSource . format_date ( response_data [ 'pub_date' ]) if pubfinder_worker . PubFinderWorker . should_update ( 'year' , response_data , publication ): publication [ 'year' ] = response_data [ 'year' ] if pubfinder_worker . PubFinderWorker . should_update ( 'publisher' , response_data , publication ): publication [ 'publisher' ] = response_data [ 'publisher' ] # todo mappings # if pubfinder_worker.PubFinderWorker.should_update('type', response_data, publication): # publication['type'] = response_data['type'] if pubfinder_worker . PubFinderWorker . should_update ( 'authors' , response_data , publication ): publication [ 'authors' ] = response_data [ 'authors' ] if pubfinder_worker . PubFinderWorker . should_update ( 'fields_of_study' , response_data , publication ): publication [ 'fields_of_study' ] = self . map_fields_of_study ( response_data [ 'fields_of_study' ]) if response_data and 'citations' in response_data and ( 'citation_count' not in publication or publication [ 'citation_count' ] == 0 ): publication [ 'citation_count' ] = len ( response_data [ 'citations' ]) if pubfinder_worker . PubFinderWorker . should_update ( 'citations' , response_data , publication ): publication [ 'citations' ] = response_data [ 'citations' ] return None","title":"map()"},{"location":"meta_source_ref/#meta_source.MetaSource.worker","text":"the worker thread function will ensure that the publications in the queue are all processed, it will sleep for 0.1s if no item is in the queue to reduce cpu usage Source code in src/meta_source.py def worker ( self ): \"\"\"the worker thread function will ensure that the publications in the queue are all processed, it will sleep for 0.1s if no item is in the queue to reduce cpu usage \"\"\" while self . running : try : item = self . work_queue . pop () except IndexError : time . sleep ( 0.1 ) pass else : if item : # logging.warning(self.log + \" item \" + str(item.get_json())) publication = pubfinder_worker . PubFinderWorker . get_publication ( item ) logging . warning ( self . log + \" work on item \" + publication [ 'doi' ]) # logging.warning(self.log + \" q \" + str(queue))x # source stuff publication_temp = self . add_data_to_publication ( publication ) # only if we have any data we set it if publication_temp : publication = publication_temp publication [ 'source' ] = self . tag # no meta since link already present source_ids = publication [ 'source_id' ] # todo check if actually anything was added source_ids . append ({ 'title' : 'Meta' , 'url' : 'https://doi.org/' + publication [ 'doi' ], 'license' : 'TODO' }) publication [ 'source_id' ] = source_ids if type ( item ) is Event : item . data [ 'obj' ][ 'data' ] = publication result = { 'item' : item , 'tag' : self . tag } self . result_deque . append ( result )","title":"worker()"},{"location":"meta_source_ref/#meta_source.get_response","text":"get a response from a given url using a given session s, a session can be used for headers, this function is cached up to 100 elements !!! arguments url: the url to get s: the session to use Source code in src/meta_source.py @lru_cache ( maxsize = 10 ) def get_response ( url , s ): \"\"\"get a response from a given url using a given session s, a session can be used for headers, this function is cached up to 100 elements Arguments: url: the url to get s: the session to use \"\"\" try : result = s . get ( url , timeout = 5 ) except ( ConnectionRefusedError , SSLError , ReadTimeoutError , requests . exceptions . TooManyRedirects , requests . exceptions . ReadTimeout , NewConnectionError , requests . exceptions . SSLError , ConnectionError ): logging . warning ( 'Meta Source - Pubfinder' ) s = Session () # get the response for the provided url headers = { 'User-Agent' : 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_11_6) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/66.0.3359.181 Safari/537.36' , 'Pragma' : 'no-cache' } s . headers . update ( headers ) else : return result return None","title":"get_response()"},{"location":"openaire_source_ref/","text":"OpenAireSource get_lxml ( self , page ) use lxml to parse the page and create a data dict from this page Parameters: Name Type Description Default page the page required Source code in src/openaire_source.py def get_lxml ( self , page ): \"\"\"use lxml to parse the page and create a data dict from this page Arguments: page: the page \"\"\" result = {} if not page : return None content = html . fromstring ( page . content ) d = content . xpath ( '//description' ) if len ( d ) > 0 : description = d [ 0 ] . text result [ 'abstract' ] = description pu = content . xpath ( '//publisher' ) if len ( pu ) > 0 : publisher = pu [ 0 ] . text result [ 'publisher' ] = publisher t = content . xpath ( \"//title[@classid='main title']\" ) if len ( t ) > 0 : title = t [ 0 ] . text result [ 'title' ] = title p = content . xpath ( \"/response/results/result/metadata/*[name()='oaf:entity']/*[name()='oaf:result']/dateofacceptance\" ) if len ( p ) > 0 : pub_date = p [ 0 ] . text result [ 'pub_date' ] = pub_date result [ 'year' ] = pub_date . split ( '-' )[ 0 ] a = content . xpath ( \"/response/results/result/metadata/*[name()='oaf:entity']/*[name()='oaf:result']/creator\" ) if len ( a ) > 0 : authors = [] for author in a : authors . append ( author . text ) result [ 'authors' ] = authors f = content . xpath ( \"/response/results/result/metadata/*[name()='oaf:entity']/*[name()='oaf:result']/subject[not(@trust)]\" ) if len ( f ) > 0 : fos = [] for fs in f : fos . append ( fs . text ) result [ 'fields_of_study' ] = fos return result fetch ( doi ) fetch response to add data to publication cache up to 100 since we should not have doi be occurring multiple times Parameters: Name Type Description Default doi the doi to be fetched required Source code in src/openaire_source.py @lru_cache ( maxsize = 10 ) def fetch ( doi ): \"\"\"fetch response to add data to publication cache up to 100 since we should not have doi be occurring multiple times Arguments: doi: the doi to be fetched \"\"\" return requests . get ( OpenAireSource . base_url + requests . utils . quote ( doi )) # check encoding","title":"openaire source"},{"location":"openaire_source_ref/#openaire_source.OpenAireSource","text":"","title":"OpenAireSource"},{"location":"openaire_source_ref/#openaire_source.OpenAireSource.get_lxml","text":"use lxml to parse the page and create a data dict from this page Parameters: Name Type Description Default page the page required Source code in src/openaire_source.py def get_lxml ( self , page ): \"\"\"use lxml to parse the page and create a data dict from this page Arguments: page: the page \"\"\" result = {} if not page : return None content = html . fromstring ( page . content ) d = content . xpath ( '//description' ) if len ( d ) > 0 : description = d [ 0 ] . text result [ 'abstract' ] = description pu = content . xpath ( '//publisher' ) if len ( pu ) > 0 : publisher = pu [ 0 ] . text result [ 'publisher' ] = publisher t = content . xpath ( \"//title[@classid='main title']\" ) if len ( t ) > 0 : title = t [ 0 ] . text result [ 'title' ] = title p = content . xpath ( \"/response/results/result/metadata/*[name()='oaf:entity']/*[name()='oaf:result']/dateofacceptance\" ) if len ( p ) > 0 : pub_date = p [ 0 ] . text result [ 'pub_date' ] = pub_date result [ 'year' ] = pub_date . split ( '-' )[ 0 ] a = content . xpath ( \"/response/results/result/metadata/*[name()='oaf:entity']/*[name()='oaf:result']/creator\" ) if len ( a ) > 0 : authors = [] for author in a : authors . append ( author . text ) result [ 'authors' ] = authors f = content . xpath ( \"/response/results/result/metadata/*[name()='oaf:entity']/*[name()='oaf:result']/subject[not(@trust)]\" ) if len ( f ) > 0 : fos = [] for fs in f : fos . append ( fs . text ) result [ 'fields_of_study' ] = fos return result","title":"get_lxml()"},{"location":"openaire_source_ref/#openaire_source.fetch","text":"fetch response to add data to publication cache up to 100 since we should not have doi be occurring multiple times Parameters: Name Type Description Default doi the doi to be fetched required Source code in src/openaire_source.py @lru_cache ( maxsize = 10 ) def fetch ( doi ): \"\"\"fetch response to add data to publication cache up to 100 since we should not have doi be occurring multiple times Arguments: doi: the doi to be fetched \"\"\" return requests . get ( OpenAireSource . base_url + requests . utils . quote ( doi )) # check encoding","title":"fetch()"},{"location":"pubfinder_worker_ref/","text":"PubFinderWorker ( EventStreamProducer ) clean_abstract ( abstract ) staticmethod cleans the title and removes unnecessary spaces and line breaks Source code in src/pubfinder_worker.py @staticmethod def clean_abstract ( abstract ): \"\"\"cleans the title and removes unnecessary spaces and line breaks \"\"\" try : abstract = re . sub ( '<.*?>|&([a-z0-9]+|#[0-9]{1,6}|#x[0-9a-f]{1,6});' , '' , abstract ) except TypeError : # logging.exception(abstract) return '' else : abstract = abstract . strip () remove_words = [ 'abstract' , 'background' , 'background:' , 'introduction' , 'objective' , 'nature' ] # logging.warning('dirty %s', abstract[:100]) while True : removed_word = False for word in remove_words : if re . match ( word , abstract , re . I ): abstract = abstract [ len ( word ):] removed_word = True if not removed_word : break abstract = re . sub ( r ' +' , ' ' , abstract ) abstract = re . sub ( r ' \\. ' , ' ' , abstract ) abstract = re . sub ( r ' *: ' , ' ' , abstract ) abstract = re . sub ( r ' - ' , ' ' , abstract ) # clean_abstract = re.sub(r'(\\s*)Abstract(\\s*)', '', clean_abstract, flags=re.IGNORECASE) # logging.warning('clean %s', abstract[:100]) return abstract . strip () clean_fos ( fos ) staticmethod cleans the title and removes unnecessary spaces and line breaks Source code in src/pubfinder_worker.py @staticmethod def clean_fos ( fos ): # todo where \"\"\"cleans the title and removes unnecessary spaces and line breaks \"\"\" results = [] for f in fos : if ';' in f : d = f . split ( 'f' ) results . extend ( d ) else : results . append ( f ) return results clean_title ( title ) staticmethod cleans the title and removes unnecessary spaces and line breaks Source code in src/pubfinder_worker.py @staticmethod def clean_title ( title ): \"\"\"cleans the title and removes unnecessary spaces and line breaks \"\"\" title . replace ( ' \\n ' , ' ' ) return re . sub ( ' +' , ' ' , title ) . strip () start ( i = 0 ) staticmethod start the consumer Source code in src/pubfinder_worker.py @staticmethod def start ( i = 0 ): \"\"\"start the consumer \"\"\" pfw = PubFinderWorker ( i ) logging . debug ( PubFinderWorker . log + 'Start %s ' % str ( i )) pfw . consume ()","title":"pubfinder worker"},{"location":"pubfinder_worker_ref/#pubfinder_worker.PubFinderWorker","text":"","title":"PubFinderWorker"},{"location":"pubfinder_worker_ref/#pubfinder_worker.PubFinderWorker.clean_abstract","text":"cleans the title and removes unnecessary spaces and line breaks Source code in src/pubfinder_worker.py @staticmethod def clean_abstract ( abstract ): \"\"\"cleans the title and removes unnecessary spaces and line breaks \"\"\" try : abstract = re . sub ( '<.*?>|&([a-z0-9]+|#[0-9]{1,6}|#x[0-9a-f]{1,6});' , '' , abstract ) except TypeError : # logging.exception(abstract) return '' else : abstract = abstract . strip () remove_words = [ 'abstract' , 'background' , 'background:' , 'introduction' , 'objective' , 'nature' ] # logging.warning('dirty %s', abstract[:100]) while True : removed_word = False for word in remove_words : if re . match ( word , abstract , re . I ): abstract = abstract [ len ( word ):] removed_word = True if not removed_word : break abstract = re . sub ( r ' +' , ' ' , abstract ) abstract = re . sub ( r ' \\. ' , ' ' , abstract ) abstract = re . sub ( r ' *: ' , ' ' , abstract ) abstract = re . sub ( r ' - ' , ' ' , abstract ) # clean_abstract = re.sub(r'(\\s*)Abstract(\\s*)', '', clean_abstract, flags=re.IGNORECASE) # logging.warning('clean %s', abstract[:100]) return abstract . strip ()","title":"clean_abstract()"},{"location":"pubfinder_worker_ref/#pubfinder_worker.PubFinderWorker.clean_fos","text":"cleans the title and removes unnecessary spaces and line breaks Source code in src/pubfinder_worker.py @staticmethod def clean_fos ( fos ): # todo where \"\"\"cleans the title and removes unnecessary spaces and line breaks \"\"\" results = [] for f in fos : if ';' in f : d = f . split ( 'f' ) results . extend ( d ) else : results . append ( f ) return results","title":"clean_fos()"},{"location":"pubfinder_worker_ref/#pubfinder_worker.PubFinderWorker.clean_title","text":"cleans the title and removes unnecessary spaces and line breaks Source code in src/pubfinder_worker.py @staticmethod def clean_title ( title ): \"\"\"cleans the title and removes unnecessary spaces and line breaks \"\"\" title . replace ( ' \\n ' , ' ' ) return re . sub ( ' +' , ' ' , title ) . strip ()","title":"clean_title()"},{"location":"pubfinder_worker_ref/#pubfinder_worker.PubFinderWorker.start","text":"start the consumer Source code in src/pubfinder_worker.py @staticmethod def start ( i = 0 ): \"\"\"start the consumer \"\"\" pfw = PubFinderWorker ( i ) logging . debug ( PubFinderWorker . log + 'Start %s ' % str ( i )) pfw . consume ()","title":"start()"},{"location":"semanticscholar_source_ref/","text":"fetch ( doi ) fetch response to add data to publication cache up to 100 since we should not have doi be occurring multiple times Parameters: Name Type Description Default doi the doi to be fetched required Source code in src/semanticscholar_source.py @lru_cache ( maxsize = 10 ) def fetch ( doi ): \"\"\"fetch response to add data to publication cache up to 100 since we should not have doi be occurring multiple times Arguments: doi: the doi to be fetched \"\"\" r = requests . get ( SemanticScholarSource . base_url + requests . utils . quote ( doi )) # check encoding if r . status_code == 200 : json_response = r . json () if 'error' not in json_response : return json_response return None","title":"semanticscholar source"},{"location":"semanticscholar_source_ref/#semanticscholar_source.fetch","text":"fetch response to add data to publication cache up to 100 since we should not have doi be occurring multiple times Parameters: Name Type Description Default doi the doi to be fetched required Source code in src/semanticscholar_source.py @lru_cache ( maxsize = 10 ) def fetch ( doi ): \"\"\"fetch response to add data to publication cache up to 100 since we should not have doi be occurring multiple times Arguments: doi: the doi to be fetched \"\"\" r = requests . get ( SemanticScholarSource . base_url + requests . utils . quote ( doi )) # check encoding if r . status_code == 200 : json_response = r . json () if 'error' not in json_response : return json_response return None","title":"fetch()"}]}