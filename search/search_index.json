{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"amba-analysis-worker-pubfinder Worker to try and find publication for unknown events","title":"Home"},{"location":"#amba-analysis-worker-pubfinder","text":"Worker to try and find publication for unknown events","title":"amba-analysis-worker-pubfinder"},{"location":"amba_source_ref/","text":"","title":"amba source"},{"location":"base_source_ref/","text":"","title":"base source"},{"location":"crossref_source_ref/","text":"fetch ( doi ) fetch response to add data to publication cache up to 100 since we should not have doi be occurring multiple times Parameters: Name Type Description Default doi the doi to be fetched required Source code in src/crossref_source.py @lru_cache ( maxsize = 100 ) def fetch ( doi ): \"\"\"fetch response to add data to publication cache up to 100 since we should not have doi be occurring multiple times Arguments: doi: the doi to be fetched \"\"\" r = requests . get ( CrossrefSource . base_url + requests . utils . quote ( doi )) # check encoding if r . status_code == 200 : json_response = r . json () if 'status' in json_response : if json_response [ 'status' ] == 'ok' : if 'message' in json_response : return json_response [ 'message' ] return None","title":"crossref source"},{"location":"crossref_source_ref/#crossref_source.fetch","text":"fetch response to add data to publication cache up to 100 since we should not have doi be occurring multiple times Parameters: Name Type Description Default doi the doi to be fetched required Source code in src/crossref_source.py @lru_cache ( maxsize = 100 ) def fetch ( doi ): \"\"\"fetch response to add data to publication cache up to 100 since we should not have doi be occurring multiple times Arguments: doi: the doi to be fetched \"\"\" r = requests . get ( CrossrefSource . base_url + requests . utils . quote ( doi )) # check encoding if r . status_code == 200 : json_response = r . json () if 'status' in json_response : if json_response [ 'status' ] == 'ok' : if 'message' in json_response : return json_response [ 'message' ] return None","title":"fetch()"},{"location":"meta_source_ref/","text":"MetaSource \" this source will try to append data using meta tags in the url of the resolved doi url __init__ ( self , pubfinder ) special setup a ThreadPool, don't need the cpu since requests are slow and we wan't to share data Parameters: Name Type Description Default pubfinder the main process where we get data from and to required Source code in src/meta_source.py def __init__ ( self , pubfinder ): \"\"\"setup a ThreadPool, don't need the cpu since requests are slow and we wan't to share data Arguments: pubfinder: the main process where we get data from and to \"\"\" if not self . work_pool : self . work_pool = ThreadPool ( self . threads , self . worker , ()) self . pubfinder = pubfinder add_data_to_publication ( self , publication ) add data to a given publication, only append, no overwriting if a value is already set Parameters: Name Type Description Default publication the publication to add data too required Source code in src/meta_source.py def add_data_to_publication ( self , publication ): \"\"\"add data to a given publication, only append, no overwriting if a value is already set Arguments: publication: the publication to add data too \"\"\" response = self . fetch ( publication [ 'doi' ]) data = self . get_lxml ( response ) return self . map ( data , publication ) fetch ( self , doi ) fetch data from the source using its doi Parameters: Name Type Description Default doi the doi of the publication required Source code in src/meta_source.py def fetch ( self , doi ): \"\"\"fetch data from the source using its doi Arguments: doi: the doi of the publication \"\"\" session = Session () headers = { 'User-Agent' : 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_11_6) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/66.0.3359.181 Safari/537.36' , 'Pragma' : 'no-cache' } session . headers . update ( headers ) return get_response ( self . base_url + doi , session ) get_lxml ( self , page ) use lxml to parse the page and create a data dict from this page Parameters: Name Type Description Default page the page required Source code in src/meta_source.py def get_lxml ( self , page ): \"\"\"use lxml to parse the page and create a data dict from this page Arguments: page: the page \"\"\" result = {} data = {} if not page : return None content = html . fromstring ( page . content ) # go through all meta tags in the head for meta in content . xpath ( '//html//meta' ): # iterate through for name , value in sorted ( meta . items ()): # abstracts if value . strip () . lower () in self . abstract_tags : result [ value . strip () . lower ()] = meta . get ( 'content' ) if value . strip () . lower () in self . title_tags : result [ value . strip () . lower ()] = meta . get ( 'content' ) if value . strip () . lower () in self . date_tags : result [ value . strip () . lower ()] = meta . get ( 'content' ) if value . strip () . lower () in self . year_tag : result [ value . strip () . lower ()] = meta . get ( 'content' ) if value . strip () . lower () in self . publisher_tags : result [ value . strip () . lower ()] = meta . get ( 'content' ) if value . strip () . lower () in self . type_tag : result [ value . strip () . lower ()] = meta . get ( 'content' ) if value . strip () . lower () in self . author_tags : result [ value . strip () . lower ()] = meta . get ( 'content' ) if value . strip () . lower () in self . author_tags : result [ value . strip () . lower ()] = meta . get ( 'content' ) if value . strip () . lower () in self . keyword_tag : result [ value . strip () . lower ()] = meta . get ( 'content' ) if value . strip () . lower () in self . citation_tag : result [ value . strip () . lower ()] = meta . get ( 'content' ) logging . debug ( self . log + \" could not resolve: \" + json . dumps ( result )) if 'abstract' not in data : for key in self . abstract_tags : if key in result : data [ 'abstract' ] = result [ key ] if 'title' not in data : for key in self . title_tags : if key in result : data [ 'title' ] = result [ key ] if 'pubDate' not in data : for key in self . date_tags : if key in result : dateTemp = result [ key ] . replace ( \"/\" , \"-\" ) data [ 'pubDate' ] = dateTemp if 'year' not in data : for key in self . year_tag : if key in result : data [ 'date' ] = result [ key ] if 'publisher' not in data : for key in self . publisher_tags : if key in result : data [ 'publisher' ] = result [ key ] if 'type' not in data : for key in self . type_tag : if key in result : data [ 'type' ] = result [ key ] if 'authors' not in data : authors = [] for key in self . author_tags : if key in result : authors . append ( result [ key ]) data [ 'authors' ] = authors if 'fieldsOfStudy' not in data : keywords = [] for key in self . keyword_tag : if key in result : keywords . append ( result [ key ]) data [ 'fieldsOfStudy' ] = keywords if 'citations' not in data : citations = [] for key in self . citation_tag : if key in result : citations . append ( result [ key ]) data [ 'citations' ] = citations return data map ( self , response_data , publication ) map response data and the publication Parameters: Name Type Description Default response_data the response data required publication the publication required Source code in src/meta_source.py def map ( self , response_data , publication ): \"\"\"map response data and the publication Arguments: response_data: the response data publication: the publication \"\"\" # min length to use 50 if response_data and 'abstract' in response_data and ( 'abstract' not in publication or len ( publication [ 'abstract' ]) < 50 ): publication [ 'abstract' ] = response_data [ 'abstract' ] if response_data and 'title' in response_data and ( 'title' not in publication or len ( publication [ 'title' ]) < 5 ): publication [ 'title' ] = response_data [ 'title' ] if response_data and 'pubDate' in response_data and 'pubDate' not in publication : publication [ 'pubDate' ] = response_data [ 'pubDate' ] if response_data and 'year' in response_data and 'year' not in publication : publication [ 'year' ] = response_data [ 'year' ] if response_data and 'publisher' in response_data and 'publisher' not in publication : publication [ 'publisher' ] = response_data [ 'publisher' ] if response_data and 'type' in response_data and 'type' not in publication : publication [ 'type' ] = response_data [ 'type' ] if response_data and 'authors' in response_data and 'authors' not in publication : publication [ 'authors' ] = response_data [ 'authors' ] if response_data and 'fieldsOfStudy' in response_data and 'fieldsOfStudy' not in publication : publication [ 'fieldsOfStudy' ] = response_data [ 'fieldsOfStudy' ] if response_data and 'citations' in response_data and 'citationCount' not in publication : publication [ 'citationCount' ] = len ( response_data [ 'citations' ]) if response_data and 'citations' in response_data and 'citations' not in publication : publication [ 'citations' ] = response_data [ 'citations' ] return None worker ( self ) the worker thread function will ensure that the publications in the queue are all processed, it will sleep for 0.1s if no item is in the queue to reduce cpu usage Source code in src/meta_source.py def worker ( self ): \"\"\"the worker thread function will ensure that the publications in the queue are all processed, it will sleep for 0.1s if no item is in the queue to reduce cpu usage \"\"\" while self . running : try : item = self . work_queue . pop () except IndexError : time . sleep ( 0.1 ) pass else : if item : # logging.warning(self.log + \" item \" + str(item.get_json())) publication = self . pubfinder . get_publication ( item ) logging . warning ( self . log + \" work on item \" + publication [ 'doi' ]) # logging.warning(self.log + \" q \" + str(queue))x # source stuff publication_temp = self . add_data_to_publication ( publication ) # only if we have any data we set it if publication_temp : publication = publication_temp publication [ 'source' ] = self . tag # no meta since link already present source_ids = publication [ 'source_id' ] # todo check if actually anything was added source_ids . append ({ 'title' : 'Meta' }) publication [ 'source_id' ] = source_ids if type ( item ) is Event : item . data [ 'obj' ][ 'data' ] = publication self . pubfinder . finish_work ( item , self . tag ) get_response ( url , s ) get a response from a given url using a given session s, a session can be used for headers, this function is cached up to 100 elements !!! arguments url: the url to get s: the session to use Source code in src/meta_source.py @lru_cache ( maxsize = 100 ) def get_response ( url , s ): \"\"\"get a response from a given url using a given session s, a session can be used for headers, this function is cached up to 100 elements Arguments: url: the url to get s: the session to use \"\"\" return s . get ( url )","title":"meta source"},{"location":"meta_source_ref/#meta_source.MetaSource","text":"\" this source will try to append data using meta tags in the url of the resolved doi url","title":"MetaSource"},{"location":"meta_source_ref/#meta_source.MetaSource.__init__","text":"setup a ThreadPool, don't need the cpu since requests are slow and we wan't to share data Parameters: Name Type Description Default pubfinder the main process where we get data from and to required Source code in src/meta_source.py def __init__ ( self , pubfinder ): \"\"\"setup a ThreadPool, don't need the cpu since requests are slow and we wan't to share data Arguments: pubfinder: the main process where we get data from and to \"\"\" if not self . work_pool : self . work_pool = ThreadPool ( self . threads , self . worker , ()) self . pubfinder = pubfinder","title":"__init__()"},{"location":"meta_source_ref/#meta_source.MetaSource.add_data_to_publication","text":"add data to a given publication, only append, no overwriting if a value is already set Parameters: Name Type Description Default publication the publication to add data too required Source code in src/meta_source.py def add_data_to_publication ( self , publication ): \"\"\"add data to a given publication, only append, no overwriting if a value is already set Arguments: publication: the publication to add data too \"\"\" response = self . fetch ( publication [ 'doi' ]) data = self . get_lxml ( response ) return self . map ( data , publication )","title":"add_data_to_publication()"},{"location":"meta_source_ref/#meta_source.MetaSource.fetch","text":"fetch data from the source using its doi Parameters: Name Type Description Default doi the doi of the publication required Source code in src/meta_source.py def fetch ( self , doi ): \"\"\"fetch data from the source using its doi Arguments: doi: the doi of the publication \"\"\" session = Session () headers = { 'User-Agent' : 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_11_6) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/66.0.3359.181 Safari/537.36' , 'Pragma' : 'no-cache' } session . headers . update ( headers ) return get_response ( self . base_url + doi , session )","title":"fetch()"},{"location":"meta_source_ref/#meta_source.MetaSource.get_lxml","text":"use lxml to parse the page and create a data dict from this page Parameters: Name Type Description Default page the page required Source code in src/meta_source.py def get_lxml ( self , page ): \"\"\"use lxml to parse the page and create a data dict from this page Arguments: page: the page \"\"\" result = {} data = {} if not page : return None content = html . fromstring ( page . content ) # go through all meta tags in the head for meta in content . xpath ( '//html//meta' ): # iterate through for name , value in sorted ( meta . items ()): # abstracts if value . strip () . lower () in self . abstract_tags : result [ value . strip () . lower ()] = meta . get ( 'content' ) if value . strip () . lower () in self . title_tags : result [ value . strip () . lower ()] = meta . get ( 'content' ) if value . strip () . lower () in self . date_tags : result [ value . strip () . lower ()] = meta . get ( 'content' ) if value . strip () . lower () in self . year_tag : result [ value . strip () . lower ()] = meta . get ( 'content' ) if value . strip () . lower () in self . publisher_tags : result [ value . strip () . lower ()] = meta . get ( 'content' ) if value . strip () . lower () in self . type_tag : result [ value . strip () . lower ()] = meta . get ( 'content' ) if value . strip () . lower () in self . author_tags : result [ value . strip () . lower ()] = meta . get ( 'content' ) if value . strip () . lower () in self . author_tags : result [ value . strip () . lower ()] = meta . get ( 'content' ) if value . strip () . lower () in self . keyword_tag : result [ value . strip () . lower ()] = meta . get ( 'content' ) if value . strip () . lower () in self . citation_tag : result [ value . strip () . lower ()] = meta . get ( 'content' ) logging . debug ( self . log + \" could not resolve: \" + json . dumps ( result )) if 'abstract' not in data : for key in self . abstract_tags : if key in result : data [ 'abstract' ] = result [ key ] if 'title' not in data : for key in self . title_tags : if key in result : data [ 'title' ] = result [ key ] if 'pubDate' not in data : for key in self . date_tags : if key in result : dateTemp = result [ key ] . replace ( \"/\" , \"-\" ) data [ 'pubDate' ] = dateTemp if 'year' not in data : for key in self . year_tag : if key in result : data [ 'date' ] = result [ key ] if 'publisher' not in data : for key in self . publisher_tags : if key in result : data [ 'publisher' ] = result [ key ] if 'type' not in data : for key in self . type_tag : if key in result : data [ 'type' ] = result [ key ] if 'authors' not in data : authors = [] for key in self . author_tags : if key in result : authors . append ( result [ key ]) data [ 'authors' ] = authors if 'fieldsOfStudy' not in data : keywords = [] for key in self . keyword_tag : if key in result : keywords . append ( result [ key ]) data [ 'fieldsOfStudy' ] = keywords if 'citations' not in data : citations = [] for key in self . citation_tag : if key in result : citations . append ( result [ key ]) data [ 'citations' ] = citations return data","title":"get_lxml()"},{"location":"meta_source_ref/#meta_source.MetaSource.map","text":"map response data and the publication Parameters: Name Type Description Default response_data the response data required publication the publication required Source code in src/meta_source.py def map ( self , response_data , publication ): \"\"\"map response data and the publication Arguments: response_data: the response data publication: the publication \"\"\" # min length to use 50 if response_data and 'abstract' in response_data and ( 'abstract' not in publication or len ( publication [ 'abstract' ]) < 50 ): publication [ 'abstract' ] = response_data [ 'abstract' ] if response_data and 'title' in response_data and ( 'title' not in publication or len ( publication [ 'title' ]) < 5 ): publication [ 'title' ] = response_data [ 'title' ] if response_data and 'pubDate' in response_data and 'pubDate' not in publication : publication [ 'pubDate' ] = response_data [ 'pubDate' ] if response_data and 'year' in response_data and 'year' not in publication : publication [ 'year' ] = response_data [ 'year' ] if response_data and 'publisher' in response_data and 'publisher' not in publication : publication [ 'publisher' ] = response_data [ 'publisher' ] if response_data and 'type' in response_data and 'type' not in publication : publication [ 'type' ] = response_data [ 'type' ] if response_data and 'authors' in response_data and 'authors' not in publication : publication [ 'authors' ] = response_data [ 'authors' ] if response_data and 'fieldsOfStudy' in response_data and 'fieldsOfStudy' not in publication : publication [ 'fieldsOfStudy' ] = response_data [ 'fieldsOfStudy' ] if response_data and 'citations' in response_data and 'citationCount' not in publication : publication [ 'citationCount' ] = len ( response_data [ 'citations' ]) if response_data and 'citations' in response_data and 'citations' not in publication : publication [ 'citations' ] = response_data [ 'citations' ] return None","title":"map()"},{"location":"meta_source_ref/#meta_source.MetaSource.worker","text":"the worker thread function will ensure that the publications in the queue are all processed, it will sleep for 0.1s if no item is in the queue to reduce cpu usage Source code in src/meta_source.py def worker ( self ): \"\"\"the worker thread function will ensure that the publications in the queue are all processed, it will sleep for 0.1s if no item is in the queue to reduce cpu usage \"\"\" while self . running : try : item = self . work_queue . pop () except IndexError : time . sleep ( 0.1 ) pass else : if item : # logging.warning(self.log + \" item \" + str(item.get_json())) publication = self . pubfinder . get_publication ( item ) logging . warning ( self . log + \" work on item \" + publication [ 'doi' ]) # logging.warning(self.log + \" q \" + str(queue))x # source stuff publication_temp = self . add_data_to_publication ( publication ) # only if we have any data we set it if publication_temp : publication = publication_temp publication [ 'source' ] = self . tag # no meta since link already present source_ids = publication [ 'source_id' ] # todo check if actually anything was added source_ids . append ({ 'title' : 'Meta' }) publication [ 'source_id' ] = source_ids if type ( item ) is Event : item . data [ 'obj' ][ 'data' ] = publication self . pubfinder . finish_work ( item , self . tag )","title":"worker()"},{"location":"meta_source_ref/#meta_source.get_response","text":"get a response from a given url using a given session s, a session can be used for headers, this function is cached up to 100 elements !!! arguments url: the url to get s: the session to use Source code in src/meta_source.py @lru_cache ( maxsize = 100 ) def get_response ( url , s ): \"\"\"get a response from a given url using a given session s, a session can be used for headers, this function is cached up to 100 elements Arguments: url: the url to get s: the session to use \"\"\" return s . get ( url )","title":"get_response()"},{"location":"openaire_source_ref/","text":"OpenAireSource get_lxml ( self , page ) use lxml to parse the page and create a data dict from this page Parameters: Name Type Description Default page the page required Source code in src/openaire_source.py def get_lxml ( self , page ): \"\"\"use lxml to parse the page and create a data dict from this page Arguments: page: the page \"\"\" result = {} data = {} if not page : return None content = html . fromstring ( page . content ) # go through all meta tags in the head for meta in content . xpath ( '//html//head//meta' ): # iterate through for name , value in sorted ( meta . items ()): # abstracts if value . strip () . lower () in self . tags : result [ value . strip () . lower ()] = meta . get ( 'content' ) # logging.debug(self.log + \" could not resolve: \" + json.dumps(result)) # /response/results/result/metadata/oaf:entity/oaf:result/description if 'abstract' not in data : if 'description' in result : data [ 'abstract' ] = result [ 'description' ] # <title classid=\"main title\" classname=\"main title\" schemeid=\"dnet:dataCite_title\" schemename=\"dnet:dataCite_title\" inferred=\"false\" provenanceaction=\"sysimport:crosswalk:repository\" trust=\"0.9\">The origin of extracellular fields and currents \u2014 EEG, ECoG, LFP and spikes</title> # /response/results/result/metadata/oaf:entity/oaf:result/title[2] if 'title' not in data : for key in self . title_tags : if key in result : data [ 'title' ] = result [ key ] # /response/results/result/metadata/oaf:entity/oaf:result/dateofacceptance if 'pubDate' not in data : for key in self . date_tags : if key in result : data [ 'pubDate' ] = result [ key ] if 'year' not in data : for key in self . year_tag : if key in result : data [ 'date' ] = result [ key ] if 'publisher' not in data : for key in self . publisher_tags : if key in result : data [ 'publisher' ] = result [ key ] if 'type' not in data : for key in self . type_tag : if key in result : data [ 'type' ] = result [ key ] # /response/results/result/metadata/oaf:entity/oaf:result/creator if 'authors' not in data : authors = [] for key in self . author_tags : if key in result : authors . append ( result [ key ]) data [ 'authors' ] = authors # check that no trust value or context # /response/results/result/metadata/oaf:entity/oaf:result/subject if 'fieldsOfStudy' not in data : keywords = [] for key in self . keyword_tag : if key in result : keywords . append ( result [ key ]) data [ 'fieldsOfStudy' ] = keywords # try and find top level existing field of study and put their levels under it return data","title":"openaire source"},{"location":"openaire_source_ref/#openaire_source.OpenAireSource","text":"","title":"OpenAireSource"},{"location":"openaire_source_ref/#openaire_source.OpenAireSource.get_lxml","text":"use lxml to parse the page and create a data dict from this page Parameters: Name Type Description Default page the page required Source code in src/openaire_source.py def get_lxml ( self , page ): \"\"\"use lxml to parse the page and create a data dict from this page Arguments: page: the page \"\"\" result = {} data = {} if not page : return None content = html . fromstring ( page . content ) # go through all meta tags in the head for meta in content . xpath ( '//html//head//meta' ): # iterate through for name , value in sorted ( meta . items ()): # abstracts if value . strip () . lower () in self . tags : result [ value . strip () . lower ()] = meta . get ( 'content' ) # logging.debug(self.log + \" could not resolve: \" + json.dumps(result)) # /response/results/result/metadata/oaf:entity/oaf:result/description if 'abstract' not in data : if 'description' in result : data [ 'abstract' ] = result [ 'description' ] # <title classid=\"main title\" classname=\"main title\" schemeid=\"dnet:dataCite_title\" schemename=\"dnet:dataCite_title\" inferred=\"false\" provenanceaction=\"sysimport:crosswalk:repository\" trust=\"0.9\">The origin of extracellular fields and currents \u2014 EEG, ECoG, LFP and spikes</title> # /response/results/result/metadata/oaf:entity/oaf:result/title[2] if 'title' not in data : for key in self . title_tags : if key in result : data [ 'title' ] = result [ key ] # /response/results/result/metadata/oaf:entity/oaf:result/dateofacceptance if 'pubDate' not in data : for key in self . date_tags : if key in result : data [ 'pubDate' ] = result [ key ] if 'year' not in data : for key in self . year_tag : if key in result : data [ 'date' ] = result [ key ] if 'publisher' not in data : for key in self . publisher_tags : if key in result : data [ 'publisher' ] = result [ key ] if 'type' not in data : for key in self . type_tag : if key in result : data [ 'type' ] = result [ key ] # /response/results/result/metadata/oaf:entity/oaf:result/creator if 'authors' not in data : authors = [] for key in self . author_tags : if key in result : authors . append ( result [ key ]) data [ 'authors' ] = authors # check that no trust value or context # /response/results/result/metadata/oaf:entity/oaf:result/subject if 'fieldsOfStudy' not in data : keywords = [] for key in self . keyword_tag : if key in result : keywords . append ( result [ key ]) data [ 'fieldsOfStudy' ] = keywords # try and find top level existing field of study and put their levels under it return data","title":"get_lxml()"},{"location":"pubfinder_worker_ref/","text":"PubFinderWorker save_not_found ( self , event ) save thea not found event for debug Parameters: Name Type Description Default event Event the event to save required Source code in src/pubfinder_worker.py def save_not_found ( self , event : Event ): \"\"\" save thea not found event for debug Arguments: event: the event to save \"\"\" logging . debug ( 'save publication to mongo' ) try : event . data [ '_id' ] = event . data [ 'id' ] self . collectionFailed . insert_one ( event . data ) except pymongo . errors . DuplicateKeyError : logging . warning ( \"MongoDB, not found event\" )","title":"pubfinder worker"},{"location":"pubfinder_worker_ref/#pubfinder_worker.PubFinderWorker","text":"","title":"PubFinderWorker"},{"location":"pubfinder_worker_ref/#pubfinder_worker.PubFinderWorker.save_not_found","text":"save thea not found event for debug Parameters: Name Type Description Default event Event the event to save required Source code in src/pubfinder_worker.py def save_not_found ( self , event : Event ): \"\"\" save thea not found event for debug Arguments: event: the event to save \"\"\" logging . debug ( 'save publication to mongo' ) try : event . data [ '_id' ] = event . data [ 'id' ] self . collectionFailed . insert_one ( event . data ) except pymongo . errors . DuplicateKeyError : logging . warning ( \"MongoDB, not found event\" )","title":"save_not_found()"}]}