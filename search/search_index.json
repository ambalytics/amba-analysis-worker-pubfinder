{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"amba-analysis-worker-pubfinder The Pubfinder component retrieves publication metadata and, if possible, abstracts for publications that are linked to an event. The Pubfinder component is implemented in python by using the amba-event-streams package. The process of retrieving publication data can be seen in Figure 1. The Pubfinder is built around its main worker utilizing individual sources which are integrated similar. This architecture allows adding, remove or modify sources since their only difference is in the source retrieval and transformation. This means the Kafka and database connections are maintained by the main worker, while the sources are separated to only retrieve information for a given DOI. Process Overview of Retrieving Publication Data All sources have their own retrieval mechanisms and data transformations, as well as API limits that must be regarded. The following sources are used: ambalytics , CrossRef , OpenAIRE , Semantic Scholar and Meta Tags. CrossRef, OpenAIRE and Semantic Scholar are service that offer APIs, while Meta Tags are Dublin Core HTML header meta tags that may be embedded in the respective publication web page of the publisher. Table 1 shows the individual fields that can be retrieved by a source, as well as their rate limit and URL. Ambalytics is the internal database (see Figure 1), is technically not a real source and therefore not listed in the table. Even though CrossRef and Meta Tags are not rate limited, the number of requests per time unit is limited to comply with the recommended usage of the respective APIs. Since all sources are needed for a full check, the maximum throughput is limited by the slowest source or in this case, the source with the lowest rate limit. However, using this rate limit as base for the worst-case calculation 100 publications per 5 minutes will amount to 28800 per 24h period. Each source is integrated similarly to the main python program, but runs it own threads independently. In order to allow fast and parallel workloads, the process can either be implemented asynchronously allowing to use the await syntax to avoid blocking the thread or using queues and multi-threading. Unfortunately, the support of asynchronous python libraries is still limited and running blocking functions in asynchronous environment would defeat its workings. Following, the Pubfinder is developed based on the queue architecture pattern . Source CrossRef OpenAIRE Semantic Scholar Meta Tags URL api.crossref.org develop.openaire.eu api.semanticscholar.org - Rate Limit - 3600/h 100/5m - Title x x x x Authors x x x x Year x x x x Type x - - - Citation Count x - x x Pub Date x x - x Abstract x x x x References x - - - Citations x - x x Publisher x x x x Field Of Study x x x x License x - - - Publication Meta Data Sources Queues allow each source to run indecently and multi-threaded while reading and writing events into and from queues. A worker queue for each source contains all events, and all sources share a result queue. The queue is implemented as deque allowing for thread-safe operation and adding references and citations. Using a deque allows to add new DOI\u2019s to the front while adding references and citations to the end, resulting in earlier retrieval for the more important event publications. The management of the queues is done in the Pubfinder, checking for finished publications and adding an element to the correct source if more data is required. Since a publication resolution runs through each source until it is complete and implements different limits and speeds, the order is critical for overall speed. The API with the lowest rate limit should be last. This order of sources reduces the total request count affecting the limit since publications are more likely to be complete before and therefore reducing the count. Further, it will not slow down the processing. Each source uses the same base way of finding data. The given DOI will request the data in a defined way while ensuring API limits are kept. Once data is retrieved, the needed data is extracted and mapped into the internal publication structure. If a source adds data, it will also add its source. Finally, the publication data is added to the event, and the event is added to the result queue. Once a publication is complete or run through all sources and contains enough data, it is stored in PostgreSQL. Then publication data is added to the event, the event state is changed to linked , and finally, the event is sent to Kafka.","title":"Home"},{"location":"#amba-analysis-worker-pubfinder","text":"The Pubfinder component retrieves publication metadata and, if possible, abstracts for publications that are linked to an event. The Pubfinder component is implemented in python by using the amba-event-streams package. The process of retrieving publication data can be seen in Figure 1. The Pubfinder is built around its main worker utilizing individual sources which are integrated similar. This architecture allows adding, remove or modify sources since their only difference is in the source retrieval and transformation. This means the Kafka and database connections are maintained by the main worker, while the sources are separated to only retrieve information for a given DOI. Process Overview of Retrieving Publication Data All sources have their own retrieval mechanisms and data transformations, as well as API limits that must be regarded. The following sources are used: ambalytics , CrossRef , OpenAIRE , Semantic Scholar and Meta Tags. CrossRef, OpenAIRE and Semantic Scholar are service that offer APIs, while Meta Tags are Dublin Core HTML header meta tags that may be embedded in the respective publication web page of the publisher. Table 1 shows the individual fields that can be retrieved by a source, as well as their rate limit and URL. Ambalytics is the internal database (see Figure 1), is technically not a real source and therefore not listed in the table. Even though CrossRef and Meta Tags are not rate limited, the number of requests per time unit is limited to comply with the recommended usage of the respective APIs. Since all sources are needed for a full check, the maximum throughput is limited by the slowest source or in this case, the source with the lowest rate limit. However, using this rate limit as base for the worst-case calculation 100 publications per 5 minutes will amount to 28800 per 24h period. Each source is integrated similarly to the main python program, but runs it own threads independently. In order to allow fast and parallel workloads, the process can either be implemented asynchronously allowing to use the await syntax to avoid blocking the thread or using queues and multi-threading. Unfortunately, the support of asynchronous python libraries is still limited and running blocking functions in asynchronous environment would defeat its workings. Following, the Pubfinder is developed based on the queue architecture pattern . Source CrossRef OpenAIRE Semantic Scholar Meta Tags URL api.crossref.org develop.openaire.eu api.semanticscholar.org - Rate Limit - 3600/h 100/5m - Title x x x x Authors x x x x Year x x x x Type x - - - Citation Count x - x x Pub Date x x - x Abstract x x x x References x - - - Citations x - x x Publisher x x x x Field Of Study x x x x License x - - - Publication Meta Data Sources Queues allow each source to run indecently and multi-threaded while reading and writing events into and from queues. A worker queue for each source contains all events, and all sources share a result queue. The queue is implemented as deque allowing for thread-safe operation and adding references and citations. Using a deque allows to add new DOI\u2019s to the front while adding references and citations to the end, resulting in earlier retrieval for the more important event publications. The management of the queues is done in the Pubfinder, checking for finished publications and adding an element to the correct source if more data is required. Since a publication resolution runs through each source until it is complete and implements different limits and speeds, the order is critical for overall speed. The API with the lowest rate limit should be last. This order of sources reduces the total request count affecting the limit since publications are more likely to be complete before and therefore reducing the count. Further, it will not slow down the processing. Each source uses the same base way of finding data. The given DOI will request the data in a defined way while ensuring API limits are kept. Once data is retrieved, the needed data is extracted and mapped into the internal publication structure. If a source adds data, it will also add its source. Finally, the publication data is added to the event, and the event is added to the result queue. Once a publication is complete or run through all sources and contains enough data, it is stored in PostgreSQL. Then publication data is added to the event, the event state is changed to linked , and finally, the event is sent to Kafka.","title":"amba-analysis-worker-pubfinder"},{"location":"amba_source_ref/","text":"AmbaSource Source code in src/amba_source.py class AmbaSource ( object ): tag = 'amba' log = 'SourceAmba' threads = 3 url = \"https://api.ambalytics.cloud/entities\" work_queue = deque () work_pool = None running = True def __init__ ( self , result_deque ): if not self . work_pool : self . work_pool = ThreadPool ( self . threads , self . worker , ()) self . result_deque = result_deque def worker ( self ): \"\"\" worker function run in thread pool adding publication data \"\"\" amba_client = self . prepare_amba_connection () while self . running : try : item = self . work_queue . pop () except IndexError : time . sleep ( 0.1 ) pass else : if item : publication = PubFinderHelper . get_publication ( item ) logging . warning ( self . log + \" work on item \" + publication [ 'doi' ]) # publication_temp = self.add_data_to_publication(publication, amba_client) if False : # publication_temp: publication = publication_temp publication [ 'source' ] = self . tag source_ids = [] if 'source_id' in publication : source_ids = publication [ 'source_id' ] source_ids . append ({ 'title' : 'Ambalytics' , 'url' : 'https://ambalytics.com' , 'license' : 'MIT' }) publication [ 'source_id' ] = source_ids if False : # type(item) is Event: item . data [ 'obj' ][ 'data' ] = publication result = { 'item' : item , 'tag' : self . tag } self . result_deque . append ( result ) def add_data_to_publication ( self , publication , ac ): \"\"\" add data from amba to publication using a supplied amba client \"\"\" amba_publication = self . get_publication_wrapper ( publication [ 'doi' ], ac ) if not amba_publication : return publication else : return amba_publication @staticmethod def get_publication_wrapper ( doi , ac ): \"\"\"just a wrapper\"\"\" return get_publication_from_amba ( doi , ac ) def prepare_amba_connection ( self ): \"\"\"prepare the connection to amba\"\"\" transport = RequestsHTTPTransport ( url = self . url , verify = False , retries = 1 ) return Client ( transport = transport , fetch_schema_from_transport = True ) add_data_to_publication ( self , publication , ac ) add data from amba to publication using a supplied amba client Source code in src/amba_source.py def add_data_to_publication ( self , publication , ac ): \"\"\" add data from amba to publication using a supplied amba client \"\"\" amba_publication = self . get_publication_wrapper ( publication [ 'doi' ], ac ) if not amba_publication : return publication else : return amba_publication get_publication_wrapper ( doi , ac ) staticmethod just a wrapper Source code in src/amba_source.py @staticmethod def get_publication_wrapper ( doi , ac ): \"\"\"just a wrapper\"\"\" return get_publication_from_amba ( doi , ac ) prepare_amba_connection ( self ) prepare the connection to amba Source code in src/amba_source.py def prepare_amba_connection ( self ): \"\"\"prepare the connection to amba\"\"\" transport = RequestsHTTPTransport ( url = self . url , verify = False , retries = 1 ) return Client ( transport = transport , fetch_schema_from_transport = True ) worker ( self ) worker function run in thread pool adding publication data Source code in src/amba_source.py def worker ( self ): \"\"\" worker function run in thread pool adding publication data \"\"\" amba_client = self . prepare_amba_connection () while self . running : try : item = self . work_queue . pop () except IndexError : time . sleep ( 0.1 ) pass else : if item : publication = PubFinderHelper . get_publication ( item ) logging . warning ( self . log + \" work on item \" + publication [ 'doi' ]) # publication_temp = self.add_data_to_publication(publication, amba_client) if False : # publication_temp: publication = publication_temp publication [ 'source' ] = self . tag source_ids = [] if 'source_id' in publication : source_ids = publication [ 'source_id' ] source_ids . append ({ 'title' : 'Ambalytics' , 'url' : 'https://ambalytics.com' , 'license' : 'MIT' }) publication [ 'source_id' ] = source_ids if False : # type(item) is Event: item . data [ 'obj' ][ 'data' ] = publication result = { 'item' : item , 'tag' : self . tag } self . result_deque . append ( result )","title":"amba source"},{"location":"amba_source_ref/#amba_source.AmbaSource","text":"Source code in src/amba_source.py class AmbaSource ( object ): tag = 'amba' log = 'SourceAmba' threads = 3 url = \"https://api.ambalytics.cloud/entities\" work_queue = deque () work_pool = None running = True def __init__ ( self , result_deque ): if not self . work_pool : self . work_pool = ThreadPool ( self . threads , self . worker , ()) self . result_deque = result_deque def worker ( self ): \"\"\" worker function run in thread pool adding publication data \"\"\" amba_client = self . prepare_amba_connection () while self . running : try : item = self . work_queue . pop () except IndexError : time . sleep ( 0.1 ) pass else : if item : publication = PubFinderHelper . get_publication ( item ) logging . warning ( self . log + \" work on item \" + publication [ 'doi' ]) # publication_temp = self.add_data_to_publication(publication, amba_client) if False : # publication_temp: publication = publication_temp publication [ 'source' ] = self . tag source_ids = [] if 'source_id' in publication : source_ids = publication [ 'source_id' ] source_ids . append ({ 'title' : 'Ambalytics' , 'url' : 'https://ambalytics.com' , 'license' : 'MIT' }) publication [ 'source_id' ] = source_ids if False : # type(item) is Event: item . data [ 'obj' ][ 'data' ] = publication result = { 'item' : item , 'tag' : self . tag } self . result_deque . append ( result ) def add_data_to_publication ( self , publication , ac ): \"\"\" add data from amba to publication using a supplied amba client \"\"\" amba_publication = self . get_publication_wrapper ( publication [ 'doi' ], ac ) if not amba_publication : return publication else : return amba_publication @staticmethod def get_publication_wrapper ( doi , ac ): \"\"\"just a wrapper\"\"\" return get_publication_from_amba ( doi , ac ) def prepare_amba_connection ( self ): \"\"\"prepare the connection to amba\"\"\" transport = RequestsHTTPTransport ( url = self . url , verify = False , retries = 1 ) return Client ( transport = transport , fetch_schema_from_transport = True )","title":"AmbaSource"},{"location":"amba_source_ref/#amba_source.AmbaSource.add_data_to_publication","text":"add data from amba to publication using a supplied amba client Source code in src/amba_source.py def add_data_to_publication ( self , publication , ac ): \"\"\" add data from amba to publication using a supplied amba client \"\"\" amba_publication = self . get_publication_wrapper ( publication [ 'doi' ], ac ) if not amba_publication : return publication else : return amba_publication","title":"add_data_to_publication()"},{"location":"amba_source_ref/#amba_source.AmbaSource.get_publication_wrapper","text":"just a wrapper Source code in src/amba_source.py @staticmethod def get_publication_wrapper ( doi , ac ): \"\"\"just a wrapper\"\"\" return get_publication_from_amba ( doi , ac )","title":"get_publication_wrapper()"},{"location":"amba_source_ref/#amba_source.AmbaSource.prepare_amba_connection","text":"prepare the connection to amba Source code in src/amba_source.py def prepare_amba_connection ( self ): \"\"\"prepare the connection to amba\"\"\" transport = RequestsHTTPTransport ( url = self . url , verify = False , retries = 1 ) return Client ( transport = transport , fetch_schema_from_transport = True )","title":"prepare_amba_connection()"},{"location":"amba_source_ref/#amba_source.AmbaSource.worker","text":"worker function run in thread pool adding publication data Source code in src/amba_source.py def worker ( self ): \"\"\" worker function run in thread pool adding publication data \"\"\" amba_client = self . prepare_amba_connection () while self . running : try : item = self . work_queue . pop () except IndexError : time . sleep ( 0.1 ) pass else : if item : publication = PubFinderHelper . get_publication ( item ) logging . warning ( self . log + \" work on item \" + publication [ 'doi' ]) # publication_temp = self.add_data_to_publication(publication, amba_client) if False : # publication_temp: publication = publication_temp publication [ 'source' ] = self . tag source_ids = [] if 'source_id' in publication : source_ids = publication [ 'source_id' ] source_ids . append ({ 'title' : 'Ambalytics' , 'url' : 'https://ambalytics.com' , 'license' : 'MIT' }) publication [ 'source_id' ] = source_ids if False : # type(item) is Event: item . data [ 'obj' ][ 'data' ] = publication result = { 'item' : item , 'tag' : self . tag } self . result_deque . append ( result )","title":"worker()"},{"location":"crossref_source_ref/","text":"CrossrefSource Source code in src/crossref_source.py class CrossrefSource ( object ): base_url = \"https://api.crossref.org/works/\" publication_type_translation = { 'unknown' : 'UNKNOWN' , 'book' : 'BOOK' , 'book-chapter' : 'BOOK_CHAPTER' , 'proceedings-article' : 'CONFERENCE_PAPER' , 'dataset' : 'DATASET' , 'journal-article' : 'JOURNAL_ARTICLE' , 'patent' : 'PATENT' , 'repository' : 'REPOSITORY' , 'reference-book' : 'BOOK_REFERENCE_ENTRY' } tag = 'crossref' log = 'SourceCrossref' work_queue = deque () work_pool = None running = True threads = 4 def __init__ ( self , result_deque ): if not self . work_pool : self . work_pool = ThreadPool ( self . threads , self . worker , ()) self . result_deque = result_deque self . cleanr = re . compile ( '<.*?>|&([a-z0-9]+|#[0-9]{1,6}|#x[0-9a-f]{1,6});' ) def worker ( self ): \"\"\" worker function run in thread pool adding publication data \"\"\" while self . running : try : item = self . work_queue . pop () except IndexError : time . sleep ( 0.1 ) pass else : if item : publication = PubFinderHelper . get_publication ( item ) logging . warning ( self . log + \" work on item \" + publication [ 'doi' ]) publication_temp = self . add_data_to_publication ( publication ) if publication_temp : publication = publication_temp if type ( item ) is Event : item . data [ 'obj' ][ 'data' ] = publication result = { 'item' : item , 'tag' : self . tag } self . result_deque . append ( result ) def add_data_to_publication ( self , publication ): \"\"\" fetch and map to add data \"\"\" response = fetch ( publication [ 'doi' ]) return self . map ( response , publication ) def map ( self , response_data , publication ): \"\"\" map given data from a response to a publication object \"\"\" added_data = False if response_data : if 'type' not in publication : if response_data [ 'type' ] in self . publication_type_translation : publication [ 'type' ] = self . publication_type_translation [ response_data [ 'type' ]] else : publication [ 'type' ] = self . publication_type_translation [ 'unknown' ] added_data = True if 'published' in response_data and 'date-parts' in response_data [ 'published' ] and 'pub_date' not in publication : if len ( response_data [ 'published' ][ 'date-parts' ][ 0 ]) == 3 : publication [ 'pub_date' ] = ' {0} - {1} - {2} ' . format ( str ( response_data [ 'published' ][ 'date-parts' ][ 0 ][ 0 ]), str ( response_data [ 'published' ][ 'date-parts' ][ 0 ][ 1 ]), str ( response_data [ 'published' ][ 'date-parts' ][ 0 ][ 2 ])) publication [ 'year' ] = response_data [ 'published' ][ 'date-parts' ][ 0 ][ 0 ] if PubFinderHelper . should_update ( 'publisher' , response_data , publication ): publication [ 'publisher' ] = response_data [ 'publisher' ] if 'is-referenced-by-count' in response_data and ( 'citation_count' not in publication or publication [ 'citation_count' ] == 0 ): publication [ 'citation_count' ] = response_data [ 'is-referenced-by-count' ] if PubFinderHelper . should_update ( 'title' , response_data , publication ): if len ( response_data [ 'title' ]) > 0 : publication [ 'title' ] = PubFinderHelper . clean_title ( response_data [ 'title' ][ 0 ]) publication [ 'normalized_title' ] = PubFinderHelper . normalize ( publication [ 'title' ]) if 'reference' in response_data and 'refs' not in publication : publication [ 'refs' ] = self . map_refs ( response_data [ 'reference' ]) added_data = True if 'abstract' in response_data and ( 'abstract' not in publication or not PubFinderHelper . valid_abstract ( publication [ 'abstract' ])): abstract = PubFinderHelper . clean_abstract ( response_data [ 'abstract' ]) if PubFinderHelper . valid_abstract ( abstract ): publication [ 'abstract' ] = abstract added_data = True if 'author' in response_data and 'authors' not in publication : publication [ 'authors' ] = self . map_author ( response_data [ 'author' ]) added_data = True if 'subject' in response_data and 'fields_of_study' not in publication : publication [ 'fields_of_study' ] = self . map_fields_of_study ( response_data [ 'subject' ]) added_data = True if 'license' in response_data : publication [ 'license' ] = response_data [ 'license' ][ 0 ][ 'URL' ] added_data = True if added_data : source_ids = publication [ 'source_id' ] source_ids . append ({ 'title' : 'Crossref' , 'url' : 'https://www.crossref.org/' , 'license' : 'TODO' }) publication [ 'source_id' ] = source_ids return publication def map_author ( self , authors ): \"\"\" map authors and add normalized \"\"\" result = [] for author in authors : name = '' if 'given' in author : name = author [ 'given' ] + ' ' else : logging . warning ( self . log + ' no author given ' + json . dumps ( author )) if 'family' in author : name = name + author [ 'family' ] else : logging . warning ( self . log + ' no author family ' + json . dumps ( author )) if len ( name . strip ()) > 1 : normalized_name = PubFinderHelper . normalize ( name ) result . append ({ 'name' : name , 'normalized_name' : normalized_name }) return result def map_refs ( self , refs ): \"\"\" map references \"\"\" result = [] for ref in refs : if 'DOI' in ref : result . append ({ 'doi' : ref [ 'DOI' ]}) return result def map_fields_of_study ( self , fields ): \"\"\" map field of study and add normalized \"\"\" result = [] for field in fields : name = re . sub ( r \"[\\(\\[].*?[\\)\\]]\" , \"\" , field ) normalized_name = PubFinderHelper . normalize ( name ) if not any ( d [ 'normalized_name' ] == normalized_name for d in result ): result . append ({ 'name' : name , 'normalized_name' : normalized_name }) return result add_data_to_publication ( self , publication ) fetch and map to add data Source code in src/crossref_source.py def add_data_to_publication ( self , publication ): \"\"\" fetch and map to add data \"\"\" response = fetch ( publication [ 'doi' ]) return self . map ( response , publication ) map ( self , response_data , publication ) map given data from a response to a publication object Source code in src/crossref_source.py def map ( self , response_data , publication ): \"\"\" map given data from a response to a publication object \"\"\" added_data = False if response_data : if 'type' not in publication : if response_data [ 'type' ] in self . publication_type_translation : publication [ 'type' ] = self . publication_type_translation [ response_data [ 'type' ]] else : publication [ 'type' ] = self . publication_type_translation [ 'unknown' ] added_data = True if 'published' in response_data and 'date-parts' in response_data [ 'published' ] and 'pub_date' not in publication : if len ( response_data [ 'published' ][ 'date-parts' ][ 0 ]) == 3 : publication [ 'pub_date' ] = ' {0} - {1} - {2} ' . format ( str ( response_data [ 'published' ][ 'date-parts' ][ 0 ][ 0 ]), str ( response_data [ 'published' ][ 'date-parts' ][ 0 ][ 1 ]), str ( response_data [ 'published' ][ 'date-parts' ][ 0 ][ 2 ])) publication [ 'year' ] = response_data [ 'published' ][ 'date-parts' ][ 0 ][ 0 ] if PubFinderHelper . should_update ( 'publisher' , response_data , publication ): publication [ 'publisher' ] = response_data [ 'publisher' ] if 'is-referenced-by-count' in response_data and ( 'citation_count' not in publication or publication [ 'citation_count' ] == 0 ): publication [ 'citation_count' ] = response_data [ 'is-referenced-by-count' ] if PubFinderHelper . should_update ( 'title' , response_data , publication ): if len ( response_data [ 'title' ]) > 0 : publication [ 'title' ] = PubFinderHelper . clean_title ( response_data [ 'title' ][ 0 ]) publication [ 'normalized_title' ] = PubFinderHelper . normalize ( publication [ 'title' ]) if 'reference' in response_data and 'refs' not in publication : publication [ 'refs' ] = self . map_refs ( response_data [ 'reference' ]) added_data = True if 'abstract' in response_data and ( 'abstract' not in publication or not PubFinderHelper . valid_abstract ( publication [ 'abstract' ])): abstract = PubFinderHelper . clean_abstract ( response_data [ 'abstract' ]) if PubFinderHelper . valid_abstract ( abstract ): publication [ 'abstract' ] = abstract added_data = True if 'author' in response_data and 'authors' not in publication : publication [ 'authors' ] = self . map_author ( response_data [ 'author' ]) added_data = True if 'subject' in response_data and 'fields_of_study' not in publication : publication [ 'fields_of_study' ] = self . map_fields_of_study ( response_data [ 'subject' ]) added_data = True if 'license' in response_data : publication [ 'license' ] = response_data [ 'license' ][ 0 ][ 'URL' ] added_data = True if added_data : source_ids = publication [ 'source_id' ] source_ids . append ({ 'title' : 'Crossref' , 'url' : 'https://www.crossref.org/' , 'license' : 'TODO' }) publication [ 'source_id' ] = source_ids return publication map_author ( self , authors ) map authors and add normalized Source code in src/crossref_source.py def map_author ( self , authors ): \"\"\" map authors and add normalized \"\"\" result = [] for author in authors : name = '' if 'given' in author : name = author [ 'given' ] + ' ' else : logging . warning ( self . log + ' no author given ' + json . dumps ( author )) if 'family' in author : name = name + author [ 'family' ] else : logging . warning ( self . log + ' no author family ' + json . dumps ( author )) if len ( name . strip ()) > 1 : normalized_name = PubFinderHelper . normalize ( name ) result . append ({ 'name' : name , 'normalized_name' : normalized_name }) return result map_fields_of_study ( self , fields ) map field of study and add normalized Source code in src/crossref_source.py def map_fields_of_study ( self , fields ): \"\"\" map field of study and add normalized \"\"\" result = [] for field in fields : name = re . sub ( r \"[\\(\\[].*?[\\)\\]]\" , \"\" , field ) normalized_name = PubFinderHelper . normalize ( name ) if not any ( d [ 'normalized_name' ] == normalized_name for d in result ): result . append ({ 'name' : name , 'normalized_name' : normalized_name }) return result map_refs ( self , refs ) map references Source code in src/crossref_source.py def map_refs ( self , refs ): \"\"\" map references \"\"\" result = [] for ref in refs : if 'DOI' in ref : result . append ({ 'doi' : ref [ 'DOI' ]}) return result worker ( self ) worker function run in thread pool adding publication data Source code in src/crossref_source.py def worker ( self ): \"\"\" worker function run in thread pool adding publication data \"\"\" while self . running : try : item = self . work_queue . pop () except IndexError : time . sleep ( 0.1 ) pass else : if item : publication = PubFinderHelper . get_publication ( item ) logging . warning ( self . log + \" work on item \" + publication [ 'doi' ]) publication_temp = self . add_data_to_publication ( publication ) if publication_temp : publication = publication_temp if type ( item ) is Event : item . data [ 'obj' ][ 'data' ] = publication result = { 'item' : item , 'tag' : self . tag } self . result_deque . append ( result ) fetch ( doi ) fetch response to add data to publication cache up to 100 since we should not have doi be occurring multiple times Parameters: Name Type Description Default doi the doi to be fetched required Source code in src/crossref_source.py @lru_cache ( maxsize = 10 ) def fetch ( doi ): \"\"\"fetch response to add data to publication cache up to 100 since we should not have doi be occurring multiple times Arguments: doi: the doi to be fetched \"\"\" r = requests . get ( CrossrefSource . base_url + requests . utils . quote ( doi ) + '?mailto=lukas.jesche.se@gmail.com' ) if r . status_code == 200 : json_response = r . json () if 'status' in json_response : if json_response [ 'status' ] == 'ok' : if 'message' in json_response : return json_response [ 'message' ] return None","title":"crossref source"},{"location":"crossref_source_ref/#crossref_source.CrossrefSource","text":"Source code in src/crossref_source.py class CrossrefSource ( object ): base_url = \"https://api.crossref.org/works/\" publication_type_translation = { 'unknown' : 'UNKNOWN' , 'book' : 'BOOK' , 'book-chapter' : 'BOOK_CHAPTER' , 'proceedings-article' : 'CONFERENCE_PAPER' , 'dataset' : 'DATASET' , 'journal-article' : 'JOURNAL_ARTICLE' , 'patent' : 'PATENT' , 'repository' : 'REPOSITORY' , 'reference-book' : 'BOOK_REFERENCE_ENTRY' } tag = 'crossref' log = 'SourceCrossref' work_queue = deque () work_pool = None running = True threads = 4 def __init__ ( self , result_deque ): if not self . work_pool : self . work_pool = ThreadPool ( self . threads , self . worker , ()) self . result_deque = result_deque self . cleanr = re . compile ( '<.*?>|&([a-z0-9]+|#[0-9]{1,6}|#x[0-9a-f]{1,6});' ) def worker ( self ): \"\"\" worker function run in thread pool adding publication data \"\"\" while self . running : try : item = self . work_queue . pop () except IndexError : time . sleep ( 0.1 ) pass else : if item : publication = PubFinderHelper . get_publication ( item ) logging . warning ( self . log + \" work on item \" + publication [ 'doi' ]) publication_temp = self . add_data_to_publication ( publication ) if publication_temp : publication = publication_temp if type ( item ) is Event : item . data [ 'obj' ][ 'data' ] = publication result = { 'item' : item , 'tag' : self . tag } self . result_deque . append ( result ) def add_data_to_publication ( self , publication ): \"\"\" fetch and map to add data \"\"\" response = fetch ( publication [ 'doi' ]) return self . map ( response , publication ) def map ( self , response_data , publication ): \"\"\" map given data from a response to a publication object \"\"\" added_data = False if response_data : if 'type' not in publication : if response_data [ 'type' ] in self . publication_type_translation : publication [ 'type' ] = self . publication_type_translation [ response_data [ 'type' ]] else : publication [ 'type' ] = self . publication_type_translation [ 'unknown' ] added_data = True if 'published' in response_data and 'date-parts' in response_data [ 'published' ] and 'pub_date' not in publication : if len ( response_data [ 'published' ][ 'date-parts' ][ 0 ]) == 3 : publication [ 'pub_date' ] = ' {0} - {1} - {2} ' . format ( str ( response_data [ 'published' ][ 'date-parts' ][ 0 ][ 0 ]), str ( response_data [ 'published' ][ 'date-parts' ][ 0 ][ 1 ]), str ( response_data [ 'published' ][ 'date-parts' ][ 0 ][ 2 ])) publication [ 'year' ] = response_data [ 'published' ][ 'date-parts' ][ 0 ][ 0 ] if PubFinderHelper . should_update ( 'publisher' , response_data , publication ): publication [ 'publisher' ] = response_data [ 'publisher' ] if 'is-referenced-by-count' in response_data and ( 'citation_count' not in publication or publication [ 'citation_count' ] == 0 ): publication [ 'citation_count' ] = response_data [ 'is-referenced-by-count' ] if PubFinderHelper . should_update ( 'title' , response_data , publication ): if len ( response_data [ 'title' ]) > 0 : publication [ 'title' ] = PubFinderHelper . clean_title ( response_data [ 'title' ][ 0 ]) publication [ 'normalized_title' ] = PubFinderHelper . normalize ( publication [ 'title' ]) if 'reference' in response_data and 'refs' not in publication : publication [ 'refs' ] = self . map_refs ( response_data [ 'reference' ]) added_data = True if 'abstract' in response_data and ( 'abstract' not in publication or not PubFinderHelper . valid_abstract ( publication [ 'abstract' ])): abstract = PubFinderHelper . clean_abstract ( response_data [ 'abstract' ]) if PubFinderHelper . valid_abstract ( abstract ): publication [ 'abstract' ] = abstract added_data = True if 'author' in response_data and 'authors' not in publication : publication [ 'authors' ] = self . map_author ( response_data [ 'author' ]) added_data = True if 'subject' in response_data and 'fields_of_study' not in publication : publication [ 'fields_of_study' ] = self . map_fields_of_study ( response_data [ 'subject' ]) added_data = True if 'license' in response_data : publication [ 'license' ] = response_data [ 'license' ][ 0 ][ 'URL' ] added_data = True if added_data : source_ids = publication [ 'source_id' ] source_ids . append ({ 'title' : 'Crossref' , 'url' : 'https://www.crossref.org/' , 'license' : 'TODO' }) publication [ 'source_id' ] = source_ids return publication def map_author ( self , authors ): \"\"\" map authors and add normalized \"\"\" result = [] for author in authors : name = '' if 'given' in author : name = author [ 'given' ] + ' ' else : logging . warning ( self . log + ' no author given ' + json . dumps ( author )) if 'family' in author : name = name + author [ 'family' ] else : logging . warning ( self . log + ' no author family ' + json . dumps ( author )) if len ( name . strip ()) > 1 : normalized_name = PubFinderHelper . normalize ( name ) result . append ({ 'name' : name , 'normalized_name' : normalized_name }) return result def map_refs ( self , refs ): \"\"\" map references \"\"\" result = [] for ref in refs : if 'DOI' in ref : result . append ({ 'doi' : ref [ 'DOI' ]}) return result def map_fields_of_study ( self , fields ): \"\"\" map field of study and add normalized \"\"\" result = [] for field in fields : name = re . sub ( r \"[\\(\\[].*?[\\)\\]]\" , \"\" , field ) normalized_name = PubFinderHelper . normalize ( name ) if not any ( d [ 'normalized_name' ] == normalized_name for d in result ): result . append ({ 'name' : name , 'normalized_name' : normalized_name }) return result","title":"CrossrefSource"},{"location":"crossref_source_ref/#crossref_source.CrossrefSource.add_data_to_publication","text":"fetch and map to add data Source code in src/crossref_source.py def add_data_to_publication ( self , publication ): \"\"\" fetch and map to add data \"\"\" response = fetch ( publication [ 'doi' ]) return self . map ( response , publication )","title":"add_data_to_publication()"},{"location":"crossref_source_ref/#crossref_source.CrossrefSource.map","text":"map given data from a response to a publication object Source code in src/crossref_source.py def map ( self , response_data , publication ): \"\"\" map given data from a response to a publication object \"\"\" added_data = False if response_data : if 'type' not in publication : if response_data [ 'type' ] in self . publication_type_translation : publication [ 'type' ] = self . publication_type_translation [ response_data [ 'type' ]] else : publication [ 'type' ] = self . publication_type_translation [ 'unknown' ] added_data = True if 'published' in response_data and 'date-parts' in response_data [ 'published' ] and 'pub_date' not in publication : if len ( response_data [ 'published' ][ 'date-parts' ][ 0 ]) == 3 : publication [ 'pub_date' ] = ' {0} - {1} - {2} ' . format ( str ( response_data [ 'published' ][ 'date-parts' ][ 0 ][ 0 ]), str ( response_data [ 'published' ][ 'date-parts' ][ 0 ][ 1 ]), str ( response_data [ 'published' ][ 'date-parts' ][ 0 ][ 2 ])) publication [ 'year' ] = response_data [ 'published' ][ 'date-parts' ][ 0 ][ 0 ] if PubFinderHelper . should_update ( 'publisher' , response_data , publication ): publication [ 'publisher' ] = response_data [ 'publisher' ] if 'is-referenced-by-count' in response_data and ( 'citation_count' not in publication or publication [ 'citation_count' ] == 0 ): publication [ 'citation_count' ] = response_data [ 'is-referenced-by-count' ] if PubFinderHelper . should_update ( 'title' , response_data , publication ): if len ( response_data [ 'title' ]) > 0 : publication [ 'title' ] = PubFinderHelper . clean_title ( response_data [ 'title' ][ 0 ]) publication [ 'normalized_title' ] = PubFinderHelper . normalize ( publication [ 'title' ]) if 'reference' in response_data and 'refs' not in publication : publication [ 'refs' ] = self . map_refs ( response_data [ 'reference' ]) added_data = True if 'abstract' in response_data and ( 'abstract' not in publication or not PubFinderHelper . valid_abstract ( publication [ 'abstract' ])): abstract = PubFinderHelper . clean_abstract ( response_data [ 'abstract' ]) if PubFinderHelper . valid_abstract ( abstract ): publication [ 'abstract' ] = abstract added_data = True if 'author' in response_data and 'authors' not in publication : publication [ 'authors' ] = self . map_author ( response_data [ 'author' ]) added_data = True if 'subject' in response_data and 'fields_of_study' not in publication : publication [ 'fields_of_study' ] = self . map_fields_of_study ( response_data [ 'subject' ]) added_data = True if 'license' in response_data : publication [ 'license' ] = response_data [ 'license' ][ 0 ][ 'URL' ] added_data = True if added_data : source_ids = publication [ 'source_id' ] source_ids . append ({ 'title' : 'Crossref' , 'url' : 'https://www.crossref.org/' , 'license' : 'TODO' }) publication [ 'source_id' ] = source_ids return publication","title":"map()"},{"location":"crossref_source_ref/#crossref_source.CrossrefSource.map_author","text":"map authors and add normalized Source code in src/crossref_source.py def map_author ( self , authors ): \"\"\" map authors and add normalized \"\"\" result = [] for author in authors : name = '' if 'given' in author : name = author [ 'given' ] + ' ' else : logging . warning ( self . log + ' no author given ' + json . dumps ( author )) if 'family' in author : name = name + author [ 'family' ] else : logging . warning ( self . log + ' no author family ' + json . dumps ( author )) if len ( name . strip ()) > 1 : normalized_name = PubFinderHelper . normalize ( name ) result . append ({ 'name' : name , 'normalized_name' : normalized_name }) return result","title":"map_author()"},{"location":"crossref_source_ref/#crossref_source.CrossrefSource.map_fields_of_study","text":"map field of study and add normalized Source code in src/crossref_source.py def map_fields_of_study ( self , fields ): \"\"\" map field of study and add normalized \"\"\" result = [] for field in fields : name = re . sub ( r \"[\\(\\[].*?[\\)\\]]\" , \"\" , field ) normalized_name = PubFinderHelper . normalize ( name ) if not any ( d [ 'normalized_name' ] == normalized_name for d in result ): result . append ({ 'name' : name , 'normalized_name' : normalized_name }) return result","title":"map_fields_of_study()"},{"location":"crossref_source_ref/#crossref_source.CrossrefSource.map_refs","text":"map references Source code in src/crossref_source.py def map_refs ( self , refs ): \"\"\" map references \"\"\" result = [] for ref in refs : if 'DOI' in ref : result . append ({ 'doi' : ref [ 'DOI' ]}) return result","title":"map_refs()"},{"location":"crossref_source_ref/#crossref_source.CrossrefSource.worker","text":"worker function run in thread pool adding publication data Source code in src/crossref_source.py def worker ( self ): \"\"\" worker function run in thread pool adding publication data \"\"\" while self . running : try : item = self . work_queue . pop () except IndexError : time . sleep ( 0.1 ) pass else : if item : publication = PubFinderHelper . get_publication ( item ) logging . warning ( self . log + \" work on item \" + publication [ 'doi' ]) publication_temp = self . add_data_to_publication ( publication ) if publication_temp : publication = publication_temp if type ( item ) is Event : item . data [ 'obj' ][ 'data' ] = publication result = { 'item' : item , 'tag' : self . tag } self . result_deque . append ( result )","title":"worker()"},{"location":"crossref_source_ref/#crossref_source.fetch","text":"fetch response to add data to publication cache up to 100 since we should not have doi be occurring multiple times Parameters: Name Type Description Default doi the doi to be fetched required Source code in src/crossref_source.py @lru_cache ( maxsize = 10 ) def fetch ( doi ): \"\"\"fetch response to add data to publication cache up to 100 since we should not have doi be occurring multiple times Arguments: doi: the doi to be fetched \"\"\" r = requests . get ( CrossrefSource . base_url + requests . utils . quote ( doi ) + '?mailto=lukas.jesche.se@gmail.com' ) if r . status_code == 200 : json_response = r . json () if 'status' in json_response : if json_response [ 'status' ] == 'ok' : if 'message' in json_response : return json_response [ 'message' ] return None","title":"fetch()"},{"location":"meta_source_ref/","text":"MetaSource \" this source will try to append data using meta tags in the url of the resolved doi url Source code in src/meta_source.py class MetaSource ( object ): \"\"\"\" this source will try to append data using meta tags in the url of the resolved doi url \"\"\" base_url = \"http://doi.org/\" # tag must be ordered from better to worst, as soon as a result is found it will stop abstract_tags = [ 'dcterms.abstract' , 'dcterms.description' , 'prism.teaser' , 'eprints.abstract' , 'og:description' , 'dc.description' , 'description' , 'twitter:description' , 'citation_abstract' ] title_tags = [ 'og:title' , 'dc.title' , 'citation_title' , 'dcterms.title' , 'citation_journal_title' , 'dcterms.alternative' , 'twitter:title' , 'prism.alternateTitle' , 'prism.subtitle' , 'eprints.title' , 'bepress_citation_title' ] date_tags = [ 'citation_cover_date' , 'dc.date' , 'citation_online_date' , 'citation_date' , 'citation_publication_date' , 'dcterms.date' , 'dcterms.issued' , 'dcterms.created' , 'prism.coverDate' , 'prism.publicationDate' , 'bepress_citation_date' , 'eprints.date' , 'dcterms.date' , 'dcterms.dateSubmitted' , 'dcterms.dateAccepted' , 'dcterms.available' , 'dcterms.dateCopyrighted' , 'prism.creationDate' , 'prism.dateReceived' , 'eprints.datestamp' , 'bepress_citation_online_date' ] # which and order # if no date use year year_tag = [ 'citation_year' , 'prism.copyrightYear' ] # more author information author_tags = [ 'citation_author' , 'citation_authors' , 'dcterms.creator' , 'bepress_citation_author' , 'eprints.creators_name' , 'dc.creator' ] publisher_tags = [ 'dc.publisher' , 'citation_publisher' , 'dcterms.publisher' , 'citation_technical_report_institution' , 'prism.corporateEntity' , 'prism.distributor' , 'eprints.publisher' , 'bepress_citation_publisher' ] type_tag = [ 'og:type' , 'dcterms.type' , 'dc.type' , 'prism.contentType' , 'prism.genre' , 'prism.aggregationType' , 'eprints.type' , 'citation_dissertation_name' ] keyword_tag = [ 'citation_keywords' , 'dc.subject' , 'prism.academicField' , 'prism.keyword' ] citation_tag = [ 'dcterms.bibliographicCitation' , 'eprints.citation' ] tag = 'meta' log = 'SourceMeta' work_queue = deque () work_pool = None running = True threads = 4 def __init__ ( self , result_deque ): if not self . work_pool : self . work_pool = ThreadPool ( self . threads , self . worker , ()) self . result_deque = result_deque def worker ( self ): \"\"\"the worker thread function will ensure that the publications in the queue are all processed, it will sleep for 0.1s if no item is in the queue to reduce cpu usage \"\"\" while self . running : try : item = self . work_queue . pop () except IndexError : time . sleep ( 0.1 ) pass else : if item : publication = PubFinderHelper . get_publication ( item ) logging . warning ( self . log + \" work on item \" + publication [ 'doi' ]) publication_temp = self . add_data_to_publication ( publication ) # only if we have any data we set it if publication_temp : publication = publication_temp publication [ 'source' ] = self . tag # no meta since link already present source_ids = publication [ 'source_id' ] source_ids . append ({ 'title' : 'Meta' , 'url' : 'https://doi.org/' + publication [ 'doi' ], 'license' : 'TODO' }) publication [ 'source_id' ] = source_ids if type ( item ) is Event : item . data [ 'obj' ][ 'data' ] = publication result = { 'item' : item , 'tag' : self . tag } self . result_deque . append ( result ) def add_data_to_publication ( self , publication ): \"\"\"add data to a given publication, only append, no overwriting if a value is already set Arguments: publication: the publication to add data too \"\"\" response = self . fetch ( publication [ 'doi' ]) data = self . get_lxml ( response ) return self . map ( data , publication ) # fetch response to add data to publication def fetch ( self , doi ): \"\"\"fetch data from the source using its doi Arguments: doi: the doi of the publication \"\"\" session = Session () headers = { 'User-Agent' : 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_11_6) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/66.0.3359.181 Safari/537.36' , 'Pragma' : 'no-cache' } session . headers . update ( headers ) return get_response ( self . base_url + doi , session ) # map response data to publication def map ( self , response_data , publication ): \"\"\"map response data and the publication Arguments: response_data: the response data publication: the publication \"\"\" if not response_data : return None if 'abstract' in response_data and \\ ( 'abstract' not in publication or not PubFinderHelper . valid_abstract ( publication [ 'abstract' ])): abstract = PubFinderHelper . clean_abstract ( response_data [ 'abstract' ]) if PubFinderHelper . valid_abstract ( abstract ): publication [ 'abstract' ] = abstract if response_data and 'title' in response_data and ( 'title' not in publication or len ( publication [ 'title' ]) < 5 ): publication [ 'title' ] = PubFinderHelper . clean_title ( response_data [ 'title' ]) publication [ 'normalized_title' ] = PubFinderHelper . normalize ( publication [ 'title' ]) if PubFinderHelper . should_update ( 'pub_date' , response_data , publication ): pub = MetaSource . format_date ( response_data [ 'pub_date' ]) if pub : publication [ 'pub_date' ] = pub publication [ 'year' ] = pub . split ( '-' )[ 0 ] elif PubFinderHelper . should_update ( 'year' , response_data , publication ): publication [ 'year' ] = response_data [ 'year' ] if PubFinderHelper . should_update ( 'publisher' , response_data , publication ): publication [ 'publisher' ] = response_data [ 'publisher' ] if PubFinderHelper . should_update ( 'authors' , response_data , publication ): publication [ 'authors' ] = self . map_object ( response_data [ 'authors' ]) if PubFinderHelper . should_update ( 'fields_of_study' , response_data , publication ): publication [ 'fields_of_study' ] = self . map_object ( response_data [ 'fields_of_study' ]) if response_data and 'citations' in response_data and ( 'citation_count' not in publication or publication [ 'citation_count' ] == 0 ): publication [ 'citation_count' ] = len ( response_data [ 'citations' ]) if PubFinderHelper . should_update ( 'citations' , response_data , publication ): publication [ 'citations' ] = response_data [ 'citations' ] return None def map_object ( self , fields ): result = [] for field in fields : name = field normalized_name = PubFinderHelper . normalize ( name ) if not any ( d [ 'normalized_name' ] == normalized_name for d in result ) and len ( normalized_name ) < 150 : result . append ({ 'name' : name , 'normalized_name' : normalized_name }) return result @staticmethod def format_date ( date_text ): \"\"\"format a date to end up in our preferred format %Y-%m-%d possible input formats 15 Oct 2014 1969-12-01 2003-07 2014-9-11 July 2021 Example Output 2021-02-01 \"\"\" try : date = parse ( date_text ) except ValueError : logging . warning ( \"unable to parse date string %s \" % date_text ) else : return date . strftime ( '%Y-%m- %d ' ) def get_lxml ( self , page ): \"\"\"use lxml to parse the page and create a data dict from this page Arguments: page: the page \"\"\" result = {} data = {} if not page : return None content = html . fromstring ( page . content ) # go through all meta tags in the head for meta in content . xpath ( '//meta' ): # iterate through for name , value in sorted ( meta . items ()): # abstracts if value . strip () . lower () in self . abstract_tags : result [ value . strip () . lower ()] = meta . get ( 'content' ) if value . strip () . lower () in self . title_tags : result [ value . strip () . lower ()] = meta . get ( 'content' ) if value . strip () . lower () in self . date_tags : result [ value . strip () . lower ()] = meta . get ( 'content' ) if value . strip () . lower () in self . year_tag : result [ value . strip () . lower ()] = meta . get ( 'content' ) if value . strip () . lower () in self . publisher_tags : result [ value . strip () . lower ()] = meta . get ( 'content' ) if value . strip () . lower () in self . type_tag : result [ value . strip () . lower ()] = meta . get ( 'content' ) if value . strip () . lower () in self . author_tags : result [ value . strip () . lower ()] = meta . get ( 'content' ) if value . strip () . lower () in self . author_tags : result [ value . strip () . lower ()] = meta . get ( 'content' ) if value . strip () . lower () in self . keyword_tag : result [ value . strip () . lower ()] = meta . get ( 'content' ) if value . strip () . lower () in self . citation_tag : result [ value . strip () . lower ()] = meta . get ( 'content' ) # logging.debug(self.log + \" could not resolve: \" + json.dumps(result)) for key in self . abstract_tags : if key in result : abstract = PubFinderHelper . clean_abstract ( result [ key ]) if 'abstract' not in data or len ( abstract ) > len ( data [ 'abstract' ]): data [ 'abstract' ] = result [ key ] for key in self . title_tags : if 'title' not in data : if key in result : data [ 'title' ] = result [ key ] for key in self . date_tags : if 'pub_date' not in data : if key in result : dateTemp = result [ key ] . replace ( \"/\" , \"-\" ) data [ 'pub_date' ] = dateTemp for key in self . year_tag : if 'year' not in data : if key in result : data [ 'year' ] = result [ key ] for key in self . publisher_tags : if 'publisher' not in data : if key in result : data [ 'publisher' ] = result [ key ] for key in self . type_tag : if 'type' not in data : if key in result : data [ 'type' ] = result [ key ] authors = [] for key in self . author_tags : if key in result and len ( result [ key ] . strip ()) > 1 : authors . append ( result [ key ] . strip ()) data [ 'authors' ] = authors keywords = [] for key in self . keyword_tag : if key in result : keywords . append ( result [ key ]) data [ 'fields_of_study' ] = keywords citations = [] for key in self . citation_tag : if key in result : citations . append ( result [ key ]) data [ 'citations' ] = citations return data add_data_to_publication ( self , publication ) add data to a given publication, only append, no overwriting if a value is already set Parameters: Name Type Description Default publication the publication to add data too required Source code in src/meta_source.py def add_data_to_publication ( self , publication ): \"\"\"add data to a given publication, only append, no overwriting if a value is already set Arguments: publication: the publication to add data too \"\"\" response = self . fetch ( publication [ 'doi' ]) data = self . get_lxml ( response ) return self . map ( data , publication ) fetch ( self , doi ) fetch data from the source using its doi Parameters: Name Type Description Default doi the doi of the publication required Source code in src/meta_source.py def fetch ( self , doi ): \"\"\"fetch data from the source using its doi Arguments: doi: the doi of the publication \"\"\" session = Session () headers = { 'User-Agent' : 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_11_6) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/66.0.3359.181 Safari/537.36' , 'Pragma' : 'no-cache' } session . headers . update ( headers ) return get_response ( self . base_url + doi , session ) format_date ( date_text ) staticmethod format a date to end up in our preferred format %Y-%m-%d possible input formats 15 Oct 2014 1969-12-01 2003-07 2014-9-11 July 2021 Example Output 2021-02-01 Source code in src/meta_source.py @staticmethod def format_date ( date_text ): \"\"\"format a date to end up in our preferred format %Y-%m-%d possible input formats 15 Oct 2014 1969-12-01 2003-07 2014-9-11 July 2021 Example Output 2021-02-01 \"\"\" try : date = parse ( date_text ) except ValueError : logging . warning ( \"unable to parse date string %s \" % date_text ) else : return date . strftime ( '%Y-%m- %d ' ) get_lxml ( self , page ) use lxml to parse the page and create a data dict from this page Parameters: Name Type Description Default page the page required Source code in src/meta_source.py def get_lxml ( self , page ): \"\"\"use lxml to parse the page and create a data dict from this page Arguments: page: the page \"\"\" result = {} data = {} if not page : return None content = html . fromstring ( page . content ) # go through all meta tags in the head for meta in content . xpath ( '//meta' ): # iterate through for name , value in sorted ( meta . items ()): # abstracts if value . strip () . lower () in self . abstract_tags : result [ value . strip () . lower ()] = meta . get ( 'content' ) if value . strip () . lower () in self . title_tags : result [ value . strip () . lower ()] = meta . get ( 'content' ) if value . strip () . lower () in self . date_tags : result [ value . strip () . lower ()] = meta . get ( 'content' ) if value . strip () . lower () in self . year_tag : result [ value . strip () . lower ()] = meta . get ( 'content' ) if value . strip () . lower () in self . publisher_tags : result [ value . strip () . lower ()] = meta . get ( 'content' ) if value . strip () . lower () in self . type_tag : result [ value . strip () . lower ()] = meta . get ( 'content' ) if value . strip () . lower () in self . author_tags : result [ value . strip () . lower ()] = meta . get ( 'content' ) if value . strip () . lower () in self . author_tags : result [ value . strip () . lower ()] = meta . get ( 'content' ) if value . strip () . lower () in self . keyword_tag : result [ value . strip () . lower ()] = meta . get ( 'content' ) if value . strip () . lower () in self . citation_tag : result [ value . strip () . lower ()] = meta . get ( 'content' ) # logging.debug(self.log + \" could not resolve: \" + json.dumps(result)) for key in self . abstract_tags : if key in result : abstract = PubFinderHelper . clean_abstract ( result [ key ]) if 'abstract' not in data or len ( abstract ) > len ( data [ 'abstract' ]): data [ 'abstract' ] = result [ key ] for key in self . title_tags : if 'title' not in data : if key in result : data [ 'title' ] = result [ key ] for key in self . date_tags : if 'pub_date' not in data : if key in result : dateTemp = result [ key ] . replace ( \"/\" , \"-\" ) data [ 'pub_date' ] = dateTemp for key in self . year_tag : if 'year' not in data : if key in result : data [ 'year' ] = result [ key ] for key in self . publisher_tags : if 'publisher' not in data : if key in result : data [ 'publisher' ] = result [ key ] for key in self . type_tag : if 'type' not in data : if key in result : data [ 'type' ] = result [ key ] authors = [] for key in self . author_tags : if key in result and len ( result [ key ] . strip ()) > 1 : authors . append ( result [ key ] . strip ()) data [ 'authors' ] = authors keywords = [] for key in self . keyword_tag : if key in result : keywords . append ( result [ key ]) data [ 'fields_of_study' ] = keywords citations = [] for key in self . citation_tag : if key in result : citations . append ( result [ key ]) data [ 'citations' ] = citations return data map ( self , response_data , publication ) map response data and the publication Parameters: Name Type Description Default response_data the response data required publication the publication required Source code in src/meta_source.py def map ( self , response_data , publication ): \"\"\"map response data and the publication Arguments: response_data: the response data publication: the publication \"\"\" if not response_data : return None if 'abstract' in response_data and \\ ( 'abstract' not in publication or not PubFinderHelper . valid_abstract ( publication [ 'abstract' ])): abstract = PubFinderHelper . clean_abstract ( response_data [ 'abstract' ]) if PubFinderHelper . valid_abstract ( abstract ): publication [ 'abstract' ] = abstract if response_data and 'title' in response_data and ( 'title' not in publication or len ( publication [ 'title' ]) < 5 ): publication [ 'title' ] = PubFinderHelper . clean_title ( response_data [ 'title' ]) publication [ 'normalized_title' ] = PubFinderHelper . normalize ( publication [ 'title' ]) if PubFinderHelper . should_update ( 'pub_date' , response_data , publication ): pub = MetaSource . format_date ( response_data [ 'pub_date' ]) if pub : publication [ 'pub_date' ] = pub publication [ 'year' ] = pub . split ( '-' )[ 0 ] elif PubFinderHelper . should_update ( 'year' , response_data , publication ): publication [ 'year' ] = response_data [ 'year' ] if PubFinderHelper . should_update ( 'publisher' , response_data , publication ): publication [ 'publisher' ] = response_data [ 'publisher' ] if PubFinderHelper . should_update ( 'authors' , response_data , publication ): publication [ 'authors' ] = self . map_object ( response_data [ 'authors' ]) if PubFinderHelper . should_update ( 'fields_of_study' , response_data , publication ): publication [ 'fields_of_study' ] = self . map_object ( response_data [ 'fields_of_study' ]) if response_data and 'citations' in response_data and ( 'citation_count' not in publication or publication [ 'citation_count' ] == 0 ): publication [ 'citation_count' ] = len ( response_data [ 'citations' ]) if PubFinderHelper . should_update ( 'citations' , response_data , publication ): publication [ 'citations' ] = response_data [ 'citations' ] return None worker ( self ) the worker thread function will ensure that the publications in the queue are all processed, it will sleep for 0.1s if no item is in the queue to reduce cpu usage Source code in src/meta_source.py def worker ( self ): \"\"\"the worker thread function will ensure that the publications in the queue are all processed, it will sleep for 0.1s if no item is in the queue to reduce cpu usage \"\"\" while self . running : try : item = self . work_queue . pop () except IndexError : time . sleep ( 0.1 ) pass else : if item : publication = PubFinderHelper . get_publication ( item ) logging . warning ( self . log + \" work on item \" + publication [ 'doi' ]) publication_temp = self . add_data_to_publication ( publication ) # only if we have any data we set it if publication_temp : publication = publication_temp publication [ 'source' ] = self . tag # no meta since link already present source_ids = publication [ 'source_id' ] source_ids . append ({ 'title' : 'Meta' , 'url' : 'https://doi.org/' + publication [ 'doi' ], 'license' : 'TODO' }) publication [ 'source_id' ] = source_ids if type ( item ) is Event : item . data [ 'obj' ][ 'data' ] = publication result = { 'item' : item , 'tag' : self . tag } self . result_deque . append ( result ) get_response ( url , s ) get a response from a given url using a given session s, a session can be used for headers, this function is cached up to 100 elements !!! arguments url: the url to get s: the session to use Source code in src/meta_source.py @lru_cache ( maxsize = 10 ) def get_response ( url , s ): \"\"\"get a response from a given url using a given session s, a session can be used for headers, this function is cached up to 100 elements Arguments: url: the url to get s: the session to use \"\"\" try : result = s . get ( url , timeout = 5 ) except ( ConnectionRefusedError , SSLError , ReadTimeoutError , requests . exceptions . TooManyRedirects , ConnectTimeout , requests . exceptions . ReadTimeout , NewConnectionError , requests . exceptions . SSLError , ConnectionError , TimeoutError , ConnectionResetError ): logging . warning ( 'Meta Source - Pubfinder' ) s = Session () # get the response for the provided url headers = { 'User-Agent' : 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_11_6) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/66.0.3359.181 Safari/537.36' , 'Pragma' : 'no-cache' } s . headers . update ( headers ) else : return result return None","title":"meta source"},{"location":"meta_source_ref/#meta_source.MetaSource","text":"\" this source will try to append data using meta tags in the url of the resolved doi url Source code in src/meta_source.py class MetaSource ( object ): \"\"\"\" this source will try to append data using meta tags in the url of the resolved doi url \"\"\" base_url = \"http://doi.org/\" # tag must be ordered from better to worst, as soon as a result is found it will stop abstract_tags = [ 'dcterms.abstract' , 'dcterms.description' , 'prism.teaser' , 'eprints.abstract' , 'og:description' , 'dc.description' , 'description' , 'twitter:description' , 'citation_abstract' ] title_tags = [ 'og:title' , 'dc.title' , 'citation_title' , 'dcterms.title' , 'citation_journal_title' , 'dcterms.alternative' , 'twitter:title' , 'prism.alternateTitle' , 'prism.subtitle' , 'eprints.title' , 'bepress_citation_title' ] date_tags = [ 'citation_cover_date' , 'dc.date' , 'citation_online_date' , 'citation_date' , 'citation_publication_date' , 'dcterms.date' , 'dcterms.issued' , 'dcterms.created' , 'prism.coverDate' , 'prism.publicationDate' , 'bepress_citation_date' , 'eprints.date' , 'dcterms.date' , 'dcterms.dateSubmitted' , 'dcterms.dateAccepted' , 'dcterms.available' , 'dcterms.dateCopyrighted' , 'prism.creationDate' , 'prism.dateReceived' , 'eprints.datestamp' , 'bepress_citation_online_date' ] # which and order # if no date use year year_tag = [ 'citation_year' , 'prism.copyrightYear' ] # more author information author_tags = [ 'citation_author' , 'citation_authors' , 'dcterms.creator' , 'bepress_citation_author' , 'eprints.creators_name' , 'dc.creator' ] publisher_tags = [ 'dc.publisher' , 'citation_publisher' , 'dcterms.publisher' , 'citation_technical_report_institution' , 'prism.corporateEntity' , 'prism.distributor' , 'eprints.publisher' , 'bepress_citation_publisher' ] type_tag = [ 'og:type' , 'dcterms.type' , 'dc.type' , 'prism.contentType' , 'prism.genre' , 'prism.aggregationType' , 'eprints.type' , 'citation_dissertation_name' ] keyword_tag = [ 'citation_keywords' , 'dc.subject' , 'prism.academicField' , 'prism.keyword' ] citation_tag = [ 'dcterms.bibliographicCitation' , 'eprints.citation' ] tag = 'meta' log = 'SourceMeta' work_queue = deque () work_pool = None running = True threads = 4 def __init__ ( self , result_deque ): if not self . work_pool : self . work_pool = ThreadPool ( self . threads , self . worker , ()) self . result_deque = result_deque def worker ( self ): \"\"\"the worker thread function will ensure that the publications in the queue are all processed, it will sleep for 0.1s if no item is in the queue to reduce cpu usage \"\"\" while self . running : try : item = self . work_queue . pop () except IndexError : time . sleep ( 0.1 ) pass else : if item : publication = PubFinderHelper . get_publication ( item ) logging . warning ( self . log + \" work on item \" + publication [ 'doi' ]) publication_temp = self . add_data_to_publication ( publication ) # only if we have any data we set it if publication_temp : publication = publication_temp publication [ 'source' ] = self . tag # no meta since link already present source_ids = publication [ 'source_id' ] source_ids . append ({ 'title' : 'Meta' , 'url' : 'https://doi.org/' + publication [ 'doi' ], 'license' : 'TODO' }) publication [ 'source_id' ] = source_ids if type ( item ) is Event : item . data [ 'obj' ][ 'data' ] = publication result = { 'item' : item , 'tag' : self . tag } self . result_deque . append ( result ) def add_data_to_publication ( self , publication ): \"\"\"add data to a given publication, only append, no overwriting if a value is already set Arguments: publication: the publication to add data too \"\"\" response = self . fetch ( publication [ 'doi' ]) data = self . get_lxml ( response ) return self . map ( data , publication ) # fetch response to add data to publication def fetch ( self , doi ): \"\"\"fetch data from the source using its doi Arguments: doi: the doi of the publication \"\"\" session = Session () headers = { 'User-Agent' : 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_11_6) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/66.0.3359.181 Safari/537.36' , 'Pragma' : 'no-cache' } session . headers . update ( headers ) return get_response ( self . base_url + doi , session ) # map response data to publication def map ( self , response_data , publication ): \"\"\"map response data and the publication Arguments: response_data: the response data publication: the publication \"\"\" if not response_data : return None if 'abstract' in response_data and \\ ( 'abstract' not in publication or not PubFinderHelper . valid_abstract ( publication [ 'abstract' ])): abstract = PubFinderHelper . clean_abstract ( response_data [ 'abstract' ]) if PubFinderHelper . valid_abstract ( abstract ): publication [ 'abstract' ] = abstract if response_data and 'title' in response_data and ( 'title' not in publication or len ( publication [ 'title' ]) < 5 ): publication [ 'title' ] = PubFinderHelper . clean_title ( response_data [ 'title' ]) publication [ 'normalized_title' ] = PubFinderHelper . normalize ( publication [ 'title' ]) if PubFinderHelper . should_update ( 'pub_date' , response_data , publication ): pub = MetaSource . format_date ( response_data [ 'pub_date' ]) if pub : publication [ 'pub_date' ] = pub publication [ 'year' ] = pub . split ( '-' )[ 0 ] elif PubFinderHelper . should_update ( 'year' , response_data , publication ): publication [ 'year' ] = response_data [ 'year' ] if PubFinderHelper . should_update ( 'publisher' , response_data , publication ): publication [ 'publisher' ] = response_data [ 'publisher' ] if PubFinderHelper . should_update ( 'authors' , response_data , publication ): publication [ 'authors' ] = self . map_object ( response_data [ 'authors' ]) if PubFinderHelper . should_update ( 'fields_of_study' , response_data , publication ): publication [ 'fields_of_study' ] = self . map_object ( response_data [ 'fields_of_study' ]) if response_data and 'citations' in response_data and ( 'citation_count' not in publication or publication [ 'citation_count' ] == 0 ): publication [ 'citation_count' ] = len ( response_data [ 'citations' ]) if PubFinderHelper . should_update ( 'citations' , response_data , publication ): publication [ 'citations' ] = response_data [ 'citations' ] return None def map_object ( self , fields ): result = [] for field in fields : name = field normalized_name = PubFinderHelper . normalize ( name ) if not any ( d [ 'normalized_name' ] == normalized_name for d in result ) and len ( normalized_name ) < 150 : result . append ({ 'name' : name , 'normalized_name' : normalized_name }) return result @staticmethod def format_date ( date_text ): \"\"\"format a date to end up in our preferred format %Y-%m-%d possible input formats 15 Oct 2014 1969-12-01 2003-07 2014-9-11 July 2021 Example Output 2021-02-01 \"\"\" try : date = parse ( date_text ) except ValueError : logging . warning ( \"unable to parse date string %s \" % date_text ) else : return date . strftime ( '%Y-%m- %d ' ) def get_lxml ( self , page ): \"\"\"use lxml to parse the page and create a data dict from this page Arguments: page: the page \"\"\" result = {} data = {} if not page : return None content = html . fromstring ( page . content ) # go through all meta tags in the head for meta in content . xpath ( '//meta' ): # iterate through for name , value in sorted ( meta . items ()): # abstracts if value . strip () . lower () in self . abstract_tags : result [ value . strip () . lower ()] = meta . get ( 'content' ) if value . strip () . lower () in self . title_tags : result [ value . strip () . lower ()] = meta . get ( 'content' ) if value . strip () . lower () in self . date_tags : result [ value . strip () . lower ()] = meta . get ( 'content' ) if value . strip () . lower () in self . year_tag : result [ value . strip () . lower ()] = meta . get ( 'content' ) if value . strip () . lower () in self . publisher_tags : result [ value . strip () . lower ()] = meta . get ( 'content' ) if value . strip () . lower () in self . type_tag : result [ value . strip () . lower ()] = meta . get ( 'content' ) if value . strip () . lower () in self . author_tags : result [ value . strip () . lower ()] = meta . get ( 'content' ) if value . strip () . lower () in self . author_tags : result [ value . strip () . lower ()] = meta . get ( 'content' ) if value . strip () . lower () in self . keyword_tag : result [ value . strip () . lower ()] = meta . get ( 'content' ) if value . strip () . lower () in self . citation_tag : result [ value . strip () . lower ()] = meta . get ( 'content' ) # logging.debug(self.log + \" could not resolve: \" + json.dumps(result)) for key in self . abstract_tags : if key in result : abstract = PubFinderHelper . clean_abstract ( result [ key ]) if 'abstract' not in data or len ( abstract ) > len ( data [ 'abstract' ]): data [ 'abstract' ] = result [ key ] for key in self . title_tags : if 'title' not in data : if key in result : data [ 'title' ] = result [ key ] for key in self . date_tags : if 'pub_date' not in data : if key in result : dateTemp = result [ key ] . replace ( \"/\" , \"-\" ) data [ 'pub_date' ] = dateTemp for key in self . year_tag : if 'year' not in data : if key in result : data [ 'year' ] = result [ key ] for key in self . publisher_tags : if 'publisher' not in data : if key in result : data [ 'publisher' ] = result [ key ] for key in self . type_tag : if 'type' not in data : if key in result : data [ 'type' ] = result [ key ] authors = [] for key in self . author_tags : if key in result and len ( result [ key ] . strip ()) > 1 : authors . append ( result [ key ] . strip ()) data [ 'authors' ] = authors keywords = [] for key in self . keyword_tag : if key in result : keywords . append ( result [ key ]) data [ 'fields_of_study' ] = keywords citations = [] for key in self . citation_tag : if key in result : citations . append ( result [ key ]) data [ 'citations' ] = citations return data","title":"MetaSource"},{"location":"meta_source_ref/#meta_source.MetaSource.add_data_to_publication","text":"add data to a given publication, only append, no overwriting if a value is already set Parameters: Name Type Description Default publication the publication to add data too required Source code in src/meta_source.py def add_data_to_publication ( self , publication ): \"\"\"add data to a given publication, only append, no overwriting if a value is already set Arguments: publication: the publication to add data too \"\"\" response = self . fetch ( publication [ 'doi' ]) data = self . get_lxml ( response ) return self . map ( data , publication )","title":"add_data_to_publication()"},{"location":"meta_source_ref/#meta_source.MetaSource.fetch","text":"fetch data from the source using its doi Parameters: Name Type Description Default doi the doi of the publication required Source code in src/meta_source.py def fetch ( self , doi ): \"\"\"fetch data from the source using its doi Arguments: doi: the doi of the publication \"\"\" session = Session () headers = { 'User-Agent' : 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_11_6) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/66.0.3359.181 Safari/537.36' , 'Pragma' : 'no-cache' } session . headers . update ( headers ) return get_response ( self . base_url + doi , session )","title":"fetch()"},{"location":"meta_source_ref/#meta_source.MetaSource.format_date","text":"format a date to end up in our preferred format %Y-%m-%d possible input formats 15 Oct 2014 1969-12-01 2003-07 2014-9-11 July 2021 Example Output 2021-02-01 Source code in src/meta_source.py @staticmethod def format_date ( date_text ): \"\"\"format a date to end up in our preferred format %Y-%m-%d possible input formats 15 Oct 2014 1969-12-01 2003-07 2014-9-11 July 2021 Example Output 2021-02-01 \"\"\" try : date = parse ( date_text ) except ValueError : logging . warning ( \"unable to parse date string %s \" % date_text ) else : return date . strftime ( '%Y-%m- %d ' )","title":"format_date()"},{"location":"meta_source_ref/#meta_source.MetaSource.get_lxml","text":"use lxml to parse the page and create a data dict from this page Parameters: Name Type Description Default page the page required Source code in src/meta_source.py def get_lxml ( self , page ): \"\"\"use lxml to parse the page and create a data dict from this page Arguments: page: the page \"\"\" result = {} data = {} if not page : return None content = html . fromstring ( page . content ) # go through all meta tags in the head for meta in content . xpath ( '//meta' ): # iterate through for name , value in sorted ( meta . items ()): # abstracts if value . strip () . lower () in self . abstract_tags : result [ value . strip () . lower ()] = meta . get ( 'content' ) if value . strip () . lower () in self . title_tags : result [ value . strip () . lower ()] = meta . get ( 'content' ) if value . strip () . lower () in self . date_tags : result [ value . strip () . lower ()] = meta . get ( 'content' ) if value . strip () . lower () in self . year_tag : result [ value . strip () . lower ()] = meta . get ( 'content' ) if value . strip () . lower () in self . publisher_tags : result [ value . strip () . lower ()] = meta . get ( 'content' ) if value . strip () . lower () in self . type_tag : result [ value . strip () . lower ()] = meta . get ( 'content' ) if value . strip () . lower () in self . author_tags : result [ value . strip () . lower ()] = meta . get ( 'content' ) if value . strip () . lower () in self . author_tags : result [ value . strip () . lower ()] = meta . get ( 'content' ) if value . strip () . lower () in self . keyword_tag : result [ value . strip () . lower ()] = meta . get ( 'content' ) if value . strip () . lower () in self . citation_tag : result [ value . strip () . lower ()] = meta . get ( 'content' ) # logging.debug(self.log + \" could not resolve: \" + json.dumps(result)) for key in self . abstract_tags : if key in result : abstract = PubFinderHelper . clean_abstract ( result [ key ]) if 'abstract' not in data or len ( abstract ) > len ( data [ 'abstract' ]): data [ 'abstract' ] = result [ key ] for key in self . title_tags : if 'title' not in data : if key in result : data [ 'title' ] = result [ key ] for key in self . date_tags : if 'pub_date' not in data : if key in result : dateTemp = result [ key ] . replace ( \"/\" , \"-\" ) data [ 'pub_date' ] = dateTemp for key in self . year_tag : if 'year' not in data : if key in result : data [ 'year' ] = result [ key ] for key in self . publisher_tags : if 'publisher' not in data : if key in result : data [ 'publisher' ] = result [ key ] for key in self . type_tag : if 'type' not in data : if key in result : data [ 'type' ] = result [ key ] authors = [] for key in self . author_tags : if key in result and len ( result [ key ] . strip ()) > 1 : authors . append ( result [ key ] . strip ()) data [ 'authors' ] = authors keywords = [] for key in self . keyword_tag : if key in result : keywords . append ( result [ key ]) data [ 'fields_of_study' ] = keywords citations = [] for key in self . citation_tag : if key in result : citations . append ( result [ key ]) data [ 'citations' ] = citations return data","title":"get_lxml()"},{"location":"meta_source_ref/#meta_source.MetaSource.map","text":"map response data and the publication Parameters: Name Type Description Default response_data the response data required publication the publication required Source code in src/meta_source.py def map ( self , response_data , publication ): \"\"\"map response data and the publication Arguments: response_data: the response data publication: the publication \"\"\" if not response_data : return None if 'abstract' in response_data and \\ ( 'abstract' not in publication or not PubFinderHelper . valid_abstract ( publication [ 'abstract' ])): abstract = PubFinderHelper . clean_abstract ( response_data [ 'abstract' ]) if PubFinderHelper . valid_abstract ( abstract ): publication [ 'abstract' ] = abstract if response_data and 'title' in response_data and ( 'title' not in publication or len ( publication [ 'title' ]) < 5 ): publication [ 'title' ] = PubFinderHelper . clean_title ( response_data [ 'title' ]) publication [ 'normalized_title' ] = PubFinderHelper . normalize ( publication [ 'title' ]) if PubFinderHelper . should_update ( 'pub_date' , response_data , publication ): pub = MetaSource . format_date ( response_data [ 'pub_date' ]) if pub : publication [ 'pub_date' ] = pub publication [ 'year' ] = pub . split ( '-' )[ 0 ] elif PubFinderHelper . should_update ( 'year' , response_data , publication ): publication [ 'year' ] = response_data [ 'year' ] if PubFinderHelper . should_update ( 'publisher' , response_data , publication ): publication [ 'publisher' ] = response_data [ 'publisher' ] if PubFinderHelper . should_update ( 'authors' , response_data , publication ): publication [ 'authors' ] = self . map_object ( response_data [ 'authors' ]) if PubFinderHelper . should_update ( 'fields_of_study' , response_data , publication ): publication [ 'fields_of_study' ] = self . map_object ( response_data [ 'fields_of_study' ]) if response_data and 'citations' in response_data and ( 'citation_count' not in publication or publication [ 'citation_count' ] == 0 ): publication [ 'citation_count' ] = len ( response_data [ 'citations' ]) if PubFinderHelper . should_update ( 'citations' , response_data , publication ): publication [ 'citations' ] = response_data [ 'citations' ] return None","title":"map()"},{"location":"meta_source_ref/#meta_source.MetaSource.worker","text":"the worker thread function will ensure that the publications in the queue are all processed, it will sleep for 0.1s if no item is in the queue to reduce cpu usage Source code in src/meta_source.py def worker ( self ): \"\"\"the worker thread function will ensure that the publications in the queue are all processed, it will sleep for 0.1s if no item is in the queue to reduce cpu usage \"\"\" while self . running : try : item = self . work_queue . pop () except IndexError : time . sleep ( 0.1 ) pass else : if item : publication = PubFinderHelper . get_publication ( item ) logging . warning ( self . log + \" work on item \" + publication [ 'doi' ]) publication_temp = self . add_data_to_publication ( publication ) # only if we have any data we set it if publication_temp : publication = publication_temp publication [ 'source' ] = self . tag # no meta since link already present source_ids = publication [ 'source_id' ] source_ids . append ({ 'title' : 'Meta' , 'url' : 'https://doi.org/' + publication [ 'doi' ], 'license' : 'TODO' }) publication [ 'source_id' ] = source_ids if type ( item ) is Event : item . data [ 'obj' ][ 'data' ] = publication result = { 'item' : item , 'tag' : self . tag } self . result_deque . append ( result )","title":"worker()"},{"location":"meta_source_ref/#meta_source.get_response","text":"get a response from a given url using a given session s, a session can be used for headers, this function is cached up to 100 elements !!! arguments url: the url to get s: the session to use Source code in src/meta_source.py @lru_cache ( maxsize = 10 ) def get_response ( url , s ): \"\"\"get a response from a given url using a given session s, a session can be used for headers, this function is cached up to 100 elements Arguments: url: the url to get s: the session to use \"\"\" try : result = s . get ( url , timeout = 5 ) except ( ConnectionRefusedError , SSLError , ReadTimeoutError , requests . exceptions . TooManyRedirects , ConnectTimeout , requests . exceptions . ReadTimeout , NewConnectionError , requests . exceptions . SSLError , ConnectionError , TimeoutError , ConnectionResetError ): logging . warning ( 'Meta Source - Pubfinder' ) s = Session () # get the response for the provided url headers = { 'User-Agent' : 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_11_6) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/66.0.3359.181 Safari/537.36' , 'Pragma' : 'no-cache' } s . headers . update ( headers ) else : return result return None","title":"get_response()"},{"location":"openaire_source_ref/","text":"OpenAireSource Source code in src/openaire_source.py class OpenAireSource ( object ): base_url = \"https://api.openaire.eu/search/publications?doi=\" tag = 'openaire' log = 'SourceOpenAIRE' work_queue = deque () work_pool = None running = True threads = 4 api_limit = 3550 api_time = 3600 tags = [ 'main title' , 'creator' , 'relevantdate' , 'dateofacceptance' , 'description' , 'publisher' ] def __init__ ( self , result_deque ): if not self . work_pool : self . work_pool = ThreadPool ( self . threads , self . worker , ()) self . result_deque = result_deque self . fetched_counter = Value ( 'i' , 0 ) api_limit_thread = threading . Timer ( self . api_time , reset_api_limit , args = [ self . fetched_counter , self . api_time ]) api_limit_thread . daemon = True api_limit_thread . start () self . api_reset_timestamp = int ( time . time ()) def worker ( self ): \"\"\" main work function, fetch items and add data \"\"\" while self . running : try : item = self . work_queue . pop () except IndexError : time . sleep ( 0.1 ) pass else : if item : publication = PubFinderHelper . get_publication ( item ) logging . warning ( self . log + \" work on item \" + publication [ 'doi' ]) publication_temp = self . add_data_to_publication ( publication ) if publication_temp : publication = publication_temp if type ( item ) is Event : item . data [ 'obj' ][ 'data' ] = publication result = { 'item' : item , 'tag' : self . tag } self . result_deque . append ( result ) def add_data_to_publication ( self , publication ): \"\"\" add data to a given publication using the doi to fetch a response and map the data \"\"\" response = self . api_limit_watcher ( publication [ 'doi' ]) data = self . get_lxml ( response ) return self . map ( data , publication ) def map ( self , response_data , publication ): \"\"\" map a xml response to the internal data structure \"\"\" added_data = False if response_data : if PubFinderHelper . should_update ( 'title' , response_data , publication ): publication [ 'title' ] = PubFinderHelper . clean_title ( response_data [ 'title' ]) publication [ 'normalized_title' ] = PubFinderHelper . normalize ( publication [ 'title' ]) added_data = True if PubFinderHelper . should_update ( 'year' , response_data , publication ): publication [ 'year' ] = PubFinderHelper . clean_title ( response_data [ 'year' ]) added_data = True if PubFinderHelper . should_update ( 'pub_date' , response_data , publication ): publication [ 'pub_date' ] = PubFinderHelper . clean_title ( response_data [ 'pub_date' ]) added_data = True if PubFinderHelper . should_update ( 'publisher' , response_data , publication ): publication [ 'publisher' ] = response_data [ 'publisher' ] added_data = True if 'abstract' in response_data and \\ ( 'abstract' not in publication or not PubFinderHelper . valid_abstract ( publication [ 'abstract' ])): abstract = PubFinderHelper . clean_abstract ( response_data [ 'abstract' ]) if PubFinderHelper . valid_abstract ( abstract ): publication [ 'abstract' ] = abstract added_data = True if PubFinderHelper . should_update ( 'authors' , response_data , publication ): publication [ 'authors' ] = response_data [ 'authors' ] added_data = True if PubFinderHelper . should_update ( 'fields_of_study' , response_data , publication ): publication [ 'fields_of_study' ] = self . map_fields_of_study ( response_data [ 'fields_of_study' ]) added_data = True if added_data : source_ids = publication [ 'source_id' ] source_ids . append ( { 'title' : 'OpenAIRE' , 'url' : 'https://develop.openaire.eu/overview.html' , 'license' : 'TODO' }) publication [ 'source_id' ] = source_ids return publication def api_limit_watcher ( self , doi ): \"\"\" ensure api limits are kept and if the limit is reached wait for reset \"\"\" if self . fetched_counter . value < self . api_limit : with self . fetched_counter . get_lock (): self . fetched_counter . value += 1 return fetch ( doi ) else : wt = self . api_time - ( int ( time . time ()) - self . api_reset_timestamp ) % self . api_time + 1 logging . warning ( self . log + ' api limit reached, wait ' + str ( wt )) time . sleep ( wt ) self . api_limit_watcher ( doi ) def map_fields_of_study ( self , fields ): \"\"\" map fields of study \"\"\" result = [] for field in fields : name = field normalized_name = PubFinderHelper . normalize ( name ) if not any ( d [ 'normalized_name' ] == normalized_name for d in result ): result . append ({ 'name' : name , 'normalized_name' : normalized_name }) return result def get_lxml ( self , page ): \"\"\"use lxml to parse the page and create a data dict from this page Arguments: page: the page \"\"\" result = {} if not page : return None content = html . fromstring ( page . content ) d = content . xpath ( '//description' ) if len ( d ) > 0 : description = d [ 0 ] . text result [ 'abstract' ] = description pu = content . xpath ( '//publisher' ) if len ( pu ) > 0 : publisher = pu [ 0 ] . text result [ 'publisher' ] = publisher t = content . xpath ( \"//title[@classid='main title']\" ) if len ( t ) > 0 : title = t [ 0 ] . text result [ 'title' ] = title p = content . xpath ( \"/response/results/result/metadata/*[name()='oaf:entity']/*[name()='oaf:result']/dateofacceptance\" ) if len ( p ) > 0 : pub_date = p [ 0 ] . text result [ 'pub_date' ] = pub_date result [ 'year' ] = pub_date . split ( '-' )[ 0 ] a = content . xpath ( \"/response/results/result/metadata/*[name()='oaf:entity']/*[name()='oaf:result']/creator\" ) if len ( a ) > 0 : authors = [] for author in a : authors . append ( author . text ) result [ 'authors' ] = authors f = content . xpath ( \"/response/results/result/metadata/*[name()='oaf:entity']/*[name()='oaf:result']/subject[not(@trust)]\" ) if len ( f ) > 0 : fos = [] for fs in f : fos . append ( fs . text ) result [ 'fields_of_study' ] = fos return result add_data_to_publication ( self , publication ) add data to a given publication using the doi to fetch a response and map the data Source code in src/openaire_source.py def add_data_to_publication ( self , publication ): \"\"\" add data to a given publication using the doi to fetch a response and map the data \"\"\" response = self . api_limit_watcher ( publication [ 'doi' ]) data = self . get_lxml ( response ) return self . map ( data , publication ) api_limit_watcher ( self , doi ) ensure api limits are kept and if the limit is reached wait for reset Source code in src/openaire_source.py def api_limit_watcher ( self , doi ): \"\"\" ensure api limits are kept and if the limit is reached wait for reset \"\"\" if self . fetched_counter . value < self . api_limit : with self . fetched_counter . get_lock (): self . fetched_counter . value += 1 return fetch ( doi ) else : wt = self . api_time - ( int ( time . time ()) - self . api_reset_timestamp ) % self . api_time + 1 logging . warning ( self . log + ' api limit reached, wait ' + str ( wt )) time . sleep ( wt ) self . api_limit_watcher ( doi ) get_lxml ( self , page ) use lxml to parse the page and create a data dict from this page Parameters: Name Type Description Default page the page required Source code in src/openaire_source.py def get_lxml ( self , page ): \"\"\"use lxml to parse the page and create a data dict from this page Arguments: page: the page \"\"\" result = {} if not page : return None content = html . fromstring ( page . content ) d = content . xpath ( '//description' ) if len ( d ) > 0 : description = d [ 0 ] . text result [ 'abstract' ] = description pu = content . xpath ( '//publisher' ) if len ( pu ) > 0 : publisher = pu [ 0 ] . text result [ 'publisher' ] = publisher t = content . xpath ( \"//title[@classid='main title']\" ) if len ( t ) > 0 : title = t [ 0 ] . text result [ 'title' ] = title p = content . xpath ( \"/response/results/result/metadata/*[name()='oaf:entity']/*[name()='oaf:result']/dateofacceptance\" ) if len ( p ) > 0 : pub_date = p [ 0 ] . text result [ 'pub_date' ] = pub_date result [ 'year' ] = pub_date . split ( '-' )[ 0 ] a = content . xpath ( \"/response/results/result/metadata/*[name()='oaf:entity']/*[name()='oaf:result']/creator\" ) if len ( a ) > 0 : authors = [] for author in a : authors . append ( author . text ) result [ 'authors' ] = authors f = content . xpath ( \"/response/results/result/metadata/*[name()='oaf:entity']/*[name()='oaf:result']/subject[not(@trust)]\" ) if len ( f ) > 0 : fos = [] for fs in f : fos . append ( fs . text ) result [ 'fields_of_study' ] = fos return result map ( self , response_data , publication ) map a xml response to the internal data structure Source code in src/openaire_source.py def map ( self , response_data , publication ): \"\"\" map a xml response to the internal data structure \"\"\" added_data = False if response_data : if PubFinderHelper . should_update ( 'title' , response_data , publication ): publication [ 'title' ] = PubFinderHelper . clean_title ( response_data [ 'title' ]) publication [ 'normalized_title' ] = PubFinderHelper . normalize ( publication [ 'title' ]) added_data = True if PubFinderHelper . should_update ( 'year' , response_data , publication ): publication [ 'year' ] = PubFinderHelper . clean_title ( response_data [ 'year' ]) added_data = True if PubFinderHelper . should_update ( 'pub_date' , response_data , publication ): publication [ 'pub_date' ] = PubFinderHelper . clean_title ( response_data [ 'pub_date' ]) added_data = True if PubFinderHelper . should_update ( 'publisher' , response_data , publication ): publication [ 'publisher' ] = response_data [ 'publisher' ] added_data = True if 'abstract' in response_data and \\ ( 'abstract' not in publication or not PubFinderHelper . valid_abstract ( publication [ 'abstract' ])): abstract = PubFinderHelper . clean_abstract ( response_data [ 'abstract' ]) if PubFinderHelper . valid_abstract ( abstract ): publication [ 'abstract' ] = abstract added_data = True if PubFinderHelper . should_update ( 'authors' , response_data , publication ): publication [ 'authors' ] = response_data [ 'authors' ] added_data = True if PubFinderHelper . should_update ( 'fields_of_study' , response_data , publication ): publication [ 'fields_of_study' ] = self . map_fields_of_study ( response_data [ 'fields_of_study' ]) added_data = True if added_data : source_ids = publication [ 'source_id' ] source_ids . append ( { 'title' : 'OpenAIRE' , 'url' : 'https://develop.openaire.eu/overview.html' , 'license' : 'TODO' }) publication [ 'source_id' ] = source_ids return publication map_fields_of_study ( self , fields ) map fields of study Source code in src/openaire_source.py def map_fields_of_study ( self , fields ): \"\"\" map fields of study \"\"\" result = [] for field in fields : name = field normalized_name = PubFinderHelper . normalize ( name ) if not any ( d [ 'normalized_name' ] == normalized_name for d in result ): result . append ({ 'name' : name , 'normalized_name' : normalized_name }) return result worker ( self ) main work function, fetch items and add data Source code in src/openaire_source.py def worker ( self ): \"\"\" main work function, fetch items and add data \"\"\" while self . running : try : item = self . work_queue . pop () except IndexError : time . sleep ( 0.1 ) pass else : if item : publication = PubFinderHelper . get_publication ( item ) logging . warning ( self . log + \" work on item \" + publication [ 'doi' ]) publication_temp = self . add_data_to_publication ( publication ) if publication_temp : publication = publication_temp if type ( item ) is Event : item . data [ 'obj' ][ 'data' ] = publication result = { 'item' : item , 'tag' : self . tag } self . result_deque . append ( result ) fetch ( doi ) fetch response to add data to publication cache up to 100 since we should not have doi be occurring multiple times Parameters: Name Type Description Default doi the doi to be fetched required Source code in src/openaire_source.py @lru_cache ( maxsize = 10 ) def fetch ( doi ): \"\"\"fetch response to add data to publication cache up to 100 since we should not have doi be occurring multiple times Arguments: doi: the doi to be fetched \"\"\" return requests . get ( OpenAireSource . base_url + requests . utils . quote ( doi )) # check encoding reset_api_limit ( v , time_delta ) timer function resetting the shared value used to keep the limits Source code in src/openaire_source.py def reset_api_limit ( v , time_delta ): \"\"\" timer function resetting the shared value used to keep the limits \"\"\" logging . warning ( 'reset openaire api limit ' + str ( v . value )) with v . get_lock (): v . value = 0 api_limit_thread = threading . Timer ( time_delta , reset_api_limit , args = [ v , time_delta ]) api_limit_thread . daemon = True api_limit_thread . start ()","title":"openaire source"},{"location":"openaire_source_ref/#openaire_source.OpenAireSource","text":"Source code in src/openaire_source.py class OpenAireSource ( object ): base_url = \"https://api.openaire.eu/search/publications?doi=\" tag = 'openaire' log = 'SourceOpenAIRE' work_queue = deque () work_pool = None running = True threads = 4 api_limit = 3550 api_time = 3600 tags = [ 'main title' , 'creator' , 'relevantdate' , 'dateofacceptance' , 'description' , 'publisher' ] def __init__ ( self , result_deque ): if not self . work_pool : self . work_pool = ThreadPool ( self . threads , self . worker , ()) self . result_deque = result_deque self . fetched_counter = Value ( 'i' , 0 ) api_limit_thread = threading . Timer ( self . api_time , reset_api_limit , args = [ self . fetched_counter , self . api_time ]) api_limit_thread . daemon = True api_limit_thread . start () self . api_reset_timestamp = int ( time . time ()) def worker ( self ): \"\"\" main work function, fetch items and add data \"\"\" while self . running : try : item = self . work_queue . pop () except IndexError : time . sleep ( 0.1 ) pass else : if item : publication = PubFinderHelper . get_publication ( item ) logging . warning ( self . log + \" work on item \" + publication [ 'doi' ]) publication_temp = self . add_data_to_publication ( publication ) if publication_temp : publication = publication_temp if type ( item ) is Event : item . data [ 'obj' ][ 'data' ] = publication result = { 'item' : item , 'tag' : self . tag } self . result_deque . append ( result ) def add_data_to_publication ( self , publication ): \"\"\" add data to a given publication using the doi to fetch a response and map the data \"\"\" response = self . api_limit_watcher ( publication [ 'doi' ]) data = self . get_lxml ( response ) return self . map ( data , publication ) def map ( self , response_data , publication ): \"\"\" map a xml response to the internal data structure \"\"\" added_data = False if response_data : if PubFinderHelper . should_update ( 'title' , response_data , publication ): publication [ 'title' ] = PubFinderHelper . clean_title ( response_data [ 'title' ]) publication [ 'normalized_title' ] = PubFinderHelper . normalize ( publication [ 'title' ]) added_data = True if PubFinderHelper . should_update ( 'year' , response_data , publication ): publication [ 'year' ] = PubFinderHelper . clean_title ( response_data [ 'year' ]) added_data = True if PubFinderHelper . should_update ( 'pub_date' , response_data , publication ): publication [ 'pub_date' ] = PubFinderHelper . clean_title ( response_data [ 'pub_date' ]) added_data = True if PubFinderHelper . should_update ( 'publisher' , response_data , publication ): publication [ 'publisher' ] = response_data [ 'publisher' ] added_data = True if 'abstract' in response_data and \\ ( 'abstract' not in publication or not PubFinderHelper . valid_abstract ( publication [ 'abstract' ])): abstract = PubFinderHelper . clean_abstract ( response_data [ 'abstract' ]) if PubFinderHelper . valid_abstract ( abstract ): publication [ 'abstract' ] = abstract added_data = True if PubFinderHelper . should_update ( 'authors' , response_data , publication ): publication [ 'authors' ] = response_data [ 'authors' ] added_data = True if PubFinderHelper . should_update ( 'fields_of_study' , response_data , publication ): publication [ 'fields_of_study' ] = self . map_fields_of_study ( response_data [ 'fields_of_study' ]) added_data = True if added_data : source_ids = publication [ 'source_id' ] source_ids . append ( { 'title' : 'OpenAIRE' , 'url' : 'https://develop.openaire.eu/overview.html' , 'license' : 'TODO' }) publication [ 'source_id' ] = source_ids return publication def api_limit_watcher ( self , doi ): \"\"\" ensure api limits are kept and if the limit is reached wait for reset \"\"\" if self . fetched_counter . value < self . api_limit : with self . fetched_counter . get_lock (): self . fetched_counter . value += 1 return fetch ( doi ) else : wt = self . api_time - ( int ( time . time ()) - self . api_reset_timestamp ) % self . api_time + 1 logging . warning ( self . log + ' api limit reached, wait ' + str ( wt )) time . sleep ( wt ) self . api_limit_watcher ( doi ) def map_fields_of_study ( self , fields ): \"\"\" map fields of study \"\"\" result = [] for field in fields : name = field normalized_name = PubFinderHelper . normalize ( name ) if not any ( d [ 'normalized_name' ] == normalized_name for d in result ): result . append ({ 'name' : name , 'normalized_name' : normalized_name }) return result def get_lxml ( self , page ): \"\"\"use lxml to parse the page and create a data dict from this page Arguments: page: the page \"\"\" result = {} if not page : return None content = html . fromstring ( page . content ) d = content . xpath ( '//description' ) if len ( d ) > 0 : description = d [ 0 ] . text result [ 'abstract' ] = description pu = content . xpath ( '//publisher' ) if len ( pu ) > 0 : publisher = pu [ 0 ] . text result [ 'publisher' ] = publisher t = content . xpath ( \"//title[@classid='main title']\" ) if len ( t ) > 0 : title = t [ 0 ] . text result [ 'title' ] = title p = content . xpath ( \"/response/results/result/metadata/*[name()='oaf:entity']/*[name()='oaf:result']/dateofacceptance\" ) if len ( p ) > 0 : pub_date = p [ 0 ] . text result [ 'pub_date' ] = pub_date result [ 'year' ] = pub_date . split ( '-' )[ 0 ] a = content . xpath ( \"/response/results/result/metadata/*[name()='oaf:entity']/*[name()='oaf:result']/creator\" ) if len ( a ) > 0 : authors = [] for author in a : authors . append ( author . text ) result [ 'authors' ] = authors f = content . xpath ( \"/response/results/result/metadata/*[name()='oaf:entity']/*[name()='oaf:result']/subject[not(@trust)]\" ) if len ( f ) > 0 : fos = [] for fs in f : fos . append ( fs . text ) result [ 'fields_of_study' ] = fos return result","title":"OpenAireSource"},{"location":"openaire_source_ref/#openaire_source.OpenAireSource.add_data_to_publication","text":"add data to a given publication using the doi to fetch a response and map the data Source code in src/openaire_source.py def add_data_to_publication ( self , publication ): \"\"\" add data to a given publication using the doi to fetch a response and map the data \"\"\" response = self . api_limit_watcher ( publication [ 'doi' ]) data = self . get_lxml ( response ) return self . map ( data , publication )","title":"add_data_to_publication()"},{"location":"openaire_source_ref/#openaire_source.OpenAireSource.api_limit_watcher","text":"ensure api limits are kept and if the limit is reached wait for reset Source code in src/openaire_source.py def api_limit_watcher ( self , doi ): \"\"\" ensure api limits are kept and if the limit is reached wait for reset \"\"\" if self . fetched_counter . value < self . api_limit : with self . fetched_counter . get_lock (): self . fetched_counter . value += 1 return fetch ( doi ) else : wt = self . api_time - ( int ( time . time ()) - self . api_reset_timestamp ) % self . api_time + 1 logging . warning ( self . log + ' api limit reached, wait ' + str ( wt )) time . sleep ( wt ) self . api_limit_watcher ( doi )","title":"api_limit_watcher()"},{"location":"openaire_source_ref/#openaire_source.OpenAireSource.get_lxml","text":"use lxml to parse the page and create a data dict from this page Parameters: Name Type Description Default page the page required Source code in src/openaire_source.py def get_lxml ( self , page ): \"\"\"use lxml to parse the page and create a data dict from this page Arguments: page: the page \"\"\" result = {} if not page : return None content = html . fromstring ( page . content ) d = content . xpath ( '//description' ) if len ( d ) > 0 : description = d [ 0 ] . text result [ 'abstract' ] = description pu = content . xpath ( '//publisher' ) if len ( pu ) > 0 : publisher = pu [ 0 ] . text result [ 'publisher' ] = publisher t = content . xpath ( \"//title[@classid='main title']\" ) if len ( t ) > 0 : title = t [ 0 ] . text result [ 'title' ] = title p = content . xpath ( \"/response/results/result/metadata/*[name()='oaf:entity']/*[name()='oaf:result']/dateofacceptance\" ) if len ( p ) > 0 : pub_date = p [ 0 ] . text result [ 'pub_date' ] = pub_date result [ 'year' ] = pub_date . split ( '-' )[ 0 ] a = content . xpath ( \"/response/results/result/metadata/*[name()='oaf:entity']/*[name()='oaf:result']/creator\" ) if len ( a ) > 0 : authors = [] for author in a : authors . append ( author . text ) result [ 'authors' ] = authors f = content . xpath ( \"/response/results/result/metadata/*[name()='oaf:entity']/*[name()='oaf:result']/subject[not(@trust)]\" ) if len ( f ) > 0 : fos = [] for fs in f : fos . append ( fs . text ) result [ 'fields_of_study' ] = fos return result","title":"get_lxml()"},{"location":"openaire_source_ref/#openaire_source.OpenAireSource.map","text":"map a xml response to the internal data structure Source code in src/openaire_source.py def map ( self , response_data , publication ): \"\"\" map a xml response to the internal data structure \"\"\" added_data = False if response_data : if PubFinderHelper . should_update ( 'title' , response_data , publication ): publication [ 'title' ] = PubFinderHelper . clean_title ( response_data [ 'title' ]) publication [ 'normalized_title' ] = PubFinderHelper . normalize ( publication [ 'title' ]) added_data = True if PubFinderHelper . should_update ( 'year' , response_data , publication ): publication [ 'year' ] = PubFinderHelper . clean_title ( response_data [ 'year' ]) added_data = True if PubFinderHelper . should_update ( 'pub_date' , response_data , publication ): publication [ 'pub_date' ] = PubFinderHelper . clean_title ( response_data [ 'pub_date' ]) added_data = True if PubFinderHelper . should_update ( 'publisher' , response_data , publication ): publication [ 'publisher' ] = response_data [ 'publisher' ] added_data = True if 'abstract' in response_data and \\ ( 'abstract' not in publication or not PubFinderHelper . valid_abstract ( publication [ 'abstract' ])): abstract = PubFinderHelper . clean_abstract ( response_data [ 'abstract' ]) if PubFinderHelper . valid_abstract ( abstract ): publication [ 'abstract' ] = abstract added_data = True if PubFinderHelper . should_update ( 'authors' , response_data , publication ): publication [ 'authors' ] = response_data [ 'authors' ] added_data = True if PubFinderHelper . should_update ( 'fields_of_study' , response_data , publication ): publication [ 'fields_of_study' ] = self . map_fields_of_study ( response_data [ 'fields_of_study' ]) added_data = True if added_data : source_ids = publication [ 'source_id' ] source_ids . append ( { 'title' : 'OpenAIRE' , 'url' : 'https://develop.openaire.eu/overview.html' , 'license' : 'TODO' }) publication [ 'source_id' ] = source_ids return publication","title":"map()"},{"location":"openaire_source_ref/#openaire_source.OpenAireSource.map_fields_of_study","text":"map fields of study Source code in src/openaire_source.py def map_fields_of_study ( self , fields ): \"\"\" map fields of study \"\"\" result = [] for field in fields : name = field normalized_name = PubFinderHelper . normalize ( name ) if not any ( d [ 'normalized_name' ] == normalized_name for d in result ): result . append ({ 'name' : name , 'normalized_name' : normalized_name }) return result","title":"map_fields_of_study()"},{"location":"openaire_source_ref/#openaire_source.OpenAireSource.worker","text":"main work function, fetch items and add data Source code in src/openaire_source.py def worker ( self ): \"\"\" main work function, fetch items and add data \"\"\" while self . running : try : item = self . work_queue . pop () except IndexError : time . sleep ( 0.1 ) pass else : if item : publication = PubFinderHelper . get_publication ( item ) logging . warning ( self . log + \" work on item \" + publication [ 'doi' ]) publication_temp = self . add_data_to_publication ( publication ) if publication_temp : publication = publication_temp if type ( item ) is Event : item . data [ 'obj' ][ 'data' ] = publication result = { 'item' : item , 'tag' : self . tag } self . result_deque . append ( result )","title":"worker()"},{"location":"openaire_source_ref/#openaire_source.fetch","text":"fetch response to add data to publication cache up to 100 since we should not have doi be occurring multiple times Parameters: Name Type Description Default doi the doi to be fetched required Source code in src/openaire_source.py @lru_cache ( maxsize = 10 ) def fetch ( doi ): \"\"\"fetch response to add data to publication cache up to 100 since we should not have doi be occurring multiple times Arguments: doi: the doi to be fetched \"\"\" return requests . get ( OpenAireSource . base_url + requests . utils . quote ( doi )) # check encoding","title":"fetch()"},{"location":"openaire_source_ref/#openaire_source.reset_api_limit","text":"timer function resetting the shared value used to keep the limits Source code in src/openaire_source.py def reset_api_limit ( v , time_delta ): \"\"\" timer function resetting the shared value used to keep the limits \"\"\" logging . warning ( 'reset openaire api limit ' + str ( v . value )) with v . get_lock (): v . value = 0 api_limit_thread = threading . Timer ( time_delta , reset_api_limit , args = [ v , time_delta ]) api_limit_thread . daemon = True api_limit_thread . start ()","title":"reset_api_limit()"},{"location":"pubfinder_helper_ref/","text":"PubFinderHelper Source code in src/pubfinder_helper.py class PubFinderHelper ( object ): @staticmethod def get_publication ( item ): \"\"\"return the publication from an event\"\"\" publication = None if type ( item ) is Event : publication = item . data [ 'obj' ][ 'data' ] return publication @staticmethod def is_publication_done ( publication , save_mode = False ): \"\"\" check if a publication is either done or at least worth to be saved \"\"\" if not publication : return False if save_mode : keys = ( \"doi\" , \"publisher\" , \"abstract\" , \"title\" , \"normalized_title\" , \"year\" , \"authors\" , \"fields_of_study\" , \"source_id\" ) else : keys = ( \"type\" , \"doi\" , \"abstract\" , \"publisher\" , \"title\" , \"normalized_title\" , \"year\" , \"pub_date\" , \"authors\" , \"fields_of_study\" , \"source_id\" , \"citation_count\" ) if all ( key in publication for key in keys ): logging . debug ( 'publication done ' + publication [ 'doi' ]) if 'pub_date' not in publication : publication [ 'pub_date' ] = None if 'type' not in publication : publication [ 'type' ] = 'UNKNOWN' if 'abstract' not in publication : publication [ 'abstract' ] = None if 'citation_count' not in publication : publication [ 'citation_count' ] = 0 if 'license' not in publication : publication [ 'license' ] = None return True logging . debug ( 'publication missing ' + str ( set ( keys ) - publication . keys ())) return str ( set ( keys ) - publication . keys ()) @staticmethod def clean_fos ( fos ): \"\"\"cleans the title and removes unnecessary spaces and line breaks \"\"\" results = [] for f in fos : if ';' in f : d = f . split ( 'f' ) results . extend ( d ) else : results . append ( f ) return results @staticmethod def clean_title ( title ): \"\"\"cleans the title and removes unnecessary spaces and line breaks \"\"\" title . replace ( ' \\n ' , ' ' ) return re . sub ( ' +' , ' ' , title ) . strip () @staticmethod def clean_abstract ( abstract ): \"\"\"cleans the title and removes unnecessary spaces and line breaks \"\"\" try : abstract = re . sub ( '<.*?>|&([a-z0-9]+|#[0-9]{1,6}|#x[0-9a-f]{1,6});' , '' , abstract ) except TypeError : return '' else : abstract = abstract . strip () remove_words = [ 'abstract' , 'background' , 'background:' , 'introduction' , 'objective' , 'nature' ] while True : removed_word = False for word in remove_words : if re . match ( word , abstract , re . I ): abstract = abstract [ len ( word ):] removed_word = True if not removed_word : break abstract = re . sub ( r ' +' , ' ' , abstract ) abstract = re . sub ( r ' \\. ' , ' ' , abstract ) abstract = re . sub ( r ' *: ' , ' ' , abstract ) abstract = re . sub ( r ' - ' , ' ' , abstract ) return abstract . strip () @staticmethod def valid_abstract ( abstract ): \"\"\" check if an abstract is valid depending on its length \"\"\" return abstract and len ( abstract ) > 100 @staticmethod def normalize ( string ): \"\"\" normalize a given string\"\"\" return ( re . sub ( '[^a-zA-Z ]+' , '' , string )) . casefold () . strip () @staticmethod def should_update ( field , data , publication ): \"\"\" check if field is in need of an update\"\"\" return data and field in data and field not in publication clean_abstract ( abstract ) staticmethod cleans the title and removes unnecessary spaces and line breaks Source code in src/pubfinder_helper.py @staticmethod def clean_abstract ( abstract ): \"\"\"cleans the title and removes unnecessary spaces and line breaks \"\"\" try : abstract = re . sub ( '<.*?>|&([a-z0-9]+|#[0-9]{1,6}|#x[0-9a-f]{1,6});' , '' , abstract ) except TypeError : return '' else : abstract = abstract . strip () remove_words = [ 'abstract' , 'background' , 'background:' , 'introduction' , 'objective' , 'nature' ] while True : removed_word = False for word in remove_words : if re . match ( word , abstract , re . I ): abstract = abstract [ len ( word ):] removed_word = True if not removed_word : break abstract = re . sub ( r ' +' , ' ' , abstract ) abstract = re . sub ( r ' \\. ' , ' ' , abstract ) abstract = re . sub ( r ' *: ' , ' ' , abstract ) abstract = re . sub ( r ' - ' , ' ' , abstract ) return abstract . strip () clean_fos ( fos ) staticmethod cleans the title and removes unnecessary spaces and line breaks Source code in src/pubfinder_helper.py @staticmethod def clean_fos ( fos ): \"\"\"cleans the title and removes unnecessary spaces and line breaks \"\"\" results = [] for f in fos : if ';' in f : d = f . split ( 'f' ) results . extend ( d ) else : results . append ( f ) return results clean_title ( title ) staticmethod cleans the title and removes unnecessary spaces and line breaks Source code in src/pubfinder_helper.py @staticmethod def clean_title ( title ): \"\"\"cleans the title and removes unnecessary spaces and line breaks \"\"\" title . replace ( ' \\n ' , ' ' ) return re . sub ( ' +' , ' ' , title ) . strip () get_publication ( item ) staticmethod return the publication from an event Source code in src/pubfinder_helper.py @staticmethod def get_publication ( item ): \"\"\"return the publication from an event\"\"\" publication = None if type ( item ) is Event : publication = item . data [ 'obj' ][ 'data' ] return publication is_publication_done ( publication , save_mode = False ) staticmethod check if a publication is either done or at least worth to be saved Source code in src/pubfinder_helper.py @staticmethod def is_publication_done ( publication , save_mode = False ): \"\"\" check if a publication is either done or at least worth to be saved \"\"\" if not publication : return False if save_mode : keys = ( \"doi\" , \"publisher\" , \"abstract\" , \"title\" , \"normalized_title\" , \"year\" , \"authors\" , \"fields_of_study\" , \"source_id\" ) else : keys = ( \"type\" , \"doi\" , \"abstract\" , \"publisher\" , \"title\" , \"normalized_title\" , \"year\" , \"pub_date\" , \"authors\" , \"fields_of_study\" , \"source_id\" , \"citation_count\" ) if all ( key in publication for key in keys ): logging . debug ( 'publication done ' + publication [ 'doi' ]) if 'pub_date' not in publication : publication [ 'pub_date' ] = None if 'type' not in publication : publication [ 'type' ] = 'UNKNOWN' if 'abstract' not in publication : publication [ 'abstract' ] = None if 'citation_count' not in publication : publication [ 'citation_count' ] = 0 if 'license' not in publication : publication [ 'license' ] = None return True logging . debug ( 'publication missing ' + str ( set ( keys ) - publication . keys ())) return str ( set ( keys ) - publication . keys ()) normalize ( string ) staticmethod normalize a given string Source code in src/pubfinder_helper.py @staticmethod def normalize ( string ): \"\"\" normalize a given string\"\"\" return ( re . sub ( '[^a-zA-Z ]+' , '' , string )) . casefold () . strip () should_update ( field , data , publication ) staticmethod check if field is in need of an update Source code in src/pubfinder_helper.py @staticmethod def should_update ( field , data , publication ): \"\"\" check if field is in need of an update\"\"\" return data and field in data and field not in publication valid_abstract ( abstract ) staticmethod check if an abstract is valid depending on its length Source code in src/pubfinder_helper.py @staticmethod def valid_abstract ( abstract ): \"\"\" check if an abstract is valid depending on its length \"\"\" return abstract and len ( abstract ) > 100","title":"pubfinder helper"},{"location":"pubfinder_helper_ref/#pubfinder_helper.PubFinderHelper","text":"Source code in src/pubfinder_helper.py class PubFinderHelper ( object ): @staticmethod def get_publication ( item ): \"\"\"return the publication from an event\"\"\" publication = None if type ( item ) is Event : publication = item . data [ 'obj' ][ 'data' ] return publication @staticmethod def is_publication_done ( publication , save_mode = False ): \"\"\" check if a publication is either done or at least worth to be saved \"\"\" if not publication : return False if save_mode : keys = ( \"doi\" , \"publisher\" , \"abstract\" , \"title\" , \"normalized_title\" , \"year\" , \"authors\" , \"fields_of_study\" , \"source_id\" ) else : keys = ( \"type\" , \"doi\" , \"abstract\" , \"publisher\" , \"title\" , \"normalized_title\" , \"year\" , \"pub_date\" , \"authors\" , \"fields_of_study\" , \"source_id\" , \"citation_count\" ) if all ( key in publication for key in keys ): logging . debug ( 'publication done ' + publication [ 'doi' ]) if 'pub_date' not in publication : publication [ 'pub_date' ] = None if 'type' not in publication : publication [ 'type' ] = 'UNKNOWN' if 'abstract' not in publication : publication [ 'abstract' ] = None if 'citation_count' not in publication : publication [ 'citation_count' ] = 0 if 'license' not in publication : publication [ 'license' ] = None return True logging . debug ( 'publication missing ' + str ( set ( keys ) - publication . keys ())) return str ( set ( keys ) - publication . keys ()) @staticmethod def clean_fos ( fos ): \"\"\"cleans the title and removes unnecessary spaces and line breaks \"\"\" results = [] for f in fos : if ';' in f : d = f . split ( 'f' ) results . extend ( d ) else : results . append ( f ) return results @staticmethod def clean_title ( title ): \"\"\"cleans the title and removes unnecessary spaces and line breaks \"\"\" title . replace ( ' \\n ' , ' ' ) return re . sub ( ' +' , ' ' , title ) . strip () @staticmethod def clean_abstract ( abstract ): \"\"\"cleans the title and removes unnecessary spaces and line breaks \"\"\" try : abstract = re . sub ( '<.*?>|&([a-z0-9]+|#[0-9]{1,6}|#x[0-9a-f]{1,6});' , '' , abstract ) except TypeError : return '' else : abstract = abstract . strip () remove_words = [ 'abstract' , 'background' , 'background:' , 'introduction' , 'objective' , 'nature' ] while True : removed_word = False for word in remove_words : if re . match ( word , abstract , re . I ): abstract = abstract [ len ( word ):] removed_word = True if not removed_word : break abstract = re . sub ( r ' +' , ' ' , abstract ) abstract = re . sub ( r ' \\. ' , ' ' , abstract ) abstract = re . sub ( r ' *: ' , ' ' , abstract ) abstract = re . sub ( r ' - ' , ' ' , abstract ) return abstract . strip () @staticmethod def valid_abstract ( abstract ): \"\"\" check if an abstract is valid depending on its length \"\"\" return abstract and len ( abstract ) > 100 @staticmethod def normalize ( string ): \"\"\" normalize a given string\"\"\" return ( re . sub ( '[^a-zA-Z ]+' , '' , string )) . casefold () . strip () @staticmethod def should_update ( field , data , publication ): \"\"\" check if field is in need of an update\"\"\" return data and field in data and field not in publication","title":"PubFinderHelper"},{"location":"pubfinder_helper_ref/#pubfinder_helper.PubFinderHelper.clean_abstract","text":"cleans the title and removes unnecessary spaces and line breaks Source code in src/pubfinder_helper.py @staticmethod def clean_abstract ( abstract ): \"\"\"cleans the title and removes unnecessary spaces and line breaks \"\"\" try : abstract = re . sub ( '<.*?>|&([a-z0-9]+|#[0-9]{1,6}|#x[0-9a-f]{1,6});' , '' , abstract ) except TypeError : return '' else : abstract = abstract . strip () remove_words = [ 'abstract' , 'background' , 'background:' , 'introduction' , 'objective' , 'nature' ] while True : removed_word = False for word in remove_words : if re . match ( word , abstract , re . I ): abstract = abstract [ len ( word ):] removed_word = True if not removed_word : break abstract = re . sub ( r ' +' , ' ' , abstract ) abstract = re . sub ( r ' \\. ' , ' ' , abstract ) abstract = re . sub ( r ' *: ' , ' ' , abstract ) abstract = re . sub ( r ' - ' , ' ' , abstract ) return abstract . strip ()","title":"clean_abstract()"},{"location":"pubfinder_helper_ref/#pubfinder_helper.PubFinderHelper.clean_fos","text":"cleans the title and removes unnecessary spaces and line breaks Source code in src/pubfinder_helper.py @staticmethod def clean_fos ( fos ): \"\"\"cleans the title and removes unnecessary spaces and line breaks \"\"\" results = [] for f in fos : if ';' in f : d = f . split ( 'f' ) results . extend ( d ) else : results . append ( f ) return results","title":"clean_fos()"},{"location":"pubfinder_helper_ref/#pubfinder_helper.PubFinderHelper.clean_title","text":"cleans the title and removes unnecessary spaces and line breaks Source code in src/pubfinder_helper.py @staticmethod def clean_title ( title ): \"\"\"cleans the title and removes unnecessary spaces and line breaks \"\"\" title . replace ( ' \\n ' , ' ' ) return re . sub ( ' +' , ' ' , title ) . strip ()","title":"clean_title()"},{"location":"pubfinder_helper_ref/#pubfinder_helper.PubFinderHelper.get_publication","text":"return the publication from an event Source code in src/pubfinder_helper.py @staticmethod def get_publication ( item ): \"\"\"return the publication from an event\"\"\" publication = None if type ( item ) is Event : publication = item . data [ 'obj' ][ 'data' ] return publication","title":"get_publication()"},{"location":"pubfinder_helper_ref/#pubfinder_helper.PubFinderHelper.is_publication_done","text":"check if a publication is either done or at least worth to be saved Source code in src/pubfinder_helper.py @staticmethod def is_publication_done ( publication , save_mode = False ): \"\"\" check if a publication is either done or at least worth to be saved \"\"\" if not publication : return False if save_mode : keys = ( \"doi\" , \"publisher\" , \"abstract\" , \"title\" , \"normalized_title\" , \"year\" , \"authors\" , \"fields_of_study\" , \"source_id\" ) else : keys = ( \"type\" , \"doi\" , \"abstract\" , \"publisher\" , \"title\" , \"normalized_title\" , \"year\" , \"pub_date\" , \"authors\" , \"fields_of_study\" , \"source_id\" , \"citation_count\" ) if all ( key in publication for key in keys ): logging . debug ( 'publication done ' + publication [ 'doi' ]) if 'pub_date' not in publication : publication [ 'pub_date' ] = None if 'type' not in publication : publication [ 'type' ] = 'UNKNOWN' if 'abstract' not in publication : publication [ 'abstract' ] = None if 'citation_count' not in publication : publication [ 'citation_count' ] = 0 if 'license' not in publication : publication [ 'license' ] = None return True logging . debug ( 'publication missing ' + str ( set ( keys ) - publication . keys ())) return str ( set ( keys ) - publication . keys ())","title":"is_publication_done()"},{"location":"pubfinder_helper_ref/#pubfinder_helper.PubFinderHelper.normalize","text":"normalize a given string Source code in src/pubfinder_helper.py @staticmethod def normalize ( string ): \"\"\" normalize a given string\"\"\" return ( re . sub ( '[^a-zA-Z ]+' , '' , string )) . casefold () . strip ()","title":"normalize()"},{"location":"pubfinder_helper_ref/#pubfinder_helper.PubFinderHelper.should_update","text":"check if field is in need of an update Source code in src/pubfinder_helper.py @staticmethod def should_update ( field , data , publication ): \"\"\" check if field is in need of an update\"\"\" return data and field in data and field not in publication","title":"should_update()"},{"location":"pubfinder_helper_ref/#pubfinder_helper.PubFinderHelper.valid_abstract","text":"check if an abstract is valid depending on its length Source code in src/pubfinder_helper.py @staticmethod def valid_abstract ( abstract ): \"\"\" check if an abstract is valid depending on its length \"\"\" return abstract and len ( abstract ) > 100","title":"valid_abstract()"},{"location":"pubfinder_worker_ref/","text":"PubFinderWorker ( EventStreamProducer ) Source code in src/pubfinder_worker.py class PubFinderWorker ( EventStreamProducer ): state = \"unknown\" relation_type = \"discusses\" log = \"PubFinderWorker \" group_id = \"pub-finder-worker\" required_fields = { 'type' , 'doi' , 'abstract' , 'pub_date' , 'publisher' , 'citation_count' , 'title' , 'normalized_title' , 'year' , 'citations' , 'refs' , 'authors' , 'fieldOfStudy' , 'source_id' } consumer = None collection = None collectionFailed = None sources = [ 'db' , 'amba' , 'crossref' , 'openaire' , 'semanticscholar' ] db_pool = None db_queue = deque () result_pool = None results = deque () amba_source = None crossref_source = None meta_source = None openaire_source = None semanticscholar_source = None dao = None def create_consumer ( self ): \"\"\" create and setup all needed sources and connectinos\"\"\" if self . state == 'all' : self . topics = self . build_topic_list () if isinstance ( self . state , six . string_types ): self . state = [ self . state ] if isinstance ( self . relation_type , six . string_types ): self . relation_type = [ self . relation_type ] self . results = deque () if not self . result_pool : self . result_pool = ThreadPool ( 1 , self . worker_results , ( self . results ,)) self . amba_source = AmbaSource ( self . results ) self . crossref_source = CrossrefSource ( self . results ) self . meta_source = MetaSource ( self . results ) self . openaire_source = OpenAireSource ( self . results ) self . semanticscholar_source = SemanticScholarSource ( self . results ) if not self . topics : self . topics = list () for state in self . state : for relation_type in self . relation_type : self . topics . append ( self . get_topic_name ( state = state , relation_type = relation_type )) logging . debug ( self . log + \"get consumer for topic: %s \" % self . topics ) self . consumer = KafkaConsumer ( group_id = self . group_id , bootstrap_servers = self . bootstrap_servers , api_version = self . api_version , consumer_timeout_ms = self . consumer_timeout_ms ) for topic in self . topics : logging . debug ( self . log + \"consumer subscribe: %s \" % topic ) self . consumer . subscribe ( topic ) logging . warning ( self . log + \"consumer subscribed to: %s \" % self . consumer . topics ()) def consume ( self ): \"\"\" consume new events from kafka using a thread pool\"\"\" logging . warning ( self . log + \"start consume\" ) self . running = True if not self . consumer : self . create_consumer () if not self . db_pool : self . db_pool = ThreadPool ( 1 , self . worker_db , ( self . db_queue ,)) if not self . dao : self . dao = DAO () logging . debug ( self . log + \"wait for messages\" ) while self . running : try : for msg in self . consumer : e = Event () e . from_json ( json . loads ( msg . value . decode ( 'utf-8' ))) if e is not None : self . db_queue . append ( e ) except Exception as exc : self . consumer . close () logging . error ( self . log + 'stream Consumer generated an exception: %s ' % exc ) logging . warning ( self . log + \"Consumer closed\" ) break # keep alive if self . running : return self . consume () self . result_pool . close () self . db_pool . close () logging . warning ( self . log + \"Consumer shutdown\" ) def worker_results ( self , queue ): \"\"\" worker functions to handle resulting \"\"\" logging . warning ( self . log + \"worker results\" ) while self . running : try : result = queue . pop () except IndexError : time . sleep ( 0.1 ) pass else : if result and 'item' in result and 'tag' in result : self . finish_work ( result [ 'item' ], result [ 'tag' ]) def worker_db ( self , queue ): \"\"\" worker function to retrieve publication data from the postgreSQL \"\"\" while self . running : try : item = queue . pop () except IndexError : time . sleep ( 0.1 ) pass else : publication = PubFinderHelper . get_publication ( item ) publication_temp = self . dao . get_publication ( publication [ 'doi' ]) if publication_temp and isinstance ( publication , dict ): logging . warning ( self . log + \" found in db \" + publication [ 'doi' ]) publication = publication_temp publication [ 'source' ] = 'db' publication [ 'source_id' ] = [{ 'title' : 'DB' , 'url' : 'https://ambalytics.com/' , 'license' : 'MIT' }] if type ( item ) is Event : item . data [ 'obj' ][ 'data' ] = publication self . finish_work ( item , 'db' ) def finish_work ( self , item , source ): \"\"\" check if a publication is finished or needs further processing Arguments: item: the item to work on (publication) source: the source specifies the last used source \"\"\" publication = PubFinderHelper . get_publication ( item ) pub_is_done = PubFinderHelper . is_publication_done ( publication ) if pub_is_done is True : logging . warning ( self . log + \"publication done \" + publication [ 'doi' ]) if source != 'db' : self . dao . save_publication ( publication ) if type ( item ) is Event : item . set ( 'state' , 'linked' ) logging . info ( self . log + \"publish \" + publication [ 'doi' ]) self . publish ( item ) else : # put it in next queue or stop if source in self . sources : if source == 'db' : logging . debug ( 'db -> amba ' + publication [ 'doi' ]) self . amba_source . work_queue . append ( item ) if source == 'amba' : logging . debug ( 'amba -> crossref ' + publication [ 'doi' ]) self . crossref_source . work_queue . append ( item ) if source == 'crossref' : logging . debug ( 'crossref -> openaire ' + publication [ 'doi' ]) self . openaire_source . work_queue . append ( item ) if source == 'openaire' : logging . debug ( 'openaire -> semanticscholar ' + publication [ 'doi' ]) self . semanticscholar_source . work_queue . append ( item ) if source == 'semanticscholar' : logging . debug ( 'semanticscholar -> meta ' + publication [ 'doi' ]) self . meta_source . work_queue . append ( item ) else : done_now = PubFinderHelper . is_publication_done ( publication , True ) if done_now is True : logging . warning ( self . log + \"publication done \" + publication [ 'doi' ]) if source != 'db' : self . dao . save_publication ( publication ) else : self . dao . save_publication_not_found ( publication [ 'doi' ], pub_is_done ) logging . warning ( 'unable to find publication data for ' + publication [ 'doi' ] + ' - ' + str ( done_now )) @staticmethod def start ( i = 0 ): \"\"\"start the consumer \"\"\" pfw = PubFinderWorker ( i ) logging . debug ( PubFinderWorker . log + 'Start %s ' % str ( i )) pfw . consume () consume ( self ) consume new events from kafka using a thread pool Source code in src/pubfinder_worker.py def consume ( self ): \"\"\" consume new events from kafka using a thread pool\"\"\" logging . warning ( self . log + \"start consume\" ) self . running = True if not self . consumer : self . create_consumer () if not self . db_pool : self . db_pool = ThreadPool ( 1 , self . worker_db , ( self . db_queue ,)) if not self . dao : self . dao = DAO () logging . debug ( self . log + \"wait for messages\" ) while self . running : try : for msg in self . consumer : e = Event () e . from_json ( json . loads ( msg . value . decode ( 'utf-8' ))) if e is not None : self . db_queue . append ( e ) except Exception as exc : self . consumer . close () logging . error ( self . log + 'stream Consumer generated an exception: %s ' % exc ) logging . warning ( self . log + \"Consumer closed\" ) break # keep alive if self . running : return self . consume () self . result_pool . close () self . db_pool . close () logging . warning ( self . log + \"Consumer shutdown\" ) create_consumer ( self ) create and setup all needed sources and connectinos Source code in src/pubfinder_worker.py def create_consumer ( self ): \"\"\" create and setup all needed sources and connectinos\"\"\" if self . state == 'all' : self . topics = self . build_topic_list () if isinstance ( self . state , six . string_types ): self . state = [ self . state ] if isinstance ( self . relation_type , six . string_types ): self . relation_type = [ self . relation_type ] self . results = deque () if not self . result_pool : self . result_pool = ThreadPool ( 1 , self . worker_results , ( self . results ,)) self . amba_source = AmbaSource ( self . results ) self . crossref_source = CrossrefSource ( self . results ) self . meta_source = MetaSource ( self . results ) self . openaire_source = OpenAireSource ( self . results ) self . semanticscholar_source = SemanticScholarSource ( self . results ) if not self . topics : self . topics = list () for state in self . state : for relation_type in self . relation_type : self . topics . append ( self . get_topic_name ( state = state , relation_type = relation_type )) logging . debug ( self . log + \"get consumer for topic: %s \" % self . topics ) self . consumer = KafkaConsumer ( group_id = self . group_id , bootstrap_servers = self . bootstrap_servers , api_version = self . api_version , consumer_timeout_ms = self . consumer_timeout_ms ) for topic in self . topics : logging . debug ( self . log + \"consumer subscribe: %s \" % topic ) self . consumer . subscribe ( topic ) logging . warning ( self . log + \"consumer subscribed to: %s \" % self . consumer . topics ()) finish_work ( self , item , source ) check if a publication is finished or needs further processing Parameters: Name Type Description Default item the item to work on (publication) required source the source specifies the last used source required Source code in src/pubfinder_worker.py def finish_work ( self , item , source ): \"\"\" check if a publication is finished or needs further processing Arguments: item: the item to work on (publication) source: the source specifies the last used source \"\"\" publication = PubFinderHelper . get_publication ( item ) pub_is_done = PubFinderHelper . is_publication_done ( publication ) if pub_is_done is True : logging . warning ( self . log + \"publication done \" + publication [ 'doi' ]) if source != 'db' : self . dao . save_publication ( publication ) if type ( item ) is Event : item . set ( 'state' , 'linked' ) logging . info ( self . log + \"publish \" + publication [ 'doi' ]) self . publish ( item ) else : # put it in next queue or stop if source in self . sources : if source == 'db' : logging . debug ( 'db -> amba ' + publication [ 'doi' ]) self . amba_source . work_queue . append ( item ) if source == 'amba' : logging . debug ( 'amba -> crossref ' + publication [ 'doi' ]) self . crossref_source . work_queue . append ( item ) if source == 'crossref' : logging . debug ( 'crossref -> openaire ' + publication [ 'doi' ]) self . openaire_source . work_queue . append ( item ) if source == 'openaire' : logging . debug ( 'openaire -> semanticscholar ' + publication [ 'doi' ]) self . semanticscholar_source . work_queue . append ( item ) if source == 'semanticscholar' : logging . debug ( 'semanticscholar -> meta ' + publication [ 'doi' ]) self . meta_source . work_queue . append ( item ) else : done_now = PubFinderHelper . is_publication_done ( publication , True ) if done_now is True : logging . warning ( self . log + \"publication done \" + publication [ 'doi' ]) if source != 'db' : self . dao . save_publication ( publication ) else : self . dao . save_publication_not_found ( publication [ 'doi' ], pub_is_done ) logging . warning ( 'unable to find publication data for ' + publication [ 'doi' ] + ' - ' + str ( done_now )) start ( i = 0 ) staticmethod start the consumer Source code in src/pubfinder_worker.py @staticmethod def start ( i = 0 ): \"\"\"start the consumer \"\"\" pfw = PubFinderWorker ( i ) logging . debug ( PubFinderWorker . log + 'Start %s ' % str ( i )) pfw . consume () worker_db ( self , queue ) worker function to retrieve publication data from the postgreSQL Source code in src/pubfinder_worker.py def worker_db ( self , queue ): \"\"\" worker function to retrieve publication data from the postgreSQL \"\"\" while self . running : try : item = queue . pop () except IndexError : time . sleep ( 0.1 ) pass else : publication = PubFinderHelper . get_publication ( item ) publication_temp = self . dao . get_publication ( publication [ 'doi' ]) if publication_temp and isinstance ( publication , dict ): logging . warning ( self . log + \" found in db \" + publication [ 'doi' ]) publication = publication_temp publication [ 'source' ] = 'db' publication [ 'source_id' ] = [{ 'title' : 'DB' , 'url' : 'https://ambalytics.com/' , 'license' : 'MIT' }] if type ( item ) is Event : item . data [ 'obj' ][ 'data' ] = publication self . finish_work ( item , 'db' ) worker_results ( self , queue ) worker functions to handle resulting Source code in src/pubfinder_worker.py def worker_results ( self , queue ): \"\"\" worker functions to handle resulting \"\"\" logging . warning ( self . log + \"worker results\" ) while self . running : try : result = queue . pop () except IndexError : time . sleep ( 0.1 ) pass else : if result and 'item' in result and 'tag' in result : self . finish_work ( result [ 'item' ], result [ 'tag' ])","title":"pubfinder worker"},{"location":"pubfinder_worker_ref/#pubfinder_worker.PubFinderWorker","text":"Source code in src/pubfinder_worker.py class PubFinderWorker ( EventStreamProducer ): state = \"unknown\" relation_type = \"discusses\" log = \"PubFinderWorker \" group_id = \"pub-finder-worker\" required_fields = { 'type' , 'doi' , 'abstract' , 'pub_date' , 'publisher' , 'citation_count' , 'title' , 'normalized_title' , 'year' , 'citations' , 'refs' , 'authors' , 'fieldOfStudy' , 'source_id' } consumer = None collection = None collectionFailed = None sources = [ 'db' , 'amba' , 'crossref' , 'openaire' , 'semanticscholar' ] db_pool = None db_queue = deque () result_pool = None results = deque () amba_source = None crossref_source = None meta_source = None openaire_source = None semanticscholar_source = None dao = None def create_consumer ( self ): \"\"\" create and setup all needed sources and connectinos\"\"\" if self . state == 'all' : self . topics = self . build_topic_list () if isinstance ( self . state , six . string_types ): self . state = [ self . state ] if isinstance ( self . relation_type , six . string_types ): self . relation_type = [ self . relation_type ] self . results = deque () if not self . result_pool : self . result_pool = ThreadPool ( 1 , self . worker_results , ( self . results ,)) self . amba_source = AmbaSource ( self . results ) self . crossref_source = CrossrefSource ( self . results ) self . meta_source = MetaSource ( self . results ) self . openaire_source = OpenAireSource ( self . results ) self . semanticscholar_source = SemanticScholarSource ( self . results ) if not self . topics : self . topics = list () for state in self . state : for relation_type in self . relation_type : self . topics . append ( self . get_topic_name ( state = state , relation_type = relation_type )) logging . debug ( self . log + \"get consumer for topic: %s \" % self . topics ) self . consumer = KafkaConsumer ( group_id = self . group_id , bootstrap_servers = self . bootstrap_servers , api_version = self . api_version , consumer_timeout_ms = self . consumer_timeout_ms ) for topic in self . topics : logging . debug ( self . log + \"consumer subscribe: %s \" % topic ) self . consumer . subscribe ( topic ) logging . warning ( self . log + \"consumer subscribed to: %s \" % self . consumer . topics ()) def consume ( self ): \"\"\" consume new events from kafka using a thread pool\"\"\" logging . warning ( self . log + \"start consume\" ) self . running = True if not self . consumer : self . create_consumer () if not self . db_pool : self . db_pool = ThreadPool ( 1 , self . worker_db , ( self . db_queue ,)) if not self . dao : self . dao = DAO () logging . debug ( self . log + \"wait for messages\" ) while self . running : try : for msg in self . consumer : e = Event () e . from_json ( json . loads ( msg . value . decode ( 'utf-8' ))) if e is not None : self . db_queue . append ( e ) except Exception as exc : self . consumer . close () logging . error ( self . log + 'stream Consumer generated an exception: %s ' % exc ) logging . warning ( self . log + \"Consumer closed\" ) break # keep alive if self . running : return self . consume () self . result_pool . close () self . db_pool . close () logging . warning ( self . log + \"Consumer shutdown\" ) def worker_results ( self , queue ): \"\"\" worker functions to handle resulting \"\"\" logging . warning ( self . log + \"worker results\" ) while self . running : try : result = queue . pop () except IndexError : time . sleep ( 0.1 ) pass else : if result and 'item' in result and 'tag' in result : self . finish_work ( result [ 'item' ], result [ 'tag' ]) def worker_db ( self , queue ): \"\"\" worker function to retrieve publication data from the postgreSQL \"\"\" while self . running : try : item = queue . pop () except IndexError : time . sleep ( 0.1 ) pass else : publication = PubFinderHelper . get_publication ( item ) publication_temp = self . dao . get_publication ( publication [ 'doi' ]) if publication_temp and isinstance ( publication , dict ): logging . warning ( self . log + \" found in db \" + publication [ 'doi' ]) publication = publication_temp publication [ 'source' ] = 'db' publication [ 'source_id' ] = [{ 'title' : 'DB' , 'url' : 'https://ambalytics.com/' , 'license' : 'MIT' }] if type ( item ) is Event : item . data [ 'obj' ][ 'data' ] = publication self . finish_work ( item , 'db' ) def finish_work ( self , item , source ): \"\"\" check if a publication is finished or needs further processing Arguments: item: the item to work on (publication) source: the source specifies the last used source \"\"\" publication = PubFinderHelper . get_publication ( item ) pub_is_done = PubFinderHelper . is_publication_done ( publication ) if pub_is_done is True : logging . warning ( self . log + \"publication done \" + publication [ 'doi' ]) if source != 'db' : self . dao . save_publication ( publication ) if type ( item ) is Event : item . set ( 'state' , 'linked' ) logging . info ( self . log + \"publish \" + publication [ 'doi' ]) self . publish ( item ) else : # put it in next queue or stop if source in self . sources : if source == 'db' : logging . debug ( 'db -> amba ' + publication [ 'doi' ]) self . amba_source . work_queue . append ( item ) if source == 'amba' : logging . debug ( 'amba -> crossref ' + publication [ 'doi' ]) self . crossref_source . work_queue . append ( item ) if source == 'crossref' : logging . debug ( 'crossref -> openaire ' + publication [ 'doi' ]) self . openaire_source . work_queue . append ( item ) if source == 'openaire' : logging . debug ( 'openaire -> semanticscholar ' + publication [ 'doi' ]) self . semanticscholar_source . work_queue . append ( item ) if source == 'semanticscholar' : logging . debug ( 'semanticscholar -> meta ' + publication [ 'doi' ]) self . meta_source . work_queue . append ( item ) else : done_now = PubFinderHelper . is_publication_done ( publication , True ) if done_now is True : logging . warning ( self . log + \"publication done \" + publication [ 'doi' ]) if source != 'db' : self . dao . save_publication ( publication ) else : self . dao . save_publication_not_found ( publication [ 'doi' ], pub_is_done ) logging . warning ( 'unable to find publication data for ' + publication [ 'doi' ] + ' - ' + str ( done_now )) @staticmethod def start ( i = 0 ): \"\"\"start the consumer \"\"\" pfw = PubFinderWorker ( i ) logging . debug ( PubFinderWorker . log + 'Start %s ' % str ( i )) pfw . consume ()","title":"PubFinderWorker"},{"location":"pubfinder_worker_ref/#pubfinder_worker.PubFinderWorker.consume","text":"consume new events from kafka using a thread pool Source code in src/pubfinder_worker.py def consume ( self ): \"\"\" consume new events from kafka using a thread pool\"\"\" logging . warning ( self . log + \"start consume\" ) self . running = True if not self . consumer : self . create_consumer () if not self . db_pool : self . db_pool = ThreadPool ( 1 , self . worker_db , ( self . db_queue ,)) if not self . dao : self . dao = DAO () logging . debug ( self . log + \"wait for messages\" ) while self . running : try : for msg in self . consumer : e = Event () e . from_json ( json . loads ( msg . value . decode ( 'utf-8' ))) if e is not None : self . db_queue . append ( e ) except Exception as exc : self . consumer . close () logging . error ( self . log + 'stream Consumer generated an exception: %s ' % exc ) logging . warning ( self . log + \"Consumer closed\" ) break # keep alive if self . running : return self . consume () self . result_pool . close () self . db_pool . close () logging . warning ( self . log + \"Consumer shutdown\" )","title":"consume()"},{"location":"pubfinder_worker_ref/#pubfinder_worker.PubFinderWorker.create_consumer","text":"create and setup all needed sources and connectinos Source code in src/pubfinder_worker.py def create_consumer ( self ): \"\"\" create and setup all needed sources and connectinos\"\"\" if self . state == 'all' : self . topics = self . build_topic_list () if isinstance ( self . state , six . string_types ): self . state = [ self . state ] if isinstance ( self . relation_type , six . string_types ): self . relation_type = [ self . relation_type ] self . results = deque () if not self . result_pool : self . result_pool = ThreadPool ( 1 , self . worker_results , ( self . results ,)) self . amba_source = AmbaSource ( self . results ) self . crossref_source = CrossrefSource ( self . results ) self . meta_source = MetaSource ( self . results ) self . openaire_source = OpenAireSource ( self . results ) self . semanticscholar_source = SemanticScholarSource ( self . results ) if not self . topics : self . topics = list () for state in self . state : for relation_type in self . relation_type : self . topics . append ( self . get_topic_name ( state = state , relation_type = relation_type )) logging . debug ( self . log + \"get consumer for topic: %s \" % self . topics ) self . consumer = KafkaConsumer ( group_id = self . group_id , bootstrap_servers = self . bootstrap_servers , api_version = self . api_version , consumer_timeout_ms = self . consumer_timeout_ms ) for topic in self . topics : logging . debug ( self . log + \"consumer subscribe: %s \" % topic ) self . consumer . subscribe ( topic ) logging . warning ( self . log + \"consumer subscribed to: %s \" % self . consumer . topics ())","title":"create_consumer()"},{"location":"pubfinder_worker_ref/#pubfinder_worker.PubFinderWorker.finish_work","text":"check if a publication is finished or needs further processing Parameters: Name Type Description Default item the item to work on (publication) required source the source specifies the last used source required Source code in src/pubfinder_worker.py def finish_work ( self , item , source ): \"\"\" check if a publication is finished or needs further processing Arguments: item: the item to work on (publication) source: the source specifies the last used source \"\"\" publication = PubFinderHelper . get_publication ( item ) pub_is_done = PubFinderHelper . is_publication_done ( publication ) if pub_is_done is True : logging . warning ( self . log + \"publication done \" + publication [ 'doi' ]) if source != 'db' : self . dao . save_publication ( publication ) if type ( item ) is Event : item . set ( 'state' , 'linked' ) logging . info ( self . log + \"publish \" + publication [ 'doi' ]) self . publish ( item ) else : # put it in next queue or stop if source in self . sources : if source == 'db' : logging . debug ( 'db -> amba ' + publication [ 'doi' ]) self . amba_source . work_queue . append ( item ) if source == 'amba' : logging . debug ( 'amba -> crossref ' + publication [ 'doi' ]) self . crossref_source . work_queue . append ( item ) if source == 'crossref' : logging . debug ( 'crossref -> openaire ' + publication [ 'doi' ]) self . openaire_source . work_queue . append ( item ) if source == 'openaire' : logging . debug ( 'openaire -> semanticscholar ' + publication [ 'doi' ]) self . semanticscholar_source . work_queue . append ( item ) if source == 'semanticscholar' : logging . debug ( 'semanticscholar -> meta ' + publication [ 'doi' ]) self . meta_source . work_queue . append ( item ) else : done_now = PubFinderHelper . is_publication_done ( publication , True ) if done_now is True : logging . warning ( self . log + \"publication done \" + publication [ 'doi' ]) if source != 'db' : self . dao . save_publication ( publication ) else : self . dao . save_publication_not_found ( publication [ 'doi' ], pub_is_done ) logging . warning ( 'unable to find publication data for ' + publication [ 'doi' ] + ' - ' + str ( done_now ))","title":"finish_work()"},{"location":"pubfinder_worker_ref/#pubfinder_worker.PubFinderWorker.start","text":"start the consumer Source code in src/pubfinder_worker.py @staticmethod def start ( i = 0 ): \"\"\"start the consumer \"\"\" pfw = PubFinderWorker ( i ) logging . debug ( PubFinderWorker . log + 'Start %s ' % str ( i )) pfw . consume ()","title":"start()"},{"location":"pubfinder_worker_ref/#pubfinder_worker.PubFinderWorker.worker_db","text":"worker function to retrieve publication data from the postgreSQL Source code in src/pubfinder_worker.py def worker_db ( self , queue ): \"\"\" worker function to retrieve publication data from the postgreSQL \"\"\" while self . running : try : item = queue . pop () except IndexError : time . sleep ( 0.1 ) pass else : publication = PubFinderHelper . get_publication ( item ) publication_temp = self . dao . get_publication ( publication [ 'doi' ]) if publication_temp and isinstance ( publication , dict ): logging . warning ( self . log + \" found in db \" + publication [ 'doi' ]) publication = publication_temp publication [ 'source' ] = 'db' publication [ 'source_id' ] = [{ 'title' : 'DB' , 'url' : 'https://ambalytics.com/' , 'license' : 'MIT' }] if type ( item ) is Event : item . data [ 'obj' ][ 'data' ] = publication self . finish_work ( item , 'db' )","title":"worker_db()"},{"location":"pubfinder_worker_ref/#pubfinder_worker.PubFinderWorker.worker_results","text":"worker functions to handle resulting Source code in src/pubfinder_worker.py def worker_results ( self , queue ): \"\"\" worker functions to handle resulting \"\"\" logging . warning ( self . log + \"worker results\" ) while self . running : try : result = queue . pop () except IndexError : time . sleep ( 0.1 ) pass else : if result and 'item' in result and 'tag' in result : self . finish_work ( result [ 'item' ], result [ 'tag' ])","title":"worker_results()"},{"location":"semanticscholar_source_ref/","text":"SemanticScholarSource Source code in src/semanticscholar_source.py class SemanticScholarSource ( object ): base_url = \"https://api.semanticscholar.org/v1/paper/\" tag = 'semanticscholar' log = 'SemanticScholar' work_queue = deque () work_pool = None running = True threads = 4 api_limit = 95 api_time = 300 def __init__ ( self , result_deque ): if not self . work_pool : self . work_pool = ThreadPool ( self . threads , self . worker , ()) self . result_deque = result_deque self . fetched_counter = Value ( 'i' , 0 ) api_limit_thread = threading . Timer ( self . api_time , reset_api_limit , args = [ self . fetched_counter , self . api_time ]) api_limit_thread . daemon = True api_limit_thread . start () self . api_reset_timestamp = int ( time . time ()) def worker ( self ): \"\"\" main work function, fetch items and add data \"\"\" while self . running : try : item = self . work_queue . pop () except IndexError : time . sleep ( 0.1 ) pass else : if item : publication = PubFinderHelper . get_publication ( item ) logging . warning ( self . log + \" work on item \" + publication [ 'doi' ]) publication_temp = self . add_data_to_publication ( publication ) if publication_temp : publication = publication_temp if type ( item ) is Event : item . data [ 'obj' ][ 'data' ] = publication result = { 'item' : item , 'tag' : self . tag } self . result_deque . append ( result ) def add_data_to_publication ( self , publication ): \"\"\" add data to a given publication using the doi to fetch a response and map the data \"\"\" response = self . api_limit_watcher ( publication [ 'doi' ]) return self . map ( response , publication ) def map_fields_of_study ( self , fields ): \"\"\" map fields of study \"\"\" result = [] for field in fields : name = field normalized_name = PubFinderHelper . normalize ( name ) if not any ( d [ 'normalized_name' ] == normalized_name for d in result ): result . append ({ 'name' : name , 'normalized_name' : normalized_name }) return result def map ( self , response_data , publication ): \"\"\" map a xml response to the internal data structure \"\"\" added_data = False if response_data : if PubFinderHelper . should_update ( 'title' , response_data , publication ): publication [ 'title' ] = PubFinderHelper . clean_title ( response_data [ 'title' ]) publication [ 'normalized_title' ] = PubFinderHelper . normalize ( publication [ 'title' ]) added_data = True if PubFinderHelper . should_update ( 'year' , response_data , publication ): publication [ 'year' ] = response_data [ 'year' ] added_data = True if 'venue' in response_data and 'publisher' not in publication : publication [ 'publisher' ] = response_data [ 'venue' ] added_data = True if 'numCitedBy' in response_data and ( 'citation_count' not in publication or publication [ 'citation_count' ] == 0 ): publication [ 'citation_count' ] = response_data [ 'numCitedBy' ] added_data = True if PubFinderHelper . should_update ( 'authors' , response_data , publication ): publication [ 'authors' ] = self . map_author ( response_data [ 'authors' ]) added_data = True if 'abstract' in response_data and ( 'abstract' not in publication or not PubFinderHelper . valid_abstract ( publication [ 'abstract' ])): abstract = PubFinderHelper . clean_abstract ( response_data [ 'abstract' ]) if PubFinderHelper . valid_abstract ( abstract ): publication [ 'abstract' ] = abstract added_data = True if PubFinderHelper . should_update ( 'fields_of_study' , response_data , publication ): publication [ 'fields_of_study' ] = self . map_fields_of_study ( response_data [ 'fields_of_study' ]) added_data = True if added_data : source_ids = publication [ 'source_id' ] source_ids . append ( { 'title' : 'SemanticScholar' , 'url' : 'https://www.semanticscholar.org?utm_source=api' , 'license' : 'TODO' }) publication [ 'source_id' ] = source_ids return publication def api_limit_watcher ( self , doi ): \"\"\" ensure api limits are kept and if the limit is reached wait for reset \"\"\" if self . fetched_counter . value < self . api_limit : with self . fetched_counter . get_lock (): self . fetched_counter . value += 1 return fetch ( doi ) else : wt = self . api_time - ( int ( time . time ()) - self . api_reset_timestamp ) % self . api_time + 1 logging . warning ( self . log + ' api limit reached, wait ' + str ( wt )) time . sleep ( wt ) self . api_limit_watcher ( doi ) def map_author ( self , authors ): \"\"\" amp authors \"\"\" result = [] for author in authors : if 'name' in author : name = author [ 'name' ] normalized_name = PubFinderHelper . normalize ( name ) result . append ({ 'name' : name , 'normalized_name' : normalized_name }) else : logging . warning ( self . log + ' no author name ' + json . dumps ( author )) return result add_data_to_publication ( self , publication ) add data to a given publication using the doi to fetch a response and map the data Source code in src/semanticscholar_source.py def add_data_to_publication ( self , publication ): \"\"\" add data to a given publication using the doi to fetch a response and map the data \"\"\" response = self . api_limit_watcher ( publication [ 'doi' ]) return self . map ( response , publication ) api_limit_watcher ( self , doi ) ensure api limits are kept and if the limit is reached wait for reset Source code in src/semanticscholar_source.py def api_limit_watcher ( self , doi ): \"\"\" ensure api limits are kept and if the limit is reached wait for reset \"\"\" if self . fetched_counter . value < self . api_limit : with self . fetched_counter . get_lock (): self . fetched_counter . value += 1 return fetch ( doi ) else : wt = self . api_time - ( int ( time . time ()) - self . api_reset_timestamp ) % self . api_time + 1 logging . warning ( self . log + ' api limit reached, wait ' + str ( wt )) time . sleep ( wt ) self . api_limit_watcher ( doi ) map ( self , response_data , publication ) map a xml response to the internal data structure Source code in src/semanticscholar_source.py def map ( self , response_data , publication ): \"\"\" map a xml response to the internal data structure \"\"\" added_data = False if response_data : if PubFinderHelper . should_update ( 'title' , response_data , publication ): publication [ 'title' ] = PubFinderHelper . clean_title ( response_data [ 'title' ]) publication [ 'normalized_title' ] = PubFinderHelper . normalize ( publication [ 'title' ]) added_data = True if PubFinderHelper . should_update ( 'year' , response_data , publication ): publication [ 'year' ] = response_data [ 'year' ] added_data = True if 'venue' in response_data and 'publisher' not in publication : publication [ 'publisher' ] = response_data [ 'venue' ] added_data = True if 'numCitedBy' in response_data and ( 'citation_count' not in publication or publication [ 'citation_count' ] == 0 ): publication [ 'citation_count' ] = response_data [ 'numCitedBy' ] added_data = True if PubFinderHelper . should_update ( 'authors' , response_data , publication ): publication [ 'authors' ] = self . map_author ( response_data [ 'authors' ]) added_data = True if 'abstract' in response_data and ( 'abstract' not in publication or not PubFinderHelper . valid_abstract ( publication [ 'abstract' ])): abstract = PubFinderHelper . clean_abstract ( response_data [ 'abstract' ]) if PubFinderHelper . valid_abstract ( abstract ): publication [ 'abstract' ] = abstract added_data = True if PubFinderHelper . should_update ( 'fields_of_study' , response_data , publication ): publication [ 'fields_of_study' ] = self . map_fields_of_study ( response_data [ 'fields_of_study' ]) added_data = True if added_data : source_ids = publication [ 'source_id' ] source_ids . append ( { 'title' : 'SemanticScholar' , 'url' : 'https://www.semanticscholar.org?utm_source=api' , 'license' : 'TODO' }) publication [ 'source_id' ] = source_ids return publication map_author ( self , authors ) amp authors Source code in src/semanticscholar_source.py def map_author ( self , authors ): \"\"\" amp authors \"\"\" result = [] for author in authors : if 'name' in author : name = author [ 'name' ] normalized_name = PubFinderHelper . normalize ( name ) result . append ({ 'name' : name , 'normalized_name' : normalized_name }) else : logging . warning ( self . log + ' no author name ' + json . dumps ( author )) return result map_fields_of_study ( self , fields ) map fields of study Source code in src/semanticscholar_source.py def map_fields_of_study ( self , fields ): \"\"\" map fields of study \"\"\" result = [] for field in fields : name = field normalized_name = PubFinderHelper . normalize ( name ) if not any ( d [ 'normalized_name' ] == normalized_name for d in result ): result . append ({ 'name' : name , 'normalized_name' : normalized_name }) return result worker ( self ) main work function, fetch items and add data Source code in src/semanticscholar_source.py def worker ( self ): \"\"\" main work function, fetch items and add data \"\"\" while self . running : try : item = self . work_queue . pop () except IndexError : time . sleep ( 0.1 ) pass else : if item : publication = PubFinderHelper . get_publication ( item ) logging . warning ( self . log + \" work on item \" + publication [ 'doi' ]) publication_temp = self . add_data_to_publication ( publication ) if publication_temp : publication = publication_temp if type ( item ) is Event : item . data [ 'obj' ][ 'data' ] = publication result = { 'item' : item , 'tag' : self . tag } self . result_deque . append ( result ) fetch ( doi ) fetch response to add data to publication cache up to 100 since we should not have doi be occurring multiple times Parameters: Name Type Description Default doi the doi to be fetched required Source code in src/semanticscholar_source.py @lru_cache ( maxsize = 10 ) def fetch ( doi ): \"\"\"fetch response to add data to publication cache up to 100 since we should not have doi be occurring multiple times Arguments: doi: the doi to be fetched \"\"\" r = requests . get ( SemanticScholarSource . base_url + requests . utils . quote ( doi )) # check encoding if r . status_code == 200 : json_response = r . json () if 'error' not in json_response : return json_response return None","title":"semanticscholar source"},{"location":"semanticscholar_source_ref/#semanticscholar_source.SemanticScholarSource","text":"Source code in src/semanticscholar_source.py class SemanticScholarSource ( object ): base_url = \"https://api.semanticscholar.org/v1/paper/\" tag = 'semanticscholar' log = 'SemanticScholar' work_queue = deque () work_pool = None running = True threads = 4 api_limit = 95 api_time = 300 def __init__ ( self , result_deque ): if not self . work_pool : self . work_pool = ThreadPool ( self . threads , self . worker , ()) self . result_deque = result_deque self . fetched_counter = Value ( 'i' , 0 ) api_limit_thread = threading . Timer ( self . api_time , reset_api_limit , args = [ self . fetched_counter , self . api_time ]) api_limit_thread . daemon = True api_limit_thread . start () self . api_reset_timestamp = int ( time . time ()) def worker ( self ): \"\"\" main work function, fetch items and add data \"\"\" while self . running : try : item = self . work_queue . pop () except IndexError : time . sleep ( 0.1 ) pass else : if item : publication = PubFinderHelper . get_publication ( item ) logging . warning ( self . log + \" work on item \" + publication [ 'doi' ]) publication_temp = self . add_data_to_publication ( publication ) if publication_temp : publication = publication_temp if type ( item ) is Event : item . data [ 'obj' ][ 'data' ] = publication result = { 'item' : item , 'tag' : self . tag } self . result_deque . append ( result ) def add_data_to_publication ( self , publication ): \"\"\" add data to a given publication using the doi to fetch a response and map the data \"\"\" response = self . api_limit_watcher ( publication [ 'doi' ]) return self . map ( response , publication ) def map_fields_of_study ( self , fields ): \"\"\" map fields of study \"\"\" result = [] for field in fields : name = field normalized_name = PubFinderHelper . normalize ( name ) if not any ( d [ 'normalized_name' ] == normalized_name for d in result ): result . append ({ 'name' : name , 'normalized_name' : normalized_name }) return result def map ( self , response_data , publication ): \"\"\" map a xml response to the internal data structure \"\"\" added_data = False if response_data : if PubFinderHelper . should_update ( 'title' , response_data , publication ): publication [ 'title' ] = PubFinderHelper . clean_title ( response_data [ 'title' ]) publication [ 'normalized_title' ] = PubFinderHelper . normalize ( publication [ 'title' ]) added_data = True if PubFinderHelper . should_update ( 'year' , response_data , publication ): publication [ 'year' ] = response_data [ 'year' ] added_data = True if 'venue' in response_data and 'publisher' not in publication : publication [ 'publisher' ] = response_data [ 'venue' ] added_data = True if 'numCitedBy' in response_data and ( 'citation_count' not in publication or publication [ 'citation_count' ] == 0 ): publication [ 'citation_count' ] = response_data [ 'numCitedBy' ] added_data = True if PubFinderHelper . should_update ( 'authors' , response_data , publication ): publication [ 'authors' ] = self . map_author ( response_data [ 'authors' ]) added_data = True if 'abstract' in response_data and ( 'abstract' not in publication or not PubFinderHelper . valid_abstract ( publication [ 'abstract' ])): abstract = PubFinderHelper . clean_abstract ( response_data [ 'abstract' ]) if PubFinderHelper . valid_abstract ( abstract ): publication [ 'abstract' ] = abstract added_data = True if PubFinderHelper . should_update ( 'fields_of_study' , response_data , publication ): publication [ 'fields_of_study' ] = self . map_fields_of_study ( response_data [ 'fields_of_study' ]) added_data = True if added_data : source_ids = publication [ 'source_id' ] source_ids . append ( { 'title' : 'SemanticScholar' , 'url' : 'https://www.semanticscholar.org?utm_source=api' , 'license' : 'TODO' }) publication [ 'source_id' ] = source_ids return publication def api_limit_watcher ( self , doi ): \"\"\" ensure api limits are kept and if the limit is reached wait for reset \"\"\" if self . fetched_counter . value < self . api_limit : with self . fetched_counter . get_lock (): self . fetched_counter . value += 1 return fetch ( doi ) else : wt = self . api_time - ( int ( time . time ()) - self . api_reset_timestamp ) % self . api_time + 1 logging . warning ( self . log + ' api limit reached, wait ' + str ( wt )) time . sleep ( wt ) self . api_limit_watcher ( doi ) def map_author ( self , authors ): \"\"\" amp authors \"\"\" result = [] for author in authors : if 'name' in author : name = author [ 'name' ] normalized_name = PubFinderHelper . normalize ( name ) result . append ({ 'name' : name , 'normalized_name' : normalized_name }) else : logging . warning ( self . log + ' no author name ' + json . dumps ( author )) return result","title":"SemanticScholarSource"},{"location":"semanticscholar_source_ref/#semanticscholar_source.SemanticScholarSource.add_data_to_publication","text":"add data to a given publication using the doi to fetch a response and map the data Source code in src/semanticscholar_source.py def add_data_to_publication ( self , publication ): \"\"\" add data to a given publication using the doi to fetch a response and map the data \"\"\" response = self . api_limit_watcher ( publication [ 'doi' ]) return self . map ( response , publication )","title":"add_data_to_publication()"},{"location":"semanticscholar_source_ref/#semanticscholar_source.SemanticScholarSource.api_limit_watcher","text":"ensure api limits are kept and if the limit is reached wait for reset Source code in src/semanticscholar_source.py def api_limit_watcher ( self , doi ): \"\"\" ensure api limits are kept and if the limit is reached wait for reset \"\"\" if self . fetched_counter . value < self . api_limit : with self . fetched_counter . get_lock (): self . fetched_counter . value += 1 return fetch ( doi ) else : wt = self . api_time - ( int ( time . time ()) - self . api_reset_timestamp ) % self . api_time + 1 logging . warning ( self . log + ' api limit reached, wait ' + str ( wt )) time . sleep ( wt ) self . api_limit_watcher ( doi )","title":"api_limit_watcher()"},{"location":"semanticscholar_source_ref/#semanticscholar_source.SemanticScholarSource.map","text":"map a xml response to the internal data structure Source code in src/semanticscholar_source.py def map ( self , response_data , publication ): \"\"\" map a xml response to the internal data structure \"\"\" added_data = False if response_data : if PubFinderHelper . should_update ( 'title' , response_data , publication ): publication [ 'title' ] = PubFinderHelper . clean_title ( response_data [ 'title' ]) publication [ 'normalized_title' ] = PubFinderHelper . normalize ( publication [ 'title' ]) added_data = True if PubFinderHelper . should_update ( 'year' , response_data , publication ): publication [ 'year' ] = response_data [ 'year' ] added_data = True if 'venue' in response_data and 'publisher' not in publication : publication [ 'publisher' ] = response_data [ 'venue' ] added_data = True if 'numCitedBy' in response_data and ( 'citation_count' not in publication or publication [ 'citation_count' ] == 0 ): publication [ 'citation_count' ] = response_data [ 'numCitedBy' ] added_data = True if PubFinderHelper . should_update ( 'authors' , response_data , publication ): publication [ 'authors' ] = self . map_author ( response_data [ 'authors' ]) added_data = True if 'abstract' in response_data and ( 'abstract' not in publication or not PubFinderHelper . valid_abstract ( publication [ 'abstract' ])): abstract = PubFinderHelper . clean_abstract ( response_data [ 'abstract' ]) if PubFinderHelper . valid_abstract ( abstract ): publication [ 'abstract' ] = abstract added_data = True if PubFinderHelper . should_update ( 'fields_of_study' , response_data , publication ): publication [ 'fields_of_study' ] = self . map_fields_of_study ( response_data [ 'fields_of_study' ]) added_data = True if added_data : source_ids = publication [ 'source_id' ] source_ids . append ( { 'title' : 'SemanticScholar' , 'url' : 'https://www.semanticscholar.org?utm_source=api' , 'license' : 'TODO' }) publication [ 'source_id' ] = source_ids return publication","title":"map()"},{"location":"semanticscholar_source_ref/#semanticscholar_source.SemanticScholarSource.map_author","text":"amp authors Source code in src/semanticscholar_source.py def map_author ( self , authors ): \"\"\" amp authors \"\"\" result = [] for author in authors : if 'name' in author : name = author [ 'name' ] normalized_name = PubFinderHelper . normalize ( name ) result . append ({ 'name' : name , 'normalized_name' : normalized_name }) else : logging . warning ( self . log + ' no author name ' + json . dumps ( author )) return result","title":"map_author()"},{"location":"semanticscholar_source_ref/#semanticscholar_source.SemanticScholarSource.map_fields_of_study","text":"map fields of study Source code in src/semanticscholar_source.py def map_fields_of_study ( self , fields ): \"\"\" map fields of study \"\"\" result = [] for field in fields : name = field normalized_name = PubFinderHelper . normalize ( name ) if not any ( d [ 'normalized_name' ] == normalized_name for d in result ): result . append ({ 'name' : name , 'normalized_name' : normalized_name }) return result","title":"map_fields_of_study()"},{"location":"semanticscholar_source_ref/#semanticscholar_source.SemanticScholarSource.worker","text":"main work function, fetch items and add data Source code in src/semanticscholar_source.py def worker ( self ): \"\"\" main work function, fetch items and add data \"\"\" while self . running : try : item = self . work_queue . pop () except IndexError : time . sleep ( 0.1 ) pass else : if item : publication = PubFinderHelper . get_publication ( item ) logging . warning ( self . log + \" work on item \" + publication [ 'doi' ]) publication_temp = self . add_data_to_publication ( publication ) if publication_temp : publication = publication_temp if type ( item ) is Event : item . data [ 'obj' ][ 'data' ] = publication result = { 'item' : item , 'tag' : self . tag } self . result_deque . append ( result )","title":"worker()"},{"location":"semanticscholar_source_ref/#semanticscholar_source.fetch","text":"fetch response to add data to publication cache up to 100 since we should not have doi be occurring multiple times Parameters: Name Type Description Default doi the doi to be fetched required Source code in src/semanticscholar_source.py @lru_cache ( maxsize = 10 ) def fetch ( doi ): \"\"\"fetch response to add data to publication cache up to 100 since we should not have doi be occurring multiple times Arguments: doi: the doi to be fetched \"\"\" r = requests . get ( SemanticScholarSource . base_url + requests . utils . quote ( doi )) # check encoding if r . status_code == 200 : json_response = r . json () if 'error' not in json_response : return json_response return None","title":"fetch()"}]}